<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.4"/>
<title>Cambridge SMT System: Tutorial.07.rulextract.md Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.4 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('Tutorial_807_8rulextract_8md.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Tutorial.07.rulextract.md</div>  </div>
</div><!--header-->
<div class="contents">
<a href="Tutorial_807_8rulextract_8md.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;Rule Extraction                {#rulextract}</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;=================</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;\section rulextract_start_reminder Getting started</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;See (\ref rulextract_start)</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;\section rulextract_cluster_setup Hadoop Cluster Setup</div>
<div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;</div>
<div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;**Note**: a user already having access to a Hadoop cluster</div>
<div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;may wish to skip <span class="keyword">this</span> section, after adding these dependencies</div>
<div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;to the `HADOOP_CLASSPATH` : `jcommander-1.35`, `hbase-0.92.0` and</div>
<div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;`guava-r09` .</div>
<div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;</div>
<div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;**Note**: we use Hadoop 1 as opposed to Hadoop 2</div>
<div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;(see [<span class="keyword">this</span> discussion](http:<span class="comment">//hadoop.apache.org/docs/r2.3.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html)).</span></div>
<div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;We also use the more recent API of Hadoop, which means</div>
<div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;that in general the <span class="keyword">import</span> statements use the `org.apache.hadoop.mapreduce`</div>
<div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="keyword">package </span>instead of the `org.apache.hadoop.mapred` package.</div>
<div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;</div>
<div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;We give instructions on how to set up a single</div>
<div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;node Hadoop cluster. Specifically, we follow instructions</div>
<div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;for the pseudo-distributed single node setup</div>
<div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;from http:<span class="comment">//hadoop.apache.org/docs/r1.2.1/single_node_setup.html .</span></div>
<div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;More information is available at http:<span class="comment">//hadoop.apache.org and</span></div>
<div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;in the book &quot;Hadoop, The Definitive Guide&quot; by Tom White.</div>
<div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;</div>
<div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;First, choose a working directory, for example `$HOME/hadoopcluster`, then</div>
<div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;run the following commands:</div>
<div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;</div>
<div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;    &gt; mkdir -p $HOME/hadoopcluster</div>
<div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;    &gt; cd $HOME/hadoopcluster</div>
<div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;    &gt; $RULEXTRACT/scripts/hadoopClusterSetup.bash</div>
<div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;</div>
<div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;This should install the cluster in the `$HOME/hadoopcluster/hadoop-1.2.1`</div>
<div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;directory. In the remainder of this tutorial, the `$HADOOP_ROOT`</div>
<div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;variable designates the Hadoop installation directory:</div>
<div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;</div>
<div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;    &gt; HADOOP_ROOT=$HOME/hadoopcluster/hadoop-1.2.1</div>
<div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;</div>
<div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;We now</div>
<div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;detail the steps in the `hadoopClusterSetup.bash` script. You can also</div>
<div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;have a look at the commands and comments inside the script for more information.</div>
<div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;  + The java version is checked. If java 1.7+ is not installed, then</div>
<div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;  a recent version of jdk is downloaded in the current directory, specifically</div>
<div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;  jdk1.8.0_05 .</div>
<div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;  + A recent version of Hadoop is downloaded, specifically version 1.2.1 .</div>
<div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;  + Libraries on which the code is dependent are downloaded.</div>
<div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;  + The configuration files in the Hadoop directory are modified to allow</div>
<div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;  pseudo-distributed mode and point to the correct `JAVA_HOME` . The</div>
<div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;  `HADOOP_CLASSPATH` is also modified to point to libraries that the code</div>
<div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;  depends on.</div>
<div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;  + Passwordless and passphraseless ssh is set. This is to make sure</div>
<div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;  that the command `ssh localhost` works without any password or passphrase</div>
<div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;  prompt.</div>
<div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;  + The Hadoop Distributed File System (HDFS) is formatted.</div>
<div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;  + Hadoop deamons are started. When this is done, you should</div>
<div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;  be able to check the status of HDFS and MapReduce with a browser</div>
<div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;  at the `localhost:50070` and `localhost:50030` respective addresses.</div>
<div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;  + The HDFS `ls` command is tested.</div>
<div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;  + The directory for your username (`/user/$USER`)</div>
<div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;  is created. Is is better to store your HDFS data in that directory rather</div>
<div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;  than the root directory or the `/tmp` directory.</div>
<div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;  + The cluster is shut down to avoid having java processes lying around.</div>
<div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;  You will need to restart the cluster to run MapReduce jobs with the following</div>
<div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;  command:</div>
<div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;</div>
<div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;      &gt; $HADOOP_ROOT/bin/start-all.sh</div>
<div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;</div>
<div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;Once you are done with this tutorial, you can shut down the Hadoop</div>
<div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;cluster with this command:</div>
<div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;</div>
<div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;      &gt; $HADOOP_ROOT/bin/stop-all.sh</div>
<div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;</div>
<div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;When running rule extraction commands, if you see a similar looking</div>
<div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;log message:</div>
<div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;</div>
<div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;      14/07/09 16:56:55 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)</div>
<div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;</div>
<div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;this means that the Hadoop cluster is not running and needs to be (re)started.</div>
<div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;</div>
<div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;Note that this Hadoop cluster installation is for tutorial purposes.</div>
<div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;If you have a multi-core machine and enough memory (say 16G-32G), then</div>
<div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;this cluster may be sufficient for extracting relatively large grammars.</div>
<div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;However, a proper installation will use several nodes and a different</div>
<div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;username for the Hadoop administrator.</div>
<div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;</div>
<div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;After running the installation script, if you still <a class="code" href="disambig_8main_8cpp.html#a6c553f81eec959161951231e7d0d2ec6">run</a> into</div>
<div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;trouble while running the rule extraction commands, you may <a class="code" href="disambig_8main_8cpp.html#a6c553f81eec959161951231e7d0d2ec6">run</a></div>
<div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;the following commands as a last resort (note that this</div>
<div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;will delete all your HDFS data):</div>
<div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;</div>
<div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;    &gt; $HADOOP_ROOT/bin/stop-all.sh</div>
<div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;    &gt; rm -rf /tmp/hadoop*</div>
<div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;    &gt; $HADOOP_ROOT/bin/hadoop namenode -format</div>
<div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;        &gt; $HADOOP_ROOT/bin/start-all.sh</div>
<div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;</div>
<div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;\section rulextract_pipeline_overview Pipeline Overview</div>
<div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;</div>
<div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;Grammar extraction is composed of two <a class="code" href="applylm_8main_8cpp.html#ac0f2228420376f4db7e1274f2b41667c">main</a> steps: extraction</div>
<div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;and retrieval.</div>
<div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;</div>
<div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;Extraction consists in extracting all phrase-based and hierarchical</div>
<div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;rules from word-aligned parallel text and computing model scores</div>
<div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;for these rules. Extraction is itself</div>
<div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;decomposed into the following steps:</div>
<div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160; + training data loading: the training data, i.e. word aligned</div>
<div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;parallel text, is loaded onto HDFS.</div>
<div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160; + extraction: rules are simply extracted, no scores are computed</div>
<div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160; + model score computation: scores are computed for the extracted</div>
<div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;rules. Currently, source-to-target and target-to-source probabilities</div>
<div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;are computed.</div>
<div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160; + merging: the various outputs from the previous step (score computation)</div>
<div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;are merged so that a rule can be associated to multiple scores.</div>
<div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;</div>
<div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;Rule retrieval, or rule filtering, consists in obtaining</div>
<div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;rules and scores that are only relevant to a given input test</div>
<div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;set or a given input sentence to be translated.</div>
<div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;Rule retrieval is decomposed into the following steps:</div>
<div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160; + launch lexical probability servers: lexical features</div>
<div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;are computed with a client/server architecture rather than</div>
<div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;with MapReduce because lexical models take a fair amount of memory.</div>
<div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160; + rule retrieval: rules relevant to a test set are looked up</div>
<div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;in the HFiles produced by the extraction phase.</div>
<div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160; + grammar formatting: a grammar with a format suitable for the HiFST</div>
<div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;decoder is produced.</div>
<div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;</div>
<div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;The next section details the various steps for grammar extraction.</div>
<div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;</div>
<div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;\section rulextract_grammar_extraction Grammar Extraction</div>
<div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;</div>
<div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;  \subsection rulextract_commands Running Commands</div>
<div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;</div>
<div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;  In the remainder of this tutorial, it is assumed that commands</div>
<div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;  are <a class="code" href="disambig_8main_8cpp.html#a6c553f81eec959161951231e7d0d2ec6">run</a> from the `$DEMO` directory. Please change to that</div>
<div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;  directory:</div>
<div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;</div>
<div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;      &gt; cd $DEMO</div>
<div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;</div>
<div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;  All commands look like:</div>
<div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;</div>
<div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;       &gt; $HADOOP_ROOT/bin/hadoop jar $RULEXTRACTJAR class args</div>
<div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;</div>
<div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;  where `class` is a particular java class with a <a class="code" href="applylm_8main_8cpp.html#ac0f2228420376f4db7e1274f2b41667c">main</a> method and</div>
<div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;  `args` are the command line arguments. We use [JCommander](http:<span class="comment">//jcommander.org/)</span></div>
<div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;  for command line argument parsing. You can therefore put all arguments</div>
<div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;  in a configuration file `config` and use the syntax `@config` to replace</div>
<div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;  the command line arguments (see the [@-syntax](http:<span class="comment">//jcommander.org/#Syntax)).</span></div>
<div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;</div>
<div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;  Create a directory to store logs:</div>
<div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;</div>
<div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;      &gt; mkdir -p logs</div>
<div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;</div>
<div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;  After running rule extraction commands that produce an output</div>
<div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;  on HDFS, you can visualize the output using either the SequenceFile</div>
<div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;  printer or the HFile printer provided. For example, after having</div>
<div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;  <a class="code" href="disambig_8main_8cpp.html#a6c553f81eec959161951231e7d0d2ec6">run</a> the extraction step (see below), you can see the extracted rules</div>
<div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;  as follows:</div>
<div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;</div>
<div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;          uk.ac.cam.eng.extraction.hadoop.util.SequenceFilePrint \</div>
<div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;          RUEN-WMT13/rules/part-r-00000 2&gt;/dev/null</div>
<div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;</div>
<div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;  This will print the first chunk of rules extracted.</div>
<div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;  After the merging step, you can visualize rules, alignments and features</div>
<div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;  as follows:</div>
<div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;</div>
<div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;          uk.ac.cam.eng.extraction.hadoop.util.HFilePrint \</div>
<div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;          RUEN-WMT13/merge/part-r-00000.hfile 2&gt;/dev/null</div>
<div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;</div>
<div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;  \subsection rulextract_load_data Data Loading</div>
<div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;</div>
<div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;  The first step in grammar extraction is to load the training data onto HDFS.</div>
<div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;  This is done via the following command:</div>
<div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;</div>
<div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;          uk.ac.cam.eng.extraction.hadoop.util.ExtractorDataLoader \</div>
<div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;          @configs/CF.rulextract.load \</div>
<div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;          &gt;&amp; logs/log.loaddata</div>
<div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;</div>
<div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;  You can see the following options in the `configs/CF.rulextract.load`</div>
<div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;  configuration file:</div>
<div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;</div>
<div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;    + `--source` : a gzipped text file with one source sentence per line.</div>
<div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;    + `--target` : a gzipped text file with one target sentence per line.</div>
<div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;    + `--alignment` : a gzipped text file with one sentence pair word alignment per line in</div>
<div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;    Berkeley format.</div>
<div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;    + `--provenance` : a gzipped text file with one set of space separated provenances for</div>
<div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;    a sentence pair per line. In general, each sentence pair has the &#39;<a class="code" href="applylm_8main_8cpp.html#ac0f2228420376f4db7e1274f2b41667c">main</a>&#39; provenance, unless</div>
<div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;    you want to exclude some sentence pairs from the general source-to-target and target-to-source</div>
<div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;    computation.</div>
<div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;    + `--hdfsout` : the location of the training data on HDFS.</div>
<div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;</div>
<div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;  For `--hdfsout`, you can specify a relative or absolute path. The relative path is relative</div>
<div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;  to your home directory on HDFS, that is `/user/$USER`.</div>
<div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;  Once the command is completed, you will find the training data at the following</div>
<div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;  location on HDFS:</div>
<div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;</div>
<div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;      /user/$USER/RUEN-WMT13/training_data</div>
<div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;</div>
<div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;  You can verify this by running the Hadoop ls command:</div>
<div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;</div>
<div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop fs -ls RUEN-WMT13</div>
<div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;</div>
<div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;  **Note**: for this tutorial, we use a sample of the training data available</div>
<div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;  for the Russian-English</div>
<div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;  [translation task at WMT13](http:<span class="comment">//statmt.org/wmt13/translation-task.html).</span></div>
<div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;  If you wish to test rule extraction with the entire data, modify</div>
<div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;  `$DEMO/configs/CF.rulextract.load` with the following options:</div>
<div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;</div>
<div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;    + `--source=train/ru.gz`</div>
<div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;    + `--target=train/en.gz`</div>
<div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;    + `--alignment=train/align.berkeley.gz`</div>
<div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;    + `--provenance=train/provenance.gz`</div>
<div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;</div>
<div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;  For the full data, we give indicative timing measurements obtained</div>
<div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;  on our cluster of 12 machines with 16 cores and 47G RAM each:</div>
<div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;</div>
<div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;    + extraction: 106m</div>
<div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;    + s2t : 12m</div>
<div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;    + t2s : 13m</div>
<div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;    + merge : 41m</div>
<div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;    + retrieval : 11m</div>
<div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;</div>
<div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;  \subsection rulextract_extract Rule Extraction</div>
<div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;</div>
<div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;  Once the training data has been loaded onto HDFS, rules can be extracted.</div>
<div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;  This is done via the following command:</div>
<div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;</div>
<div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;          uk.ac.cam.eng.extraction.hadoop.extraction.ExtractorJob \</div>
<div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;          @configs/CF.rulextract.extract \</div>
<div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;          &gt;&amp; logs/log.extract</div>
<div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;</div>
<div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;  You can see the following options in the `configs/CF.rulextract.extract`</div>
<div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;  configuration file:</div>
<div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;</div>
<div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;    + `--input` : the input training data on HDFS. This was the output from data loading.</div>
<div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;    + `--output` : the extracted rules on HDFS. This is a directory.</div>
<div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;    + `--remove_monotonic_repeats` : clips counts. For example, given a monotonically aligned</div>
<div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;    phrase pair</div>
<div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;    &lt;a b c, d e f&gt;, the hiero rule &lt;a X, d X&gt; can be extracted from &lt;a b, d e&gt; and from</div>
<div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;    &lt;a b c, d e f&gt;, but the occurrence count is clipped to 1.</div>
<div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;    + `--max_source_phrase` : the maximum source phrase length for a phrase-based rule.</div>
<div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;    + `--max_source_elements` : the maximum number of source elements (terminal or nonterminal)</div>
<div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;    for a hiero rule.</div>
<div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;    + `--max_terminal_length` : the maximum number of consecutive source terminals for a hiero rule.</div>
<div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;    + `--max_nonterminal_span` : the maximum number of terminals covered by a source nonterminal.</div>
<div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;    + `--provenance` : comma-separated list of provenances.</div>
<div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;</div>
<div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;  Once the extraction is complete, you will find the rules</div>
<div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;  in the `/user/$USER/RUEN-WMT13/rules/` HDFS directory.</div>
<div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;  In this directory, you should see the following files:</div>
<div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;</div>
<div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;    + `_SUCCESS` : this file indicates that the job successfully completed.</div>
<div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;    + `_logs` : this directory contains metatdata about the job that has been <a class="code" href="disambig_8main_8cpp.html#a6c553f81eec959161951231e7d0d2ec6">run</a>.</div>
<div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;    + `part-r-*` : these files are the output of the Hadoop job.</div>
<div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;</div>
<div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;  You can visualize the output by running the SequenceFile printing command:</div>
<div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;</div>
<div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;          uk.ac.cam.eng.extraction.hadoop.util.SequenceFilePrint \</div>
<div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;          RUEN-WMT13/rules/part-r-00000 2&gt;/dev/null | head -n 3</div>
<div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;</div>
<div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;  You should see a similar looking output:</div>
<div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;</div>
<div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;      0 -1_10 -1_138_20  {0=1, 1=1}        {0-0 1-2=1}</div>
<div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;      0 -1_100 -1_4_77   {0=2, 1=2}        {0-0 1-2=2}</div>
<div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;      0 -1_100288_3 -1_1672_4    {0=1, 1=1}             {0-0 1-1 2-2=1}</div>
<div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;</div>
<div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;  The first three fields correspond to the left-hand side, source side</div>
<div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;  and target side of a rule. The fourth field gives rule counts <span class="keywordflow">for</span></div>
<div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;  various provenances. The last field gives the word alignment <span class="keywordflow">for</span></div>
<div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;  the rule and its count.</div>
<div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;</div>
<div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;  \subsection rulextract_s2t Source-to-target Probability</div>
<div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;</div>
<div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;  The output of the previous job is the input to feature computation.</div>
<div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;  We start by computing source-to-target rule probabilities <span class="keywordflow">for each</span></div>
<div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;  provenance. This is done via the following command:</div>
<div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;</div>
<div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;          uk.ac.cam.eng.extraction.hadoop.features.phrase.Source2TargetJob \</div>
<div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;          -D mapred.reduce.tasks=16 \</div>
<div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;          @configs/CF.rulextract.s2t \</div>
<div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;          &gt;&amp; logs/log.s2t</div>
<div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;</div>
<div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;  You can see the following options in the `configs/CF.rulextract.s2t`</div>
<div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;  configuration file:</div>
<div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;</div>
<div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;    + `--input` : the extracted rules on HDFS. This was the output from rule extraction.</div>
<div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;    + `--output` : the source-to-target probabilities on HDFS.</div>
<div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;    + `--provenance` : comma-separated list of provenances.</div>
<div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;    + `--mapreduce_features` : comma-separated list of features. This is important</div>
<div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;    to give the correct index to each feature.</div>
<div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;</div>
<div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;  Note that the command line also has the option `-D mapred.reduce.tasks=16` .</div>
<div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;  This specifies the number of reducers at runtime. Unfortunately, the number</div>
<div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;  of reducers is not determined automatically by the Hadoop framework. You</div>
<div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;  can also specify the number of reducers in the `mapred-site.xml`</div>
<div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;  Hadoop cluster configuration file with the `mapred.reduce.tasks`</div>
<div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;  <span class="keyword">property</span>.</div>
<div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;  Because <a class="code" href="applylm_8main_8cpp.html#ac0f2228420376f4db7e1274f2b41667c">main</a> classes</div>
<div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;  all implement the `Tool` interface, you can specify <span class="keyword">generic</span> options</div>
<div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;  at the command line (see [<span class="keyword">this</span> example](http:<span class="comment">//hadoopi.wordpress.com/2013/06/05/hadoop-implementing-the-tool-interface-for-mapreduce-driver/)</span></div>
<div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;  and the [API documentation](https:<span class="comment">//hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/util/Tool.html) for more detail).</span></div>
<div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;</div>
<div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;  Once the job is complete, you will find the source-to-target probabilities</div>
<div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;  in the `/user/$USER/RUEN-WMT13/s2t` HDFS directory.</div>
<div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;</div>
<div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;  You can visualize the output by running the SequenceFile printing command:</div>
<div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;</div>
<div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;          uk.ac.cam.eng.extraction.hadoop.util.SequenceFilePrint \
          RUEN-WMT13/s2t/part-r-00000 2&gt;/dev/null | head -n 3</div>
<div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;</div>
<div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;  You should see a similar looking output:</div>
<div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;</div>
<div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;      0 -1_10003_1428_3_1752 -1_6_3_1106_784_4_911      {0=1.0, 1=1.0, 4=1.0, 5=1.0}</div>
<div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;      0 -1_100521_277 -1_3191_471_151                                   {0=1.0, 1=1.0, 4=1.0, 5=1.0}</div>
<div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;      0 -1_100529_360070_9 -1_8906_41685_40                     {0=1.0, 1=1.0, 4=1.0, 5=1.0}</div>
<div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;</div>
<div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;  The first three fields correspond to the left-hand side, source side</div>
<div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;  and target side of a rule. The last field represents the computed</div>
<div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;  features. In <span class="keyword">this</span> example, the index 0 corresponds to the source-to-target</div>
<div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;  probability, the index 1 corresponds to the rule count, the index 4 correponds</div>
<div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;  to the <span class="stringliteral">&quot;cc&quot;</span> provenance specific probability and the index 5 corresponds to the</div>
<div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;  <span class="stringliteral">&quot;cc&quot;</span> provenance specific rule count. In the sample provided, <span class="stringliteral">&quot;cc&quot;</span> is the only</div>
<div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;  provenance so features <span class="keywordflow">for</span> indices 0, 1, 4 and 5 should be the same.</div>
<div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;</div>
<div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;  \subsection rulextract_t2s Target-to-source Probability</div>
<div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;</div>
<div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;  Computation of other features can be done simultaneously.</div>
<div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;  Computing target-to-source probabilities <span class="keywordflow">for each</span> provenance</div>
<div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;  can be done as follows:</div>
<div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;</div>
<div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;          uk.ac.cam.eng.extraction.hadoop.features.phrase.Target2SourceJob \</div>
<div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;          -D mapred.reduce.tasks=16 \</div>
<div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;          @configs/CF.rulextract.t2s \</div>
<div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;          &gt;&amp; logs/log.t2s</div>
<div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;</div>
<div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;  You can see the following options in the `configs/CF.rulextract.t2s`</div>
<div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;  configuration file:</div>
<div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;</div>
<div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;    + `--input` : the extracted rules on HDFS.</div>
<div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;    + `--output` : the source-to-target probabilities on HDFS.</div>
<div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;    + `--provenance` : comma-separated list of provenances.</div>
<div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;    + `--mapreduce_features` : comma-separated list of features.</div>
<div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;</div>
<div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;  Once the job is complete, you will find the target-to-source probabilities</div>
<div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;  in the `/user/$USER/RUEN-WMT13/t2s` HDFS directory. The output is very similar</div>
<div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;  to the output of the source-to-target job but the features have indices</div>
<div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;  2, 3, 12 and 13:</div>
<div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;</div>
<div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;          uk.ac.cam.eng.extraction.hadoop.util.SequenceFilePrint \
          RUEN-WMT13/t2s/part-r-00000 2&gt;/dev/null | head -n 3</div>
<div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;      0 -1_1985 -1_100090                         {2=1.0, 3=1.0, 12=1.0, 13=1.0}</div>
<div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;      0 -1_1985_8_6 -1_100090_215_8               {2=1.0, 3=1.0, 12=1.0, 13=1.0}</div>
<div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;      0 -1_234783_5289 -1_100148_3442             {2=1.0, 3=1.0, 12=1.0, 13=1.0}</div>
<div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;</div>
<div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160;  \subsection rulextract_merge Feature Merging</div>
<div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160;</div>
<div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160;  Once all features have been computed, rules and features</div>
<div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160;  are merged into a single output. This can be done via the</div>
<div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160;  following command:</div>
<div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160;</div>
<div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop \</div>
<div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160;          jar $RULEXTRACTJAR \</div>
<div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;          uk.ac.cam.eng.extraction.hadoop.merge.MergeJob \</div>
<div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;          -D mapred.reduce.tasks=10 \</div>
<div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;          @configs/CF.rulextract.merge \</div>
<div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;          &gt;&amp; logs/log.merge</div>
<div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;</div>
<div class="line"><a name="l00384"></a><span class="lineno">  384</span>&#160;  You can see the following options in the `configs/CF.rulextract.merge`</div>
<div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;  configuration file:</div>
<div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;</div>
<div class="line"><a name="l00387"></a><span class="lineno">  387</span>&#160;    + `--input_features` : comma separated list of output from feature computation</div>
<div class="line"><a name="l00388"></a><span class="lineno">  388</span>&#160;    + `--input_rules` : the extracted rules on HDFS</div>
<div class="line"><a name="l00389"></a><span class="lineno">  389</span>&#160;    + `--output` : merged output</div>
<div class="line"><a name="l00390"></a><span class="lineno">  390</span>&#160;</div>
<div class="line"><a name="l00391"></a><span class="lineno">  391</span>&#160;  We need both rules and features as input because the merge job adds word alignment</div>
<div class="line"><a name="l00392"></a><span class="lineno">  392</span>&#160;  information into the output <span class="keyword">using</span> the rules.</div>
<div class="line"><a name="l00393"></a><span class="lineno">  393</span>&#160;  Once <span class="keyword">this</span> step is completed, there will be 10 output hfiles</div>
<div class="line"><a name="l00394"></a><span class="lineno">  394</span>&#160;  in the `/user/$USER/RUEN-WMT13/merge` HDFS directory. For backup purposes</div>
<div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;  and in order to make the subsequent retrieval step faster,</div>
<div class="line"><a name="l00396"></a><span class="lineno">  396</span>&#160;  copy these hfiles to local disk as follows:</div>
<div class="line"><a name="l00397"></a><span class="lineno">  397</span>&#160;</div>
<div class="line"><a name="l00398"></a><span class="lineno">  398</span>&#160;      &gt; mkdir -p hfile</div>
<div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;      &gt; $HADOOP_ROOT/bin/hadoop fs -copyToLocal RUEN-WMT13/merge<span class="comment">/*.hfile hfile/</span></div>
<div class="line"><a name="l00400"></a><span class="lineno">  400</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00401"></a><span class="lineno">  401</span>&#160;<span class="comment">  If you are using NFS, it&#39;s better to copy the hfiles to local disk, e.g.</span></div>
<div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;<span class="comment">  `/tmp` or `/scratch` .</span></div>
<div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;<span class="comment">  To visualize the output, run the HFile printing command:</span></div>
<div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;<span class="comment">      &gt; $HADOOP_ROOT/bin/hadoop \</span></div>
<div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;<span class="comment">          jar $RULEXTRACTJAR \</span></div>
<div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;<span class="comment">          uk.ac.cam.eng.extraction.hadoop.util.HFilePrint \</span></div>
<div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;<span class="comment">          RUEN-WMT13/merge/part-r-00000.hfile 2&gt;/dev/null | head -n 3</span></div>
<div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;<span class="comment">      0 12 3      {0-0=7}                                                         {0=0.01728395061728395, 1=7.0, 2=0.002145922746781116, 3=7.0, 4=0.01728395061728395, 5=7.0, 12=0.002145922746781116, 13=7.0}</span></div>
<div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;<span class="comment">      0 12 6      {0-0=5}                                                         {0=0.012345679012345678, 1=5.0, 2=0.010438413361169102, 3=5.0, 4=0.012345679012345678, 5=5.0, 12=0.010438413361169102, 13=5.0}</span></div>
<div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;<span class="comment">      0 12 7      {0-0=84}                                                        {0=0.2074074074074074, 1=84.0, 2=0.10218978102189781, 3=84.0, 4=0.2074074074074074, 5=84.0, 12=0.10218978102189781, 13=84.0}</span></div>
<div class="line"><a name="l00413"></a><span class="lineno">  413</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;<span class="comment">  The first three fields correspond to the rule, the fourth field to the word alignment and the last field to the computed features.</span></div>
<div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;<span class="comment">\section rulextract_retrieval Grammar Filtering</span></div>
<div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;<span class="comment">  \subsection lex_model Lexical Models Download</span></div>
<div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;<span class="comment">  Lexical models are available as a separate download</span></div>
<div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;<span class="comment">  as they take a fair amount of disk space. Run these commands:</span></div>
<div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;<span class="comment">      &gt; wget http://mi.eng.cam.ac.uk/~jmp84/share/giza_ibm_model1_filtered.tar.gz</span></div>
<div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;<span class="comment">      &gt; tar -xvf giza_ibm_model1_filtered.tar.gz</span></div>
<div class="line"><a name="l00425"></a><span class="lineno">  425</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;<span class="comment">  These lexical models were filtered with the source vocabulary and</span></div>
<div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;<span class="comment">  target vocabulary of the test set for this tutorial to obtain</span></div>
<div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;<span class="comment">  a reasonable size for these models (the source vocabulary is easily</span></div>
<div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;<span class="comment">  obtained from the test set, the target vocabulary is obtained by taking</span></div>
<div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;<span class="comment">  target words from relevant translation rules for that test set).</span></div>
<div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;<span class="comment">  If you wish, you can also download</span></div>
<div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;<span class="comment">  the [full models](http://mi.eng.cam.ac.uk/~jmp84/share/giza_ibm_model1_filtered.tar.gz)</span></div>
<div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;<span class="comment">  but you will require a machine with about 30G RAM to load the servers.</span></div>
<div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00435"></a><span class="lineno">  435</span>&#160;<span class="comment">  \subsection lex_prob_server Lexical Probability Servers</span></div>
<div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;<span class="comment">  A preliminary step prior to grammar filtering is to launch lexical</span></div>
<div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;<span class="comment">  probability servers. These servers load IBM Model 1 probabilities</span></div>
<div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;<span class="comment">  for various provenances and for source-to-target and</span></div>
<div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;<span class="comment">  target-to-source directions. These probabilities have</span></div>
<div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;<span class="comment">  been obtained by the GIZA++ toolkit. For convenience, for this</span></div>
<div class="line"><a name="l00442"></a><span class="lineno">  442</span>&#160;<span class="comment">  tutorial, we provide pretrained models. The source-to-target</span></div>
<div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;<span class="comment">  and target-to-source servers are launched as follows:</span></div>
<div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00445"></a><span class="lineno">  445</span>&#160;<span class="comment">      &gt; export HADOOP_HEAPSIZE=3000</span></div>
<div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;<span class="comment">      &gt; export HADOOP_OPTS=&quot;-XX:+UseConcMarkSweepGC -verbose:gc -server -Xms3000M&quot;</span></div>
<div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;<span class="comment">      &gt; $HADOOP_ROOT/bin/hadoop \</span></div>
<div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;<span class="comment">          jar $RULEXTRACTJAR \</span></div>
<div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;<span class="comment">          uk.ac.cam.eng.extraction.hadoop.features.lexical.TTableServer \</span></div>
<div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;<span class="comment">          @configs/CF.rulextract.lexserver \</span></div>
<div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;<span class="comment">          --ttable_direction=s2t \</span></div>
<div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;<span class="comment">          --ttable_language_pair=en2ru \</span></div>
<div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;<span class="comment">          --min_lex_prob=0.001</span></div>
<div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;<span class="comment">      &gt; $HADOOP_ROOT/bin/hadoop \</span></div>
<div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;<span class="comment">          jar $RULEXTRACTJAR \</span></div>
<div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;<span class="comment">          uk.ac.cam.eng.extraction.hadoop.features.lexical.TTableServer \</span></div>
<div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;<span class="comment">          @configs/CF.rulextract.lexserver \</span></div>
<div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;<span class="comment">          --ttable_direction=t2s \</span></div>
<div class="line"><a name="l00459"></a><span class="lineno">  459</span>&#160;<span class="comment">          --ttable_language_pair=ru2en \</span></div>
<div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;<span class="comment">          --min_lex_prob=0.001</span></div>
<div class="line"><a name="l00461"></a><span class="lineno">  461</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;<span class="comment">  Both servers should be launched in a separate terminal and without</span></div>
<div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;<span class="comment">  trailing ampersand to the commands. Once the servers are ready, a message</span></div>
<div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;<span class="comment">  similar to `&quot;TTable server ready on port: 4949&quot;` will be printed out.</span></div>
<div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;<span class="comment">  You can see the following options in the `configs/CF.rulextract.lexserver`</span></div>
<div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;<span class="comment">  configuration file:</span></div>
<div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;<span class="comment">    + `--ttable_s2t_server_port` : the port for the source-to-target server.</span></div>
<div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;<span class="comment">    + `--ttable_s2t_host` : the host for the source-to-target server.</span></div>
<div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;<span class="comment">    + `--ttable_t2s_server_port` : the port for the target-to-source server.</span></div>
<div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;<span class="comment">    + `--ttable_t2s_host` : the host for the target-to-source server.</span></div>
<div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;<span class="comment">    + `--ttable_server_template` : indicates a templated path to the lexical model</span></div>
<div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;<span class="comment">    where the templates `$GENRE` and `$DIRECTION` are replaced by their actual</span></div>
<div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;<span class="comment">        value.</span></div>
<div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;<span class="comment">    + `--provenance` : comma-separated provenances. This is used to search</span></div>
<div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;<span class="comment">    for the lexical models in the template.</span></div>
<div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;<span class="comment">    + `--min_lex_prob` : this option is used to keep the memory usage relatively low.</span></div>
<div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;<span class="comment">        Probabilities in the Model 1 table that are lower than a certain threshold are</span></div>
<div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;<span class="comment">        discarded. By default, no entry is discarded.</span></div>
<div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;<span class="comment">  \subsection hadoop_local_conf Hadoop Local Configuration</span></div>
<div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;<span class="comment">  We have mentioned that retrieval is faster from local disk than</span></div>
<div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;<span class="comment">  from HDFS. In order to run Hadoop commands locally, we need</span></div>
<div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;<span class="comment">  to use a local configuration. This can be done by modifying</span></div>
<div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;<span class="comment">  the Hadoop configuration file that was prepared when setting</span></div>
<div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;<span class="comment">  up the Hadoop cluster:</span></div>
<div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;<span class="comment">      &gt; cp -r $HADOOP_ROOT/conf configs/hadoopLocalConf</span></div>
<div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;<span class="comment">      &gt; cat $HADOOP_ROOT/conf/mapred-site.xml | \</span></div>
<div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;<span class="comment">          $RULEXTRACT/scripts/makeHadoopLocalConfig.pl \</span></div>
<div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;<span class="comment">          &gt; configs/hadoopLocalConf/mapred-site.xml</span></div>
<div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;<span class="comment">      &gt; cat $HADOOP_ROOT/conf/hdfs-site.xml | \</span></div>
<div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;<span class="comment">          $RULEXTRACT/scripts/makeHadoopLocalConfig.pl \</span></div>
<div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;<span class="comment">          &gt; configs/hadoopLocalConf/hdfs-site.xml</span></div>
<div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;<span class="comment">      &gt; cat $HADOOP_ROOT/conf/core-site.xml | \</span></div>
<div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;<span class="comment">          $RULEXTRACT/scripts/makeHadoopLocalConfig.pl \</span></div>
<div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;<span class="comment">          &gt; configs/hadoopLocalConf/core-site.xml</span></div>
<div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;<span class="comment">  \subsection retrieval Grammar Filtering</span></div>
<div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;<span class="comment">  Once the lexical probability servers are up, the Hadoop local</span></div>
<div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;<span class="comment">  configuration has been prepared, one can proceed to</span></div>
<div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;<span class="comment">  actual grammar filtering. This is done via the following</span></div>
<div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;<span class="comment">  command:</span></div>
<div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;<span class="comment">      &gt; $HADOOP_ROOT/bin/hadoop \</span></div>
<div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;<span class="comment">          --config configs/hadoopLocalConf \</span></div>
<div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;<span class="comment">          jar $RULEXTRACTJAR \</span></div>
<div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;<span class="comment">          uk.ac.cam.eng.rule.retrieval.RuleRetriever \</span></div>
<div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160;<span class="comment">          @configs/CF.rulextract.retrieval \</span></div>
<div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160;<span class="comment">          &gt;&amp; logs/log.retrieval</span></div>
<div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160;<span class="comment">  You can see the following options in the `configs/CF.rulextract.retrieval`</span></div>
<div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160;<span class="comment">  configuration file:</span></div>
<div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;<span class="comment">    + `--max_source_phrase` : the maximum source phrase length for a phrase-based rule.</span></div>
<div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;<span class="comment">    This option is used to control how source patterns are generated from the</span></div>
<div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;<span class="comment">    test file. The value for this option should be at most the value</span></div>
<div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;<span class="comment">    chosen when extracting rules. Otherwise, no rules will be found for certain</span></div>
<div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;<span class="comment">    patterns.</span></div>
<div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;<span class="comment">    + `--max_source_elements` : the maximum number of source elements (terminal</span></div>
<div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;<span class="comment">    or nonterminal) for a hiero rule. Same remarks as for `--max_source_phrase`</span></div>
<div class="line"><a name="l00525"></a><span class="lineno">  525</span>&#160;<span class="comment">    apply.</span></div>
<div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;<span class="comment">    + `--max_terminal_length` : the maximum number of consecutive source terminals</span></div>
<div class="line"><a name="l00527"></a><span class="lineno">  527</span>&#160;<span class="comment">    for a hiero rule. Same remarks as for `--max_source_phrase` apply.</span></div>
<div class="line"><a name="l00528"></a><span class="lineno">  528</span>&#160;<span class="comment">    + `--max_nonterminal_span` : the maximum number of terminals covered by a</span></div>
<div class="line"><a name="l00529"></a><span class="lineno">  529</span>&#160;<span class="comment">    source nonterminal. Usually we set the value for this option to be equal</span></div>
<div class="line"><a name="l00530"></a><span class="lineno">  530</span>&#160;<span class="comment">    to the one used in the extraction phase but it&#39;s possible to choose any</span></div>
<div class="line"><a name="l00531"></a><span class="lineno">  531</span>&#160;<span class="comment">    value smaller that the value of `--hr_max_height` .</span></div>
<div class="line"><a name="l00532"></a><span class="lineno">  532</span>&#160;<span class="comment">    + `--hr_max_height` : the maximum number of terminals covered by the entire</span></div>
<div class="line"><a name="l00533"></a><span class="lineno">  533</span>&#160;<span class="comment">    source side of a rule. The value for this option should be at most the value</span></div>
<div class="line"><a name="l00534"></a><span class="lineno">  534</span>&#160;<span class="comment">    chosen for the `--cykparser.hrmaxheight` option in the HiFST decoder, otherwise</span></div>
<div class="line"><a name="l00535"></a><span class="lineno">  535</span>&#160;<span class="comment">    some rules will never be used in decoding.</span></div>
<div class="line"><a name="l00536"></a><span class="lineno">  536</span>&#160;<span class="comment">    + `--mapreduce_features` : comma-separated list of mapreduce features. Note that</span></div>
<div class="line"><a name="l00537"></a><span class="lineno">  537</span>&#160;<span class="comment">    for the value for this option, we have used the value from the extraction phase</span></div>
<div class="line"><a name="l00538"></a><span class="lineno">  538</span>&#160;<span class="comment">    and added lexical features.</span></div>
<div class="line"><a name="l00539"></a><span class="lineno">  539</span>&#160;<span class="comment">    + `--provenance` : comma-separated list of provenances.</span></div>
<div class="line"><a name="l00540"></a><span class="lineno">  540</span>&#160;<span class="comment">    + `--features`: comma-separated list of features.</span></div>
<div class="line"><a name="l00541"></a><span class="lineno">  541</span>&#160;<span class="comment">    + `--pass_through_rules` : file containing special translation rules</span></div>
<div class="line"><a name="l00542"></a><span class="lineno">  542</span>&#160;<span class="comment">    that copy source words or source word sequences to the target.</span></div>
<div class="line"><a name="l00543"></a><span class="lineno">  543</span>&#160;<span class="comment">    + `--filter_config` : file with additional filter options. This configuration</span></div>
<div class="line"><a name="l00544"></a><span class="lineno">  544</span>&#160;<span class="comment">    file determines what patterns are allowed, minimum source-to-target and target-to-source</span></div>
<div class="line"><a name="l00545"></a><span class="lineno">  545</span>&#160;<span class="comment">    probabilities for phrase-based and hierarchical rules, etc. See the</span></div>
<div class="line"><a name="l00546"></a><span class="lineno">  546</span>&#160;<span class="comment">    comments in `$DEMO/configs/CF.rulextract.filter` for details.</span></div>
<div class="line"><a name="l00547"></a><span class="lineno">  547</span>&#160;<span class="comment">    + `--source_patterns` : list of source patterns to be used in order to</span></div>
<div class="line"><a name="l00548"></a><span class="lineno">  548</span>&#160;<span class="comment">    generate source pattern instances.</span></div>
<div class="line"><a name="l00549"></a><span class="lineno">  549</span>&#160;<span class="comment">    + `--ttable_s2t_server_port` : the port for the source-to-target server. The</span></div>
<div class="line"><a name="l00550"></a><span class="lineno">  550</span>&#160;<span class="comment">    value for this option should be same as the one used to launch the servers.</span></div>
<div class="line"><a name="l00551"></a><span class="lineno">  551</span>&#160;<span class="comment">    + `--ttable_s2t_host` : the host for the source-to-target server.</span></div>
<div class="line"><a name="l00552"></a><span class="lineno">  552</span>&#160;<span class="comment">    + `--ttable_t2s_server_port` : the port for the target-to-source server.</span></div>
<div class="line"><a name="l00553"></a><span class="lineno">  553</span>&#160;<span class="comment">    + `--ttable_t2s_host` : the host for the target-to-source server.</span></div>
<div class="line"><a name="l00554"></a><span class="lineno">  554</span>&#160;<span class="comment">    + `--retrieval_threads` : the number of threads. The value for this option</span></div>
<div class="line"><a name="l00555"></a><span class="lineno">  555</span>&#160;<span class="comment">    should be equal to the number of HFiles obtained in the merge step.</span></div>
<div class="line"><a name="l00556"></a><span class="lineno">  556</span>&#160;<span class="comment">    + `--hfile` : the directory containing the HFiles.</span></div>
<div class="line"><a name="l00557"></a><span class="lineno">  557</span>&#160;<span class="comment">    + `--test_file` : the test set to be translated.</span></div>
<div class="line"><a name="l00558"></a><span class="lineno">  558</span>&#160;<span class="comment">    + `--rules` : the output file containing relevant rules for the test set.</span></div>
<div class="line"><a name="l00559"></a><span class="lineno">  559</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00560"></a><span class="lineno">  560</span>&#160;<span class="comment">  Once this step is completed, you should obtain a rule file</span></div>
<div class="line"><a name="l00561"></a><span class="lineno">  561</span>&#160;<span class="comment">  at this location: `$DEMO/G/rules.RU.tune.idx.gz`</span></div>
<div class="line"><a name="l00562"></a><span class="lineno">  562</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00563"></a><span class="lineno">  563</span>&#160;<span class="comment">  \subsection grammar_conversion Grammar Formatting</span></div>
<div class="line"><a name="l00564"></a><span class="lineno">  564</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00565"></a><span class="lineno">  565</span>&#160;<span class="comment">  In order to obtain a shallow grammar ready to use by the HiFST</span></div>
<div class="line"><a name="l00566"></a><span class="lineno">  566</span>&#160;<span class="comment">  decoder, a postprocessing step is needed. This can be</span></div>
<div class="line"><a name="l00567"></a><span class="lineno">  567</span>&#160;<span class="comment">  achieved via the following command:</span></div>
<div class="line"><a name="l00568"></a><span class="lineno">  568</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00569"></a><span class="lineno">  569</span>&#160;<span class="comment">      &gt; zcat -f G/rules.RU.tune.idx.gz | \</span></div>
<div class="line"><a name="l00570"></a><span class="lineno">  570</span>&#160;<span class="comment">          $RULEXTRACT/scripts/prepareShallow.pl | \</span></div>
<div class="line"><a name="l00571"></a><span class="lineno">  571</span>&#160;<span class="comment">          $RULEXTRACT/scripts/shallow2hifst.pl | \</span></div>
<div class="line"><a name="l00572"></a><span class="lineno">  572</span>&#160;<span class="comment">          $RULEXTRACT/scripts/sparse2nonsparse.pl 27 | \</span></div>
<div class="line"><a name="l00573"></a><span class="lineno">  573</span>&#160;<span class="comment">          gzip &gt; G/rules.shallow.vecfea.sample.prov.gz</span></div>
<div class="line"><a name="l00574"></a><span class="lineno">  574</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00575"></a><span class="lineno">  575</span>&#160;<span class="comment">  The `G/rules.shallow.vecfea.sample.prov.gz` file should</span></div>
<div class="line"><a name="l00576"></a><span class="lineno">  576</span>&#160;<span class="comment">  be ready to be used by the HiFST decoder with the</span></div>
<div class="line"><a name="l00577"></a><span class="lineno">  577</span>&#160;<span class="comment">  `--grammar.load` option. If you&#39;ve changed</span></div>
<div class="line"><a name="l00578"></a><span class="lineno">  578</span>&#160;<span class="comment">  the `configs/CF.rulextract.load` configuration</span></div>
<div class="line"><a name="l00579"></a><span class="lineno">  579</span>&#160;<span class="comment">  file to use the entire training data, and if you have</span></div>
<div class="line"><a name="l00580"></a><span class="lineno">  580</span>&#160;<span class="comment">  used the option `--min_lex_prob` set to zero for the</span></div>
<div class="line"><a name="l00581"></a><span class="lineno">  581</span>&#160;<span class="comment">  lexical probability servers, then you should</span></div>
<div class="line"><a name="l00582"></a><span class="lineno">  582</span>&#160;<span class="comment">  obtain a file with the same rules as in</span></div>
<div class="line"><a name="l00583"></a><span class="lineno">  583</span>&#160;<span class="comment">  `G/rules.shallow.vecfea.all.gz` with the same</span></div>
<div class="line"><a name="l00584"></a><span class="lineno">  584</span>&#160;<span class="comment">  first 11 features and additional provenance features.</span></div>
<div class="line"><a name="l00585"></a><span class="lineno">  585</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00586"></a><span class="lineno">  586</span>&#160;<span class="comment">\section Development</span></div>
<div class="line"><a name="l00587"></a><span class="lineno">  587</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00588"></a><span class="lineno">  588</span>&#160;<span class="comment">  \subsection ide IDE Development</span></div>
<div class="line"><a name="l00589"></a><span class="lineno">  589</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00590"></a><span class="lineno">  590</span>&#160;<span class="comment">In order to generate a project file for Eclipse,</span></div>
<div class="line"><a name="l00591"></a><span class="lineno">  591</span>&#160;<span class="comment">please follow these [instructions](https://github.com/typesafehub/sbteclipse).</span></div>
<div class="line"><a name="l00592"></a><span class="lineno">  592</span>&#160;<span class="comment">For IntelliJ IDEA, follow these [instructions](https://github.com/mpeltonen/sbt-idea).</span></div>
<div class="line"><a name="l00593"></a><span class="lineno">  593</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00594"></a><span class="lineno">  594</span>&#160;<span class="comment">  \subsection local_feature Adding a Local Feature</span></div>
<div class="line"><a name="l00595"></a><span class="lineno">  595</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00596"></a><span class="lineno">  596</span>&#160;<span class="comment">  We give instructions on how to add a local feature to rule extraction.</span></div>
<div class="line"><a name="l00597"></a><span class="lineno">  597</span>&#160;<span class="comment">  In order to compute a local feature for a rule, we only need to consider</span></div>
<div class="line"><a name="l00598"></a><span class="lineno">  598</span>&#160;<span class="comment">  that rule. For example, the number of target terminals in a rule is a</span></div>
<div class="line"><a name="l00599"></a><span class="lineno">  599</span>&#160;<span class="comment">  local feature. Let&#39;s add that feature:</span></div>
<div class="line"><a name="l00600"></a><span class="lineno">  600</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00601"></a><span class="lineno">  601</span>&#160;<span class="comment">    + In the `uk.ac.cam.eng.rulebuilding.features` package, create</span></div>
<div class="line"><a name="l00602"></a><span class="lineno">  602</span>&#160;<span class="comment">    a new class called `NumberTargetElements` . Its featureName</span></div>
<div class="line"><a name="l00603"></a><span class="lineno">  603</span>&#160;<span class="comment">    field can be for example &quot;number_target_elements&quot;.</span></div>
<div class="line"><a name="l00604"></a><span class="lineno">  604</span>&#160;<span class="comment">    + This class should implement the `Feature` class from the same</span></div>
<div class="line"><a name="l00605"></a><span class="lineno">  605</span>&#160;<span class="comment">    package.</span></div>
<div class="line"><a name="l00606"></a><span class="lineno">  606</span>&#160;<span class="comment">    + Implement the required methods for that class. You can</span></div>
<div class="line"><a name="l00607"></a><span class="lineno">  607</span>&#160;<span class="comment">    look at the WordInsertionPenalty class in the same package for an example.</span></div>
<div class="line"><a name="l00608"></a><span class="lineno">  608</span>&#160;<span class="comment">    + In the constructor of the FeatureCreator class in the same package,</span></div>
<div class="line"><a name="l00609"></a><span class="lineno">  609</span>&#160;<span class="comment">    add the following line:</span></div>
<div class="line"><a name="l00610"></a><span class="lineno">  610</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00611"></a><span class="lineno">  611</span>&#160;<span class="comment">        features.put(&quot;number_target_elements&quot;, new NumberTargetElements());</span></div>
<div class="line"><a name="l00612"></a><span class="lineno">  612</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00613"></a><span class="lineno">  613</span>&#160;<span class="comment">    + Modify the `--features` option to include `number_target_elements`</span></div>
<div class="line"><a name="l00614"></a><span class="lineno">  614</span>&#160;<span class="comment">    in the comma-separated list of features. If you follow the tutorial</span></div>
<div class="line"><a name="l00615"></a><span class="lineno">  615</span>&#160;<span class="comment">    commands, you can modify the `configs/CF.rulextract.retrieval` configuration</span></div>
<div class="line"><a name="l00616"></a><span class="lineno">  616</span>&#160;<span class="comment">    file.</span></div>
<div class="line"><a name="l00617"></a><span class="lineno">  617</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00618"></a><span class="lineno">  618</span>&#160;<span class="comment">  If you get stuck, you can see what modifications are needed</span></div>
<div class="line"><a name="l00619"></a><span class="lineno">  619</span>&#160;<span class="comment">  [here](https://github.com/ucam-smt/ucam-smt/commit/09f697d1e7e6f35e9e04c37e3f00b0c7780b6d67)</span></div>
<div class="line"><a name="l00620"></a><span class="lineno">  620</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00621"></a><span class="lineno">  621</span>&#160;<span class="comment">  \subsection mapreduce_feature Adding a MapReduce Feature</span></div>
<div class="line"><a name="l00622"></a><span class="lineno">  622</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00623"></a><span class="lineno">  623</span>&#160;<span class="comment">  We now give instructions on how to add a MapReduce feature.</span></div>
<div class="line"><a name="l00624"></a><span class="lineno">  624</span>&#160;<span class="comment">  In order to compute a MapReduce feature, we need to consider</span></div>
<div class="line"><a name="l00625"></a><span class="lineno">  625</span>&#160;<span class="comment">  all rules extracted from the training data rather than</span></div>
<div class="line"><a name="l00626"></a><span class="lineno">  626</span>&#160;<span class="comment">  a single rule at a time. For example, let&#39;s add source-to-target</span></div>
<div class="line"><a name="l00627"></a><span class="lineno">  627</span>&#160;<span class="comment">  and provenance source-to-target probabilities with add-one smoothing:</span></div>
<div class="line"><a name="l00628"></a><span class="lineno">  628</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00629"></a><span class="lineno">  629</span>&#160;<span class="comment">    + In the `uk.ac.cam.eng.extraction.hadoop.features.phrase` package, create</span></div>
<div class="line"><a name="l00630"></a><span class="lineno">  630</span>&#160;<span class="comment">        a new class called `Source2TargetAddOneSmoothedJob` . This class</span></div>
<div class="line"><a name="l00631"></a><span class="lineno">  631</span>&#160;<span class="comment">        is very similar to `Source2TargetJob` except that the mapper</span></div>
<div class="line"><a name="l00632"></a><span class="lineno">  632</span>&#160;<span class="comment">        adds one to the rule counts.</span></div>
<div class="line"><a name="l00633"></a><span class="lineno">  633</span>&#160;<span class="comment">    + Modify the `MapReduceFeature` class to add the new feature.</span></div>
<div class="line"><a name="l00634"></a><span class="lineno">  634</span>&#160;<span class="comment">    + Follow the steps to add a local feature.</span></div>
<div class="line"><a name="l00635"></a><span class="lineno">  635</span>&#160;<span class="comment">    + Change the `--mapreduce_feature` option to be the following:</span></div>
<div class="line"><a name="l00636"></a><span class="lineno">  636</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00637"></a><span class="lineno">  637</span>&#160;<span class="comment">        --mapreduce_features=source2target_probability,target2source_probability,provenance_source2target_probability,provenance_target2source_probability,source2target_addonesmoothed_probability,provenance_source2target_addonesmoothed_probability</span></div>
<div class="line"><a name="l00638"></a><span class="lineno">  638</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00639"></a><span class="lineno">  639</span>&#160;<span class="comment">    + For merging, change the `--input_features` option to be the following:</span></div>
<div class="line"><a name="l00640"></a><span class="lineno">  640</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00641"></a><span class="lineno">  641</span>&#160;<span class="comment">        --input_features=RUEN-WMT13/s2t,RUEN-WMT13/t2s,RUEN-WMT13/s2taddone</span></div>
<div class="line"><a name="l00642"></a><span class="lineno">  642</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00643"></a><span class="lineno">  643</span>&#160;<span class="comment">    + Since there is a new feature, you need to run a command analogous to</span></div>
<div class="line"><a name="l00644"></a><span class="lineno">  644</span>&#160;<span class="comment">    the one run to obtain source-to-target probabilities</span></div>
<div class="line"><a name="l00645"></a><span class="lineno">  645</span>&#160;<span class="comment">    + For retrieval, modify the `--mapreduce_features` and the `--features`</span></div>
<div class="line"><a name="l00646"></a><span class="lineno">  646</span>&#160;<span class="comment">    options to be as follows:</span></div>
<div class="line"><a name="l00647"></a><span class="lineno">  647</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00648"></a><span class="lineno">  648</span>&#160;<span class="comment">        --mapreduce_features=source2target_probability,target2source_probability,provenance_source2target_probability,provenance_target2source_probability,source2target_addonesmoothed_probability,provenance_source2target_addonesmoothed_probability,source2target_lexical_probability,target2source_lexical_probability,provenance_source2target_lexical_probability,provenance_target2source_lexical_probability</span></div>
<div class="line"><a name="l00649"></a><span class="lineno">  649</span>&#160;<span class="comment">        --features=source2target_probability,target2source_probability,word_insertion_penalty,rule_insertion_penalty,glue_rule,insert_scale,rule_count_1,rule_count_2,rule_count_greater_than_2,source2target_lexical_probability,target2source_lexical_probability,provenance_source2target_probability,provenance_target2source_probability,provenance_source2target_lexical_probability,provenance_target2source_lexical_probability,source2target_addonesmoothed_probability,provenance_source2target_addonesmoothed_probability</span></div>
<div class="line"><a name="l00650"></a><span class="lineno">  650</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00651"></a><span class="lineno">  651</span>&#160;<span class="comment">  The code modifications are [here](https://github.com/ucam-smt/ucam-smt/commit/7cba14a596f5e428ce69fe2620e411ecfe0e8d71).</span></div>
<div class="line"><a name="l00652"></a><span class="lineno">  652</span>&#160;<span class="comment"></div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="Tutorial_807_8rulextract_8md.html">Tutorial.07.rulextract.md</a></li>
    <li class="footer">Generated on Thu Aug 28 2014 20:06:33 for Cambridge SMT System by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.4 </li>
  </ul>
</div>
</body>
</html>
