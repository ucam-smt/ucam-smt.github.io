<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.3"/>
<title>Cambridge SMT System: Cambridge SMT Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.3 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Cambridge SMT Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="Intro"></a>
Introduction</h1>
<p>This tutorial presents various tools and techniques developed in the <a href="http://divf.eng.cam.ac.uk/smt">Statistical Machine Translation</a> group at the <a href="http://eng.cam.ac.uk">Cambridge University Engineering Department</a>.</p>
<p>The tutorial is intended to serve as a guide for the use of the tools, but our research publications contain the best descriptions of the algorithms and modelling techniques described here. The most relevant publications for this tutorial are listed below (<a class="el" href="index.html#Refs">Reading Material</a>). In particular, this tutorial is based on the Russian-English SMT system developed for the WMT 2013 evaluation - we suggest reading the system description [<a class="el" href="index.html#Pino2013">Pino2013</a>] before starting on the tutorial.</p>
<p>Our complete publications can be found at <a href="http://divf.eng.cam.ac.uk/smt/Main/SmtPapers">http://divf.eng.cam.ac.uk/smt/Main/SmtPapers</a>.</p>
<p>HiFST grew out of the Ph.D. thesis work of Gonzalo Iglesias.</p>
<p>Contributors to this release are:</p>
<ul>
<li>Graeme Blackwood</li>
<li>Bill Byrne</li>
<li>Adria de Gispert</li>
<li>Federico Flego</li>
<li>Gonzalo Iglesias</li>
<li>Juan Pino</li>
<li>Rory Waite</li>
<li>Tong Xiao</li>
</ul>
<p>with thanks to Cyril Allauzen and Michael Riley.</p>
<h2><a class="anchor" id="intro_features"></a>
Features Included in this Release</h2>
<ul>
<li>HiFST &ndash; Hierarchical phrase-based statistical machine translation system based on <a class="el" href="index.html#OpenFst">OpenFst</a></li>
<li>Direct production of translation lattices as Weighted Finite State Automata</li>
<li>Efficient WFSA rescoring procedures</li>
<li><a class="el" href="index.html#OpenFst">OpenFst</a> wrappers for direct inclusion of <a class="el" href="index.html#KenLM">KenLM</a> and ARPA language models as WFSAs</li>
<li>Lattice Minimum Bayes Risk decoding</li>
<li>Lattice Minimum Error Rate training</li>
<li>Tutorial for Hiero translation using Recursive Transition Networks and Pushdown Transducers</li>
<li>Client/Server mode</li>
<li>Shallow-N translation grammars</li>
<li>Source-sentence `chopping' procedures</li>
<li>WFSA true-casing</li>
<li>...</li>
</ul>
<h1><a class="anchor" id="Refs"></a>
Reading Material</h1>
<h2><a class="anchor" id="Refs_decoding"></a>
HiFST, HiPDT and Hierarchical Phrase-Based Decoding</h2>
<p><a class="anchor" id="deGispert2010"></a>[deGispert2010] <em>Hierarchical phrase-based translation with weighted finite state transducers and Shallow-N grammars</em>. <br/>
 A. de Gispert, G. Iglesias, G. Blackwood, E. R. Banga, and W. Byrne. Computational Linguistics, 36(3). 2010. <br/>
 <a href="http://aclweb.org/anthology/J/J10/J10-3008.pdf">http://aclweb.org/anthology/J/J10/J10-3008.pdf</a></p>
<p><a class="anchor" id="Allauzen2014"></a>[Allauzen2014] <em>Pushdown automata in statistical machine translation</em>. <br/>
 C. Allauzen, W. Byrne, A. de Gispert, G. Iglesias, and M. Riley. Computational Linguistics. 2014. To appear.<br/>
 <a href="http://mi.eng.cam.ac.uk/~wjb31/ppubs/cl2013.final.pdf">http://mi.eng.cam.ac.uk/~wjb31/ppubs/cl2013.final.pdf</a></p>
<p><a class="anchor" id="Iglesias2009"></a>[Iglesias2009] <em>Hierarchical phrase-based translation with weighted finite state transducers.</em><br/>
 G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne. Proceedings of HLT. 2009.<br/>
 <a href="http://aclweb.org/anthology//N/N09/N09-1049.pdf">http://aclweb.org/anthology//N/N09/N09-1049.pdf</a> <br/>
 <a href="http://mi.eng.cam.ac.uk/~wjb31/ppubs/naaclhlt2009presentation.pdf">http://mi.eng.cam.ac.uk/~wjb31/ppubs/naaclhlt2009presentation.pdf</a></p>
<p><a class="anchor" id="Iglesias2011"></a>[Iglesias2011] <em>Hierarchical Phrase-based Translation Representations</em>. <br/>
 G. Iglesias, C. Allauzen, W. Byrne, A. de Gispert, M. Riley. Proceedings of EMNLP. 2011. <br/>
 <a href="http://aclweb.org/anthology/D/D11/D11-1127.pdf">http://aclweb.org/anthology/D/D11/D11-1127.pdf</a></p>
<p><a class="anchor" id="Iglesias2009"></a>[Iglesias2009] <em>Rule filtering by pattern for efficient hierarchical translation</em>.<br/>
 G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne. Proceedings of EACL. 2009. <br/>
 <a href="http://aclweb.org/anthology/E/E09/E09-1044.pdf">http://aclweb.org/anthology/E/E09/E09-1044.pdf</a> <br/>
</p>
<p><a class="anchor" id="Chiang2007"></a>[Chiang2007] <em>Hierarchical phrase-based translation</em>.<br/>
 Computational Linguistics. 2007 <br/>
 <a href="http://aclweb.org/anthology/J07-2003.pdf">http://aclweb.org/anthology/J07-2003.pdf</a></p>
<h2><a class="anchor" id="Refs_systems"></a>
CUED SMT System Descriptions</h2>
<p><a class="anchor" id="Pino2013"></a>[Pino2013] <em>The University of Cambridge Russian-English System at WMT13</em>. <br/>
 J. Pino, A. Waite, T. Xiao, A. de Gispert, F. Flego, and W. Byrne. Proceedings of the Eighth Workshop on Statistical Machine Translation. 2013. <br/>
 <a href="http://aclweb.org/anthology//W/W13/W13-2225.pdf">http://aclweb.org/anthology//W/W13/W13-2225.pdf</a></p>
<h2><a class="anchor" id="Refs_fsts"></a>
OpenFST and Related Modelling Techniques</h2>
<p><a class="anchor" id="OpenFst"></a>[OpenFst] The OpenFST Toolkit <a href="http://www.openfst.org/">http://www.openfst.org/</a></p>
<p><a class="anchor" id="Roark2011"></a>[Roark2011] <em>Lexicographic semirings for exact automata encoding of sequence models.</em> <br/>
 B. Roark, R. Sproat, and I. Shafran. Proceedings of ACL-HLT. 2011. <br/>
 <a href="http://aclweb.org/anthology/P/P11/P11-2001.pdf">http://aclweb.org/anthology/P/P11/P11-2001.pdf</a></p>
<h2><a class="anchor" id="Refs_lmbr"></a>
Lattice Minimum Bayes Risk Decoding using WFSAs</h2>
<p><a class="anchor" id="BlackwoodPhD"></a>[BlackwoodPhD] <em>Lattice rescoring methods for statistical machine translation</em>.<br/>
 G. Blackwood. Ph.D. Thesis. Cambridge University Engineering Department and Clare College. 2010. <br/>
 <a href="http://mi.eng.cam.ac.uk/~gwb24/publications/phd.thesis.pdf">http://mi.eng.cam.ac.uk/~gwb24/publications/phd.thesis.pdf</a></p>
<p><a class="anchor" id="Blackwood2010"></a>[Blackwood2010] <em>Efficient path counting transducers for minimum Bayes-risk decoding of statistical machine translation lattices</em>.<br/>
 G. Blackwood, A. de Gispert, W. Byrne. Proceedings of ACL Short Papers. 2010. <br/>
 <a href="http://aclweb.org/anthology//P/P10/P10-2006.pdf">http://aclweb.org/anthology//P/P10/P10-2006.pdf</a></p>
<p><a class="anchor" id="Allauzen2010"></a>[Allauzen2010] <em>Expected Sequence Similarity Maximization</em>. <br/>
 C. Allauzen, S. Kumar, W. Macherey, M. Mohri, M Riley. Proceedings of HLT-NAACL, 2010. <br/>
 <a href="http://aclweb.org/anthology//N/N10/N10-1139.pdf">http://aclweb.org/anthology//N/N10/N10-1139.pdf</a></p>
<h2><a class="anchor" id="lmert_refs"></a>
Lattice Mert</h2>
<p><a class="anchor" id="Macherey2008"></a>[Macherey2008] <em>Lattice-based Minimum Error Rate Training for Statistical Machine Translation</em>. <br/>
 W. Macherey, F. Och, I. Thayer, J. Uszkoreit. Proceedings of EMNLP, 2008. <br/>
 <a href="http://aclweb.org/anthology/D/D08/D08-1076.pdf">http://aclweb.org/anthology/D/D08/D08-1076.pdf</a></p>
<p><a class="anchor" id="Waite2012"></a>[Waite2012] <em>Lattice-based minimum error rate training using weighted finite-state transducers with tropical polynomial weights.</em> <br/>
 A. Waite, G. Blackwood, and W. Byrne. Proceedings of FSMNLP, 2012.<br/>
 <a href="http://aclweb.org/anthology-new/W/W12/W12-6219.pdf">http://aclweb.org/anthology-new/W/W12/W12-6219.pdf</a></p>
<h2><a class="anchor" id="othertools"></a>
Language Modelling Toolkits</h2>
<p><a class="anchor" id="SRILM"></a>[SRILM] SRI Language Model Toolkit<br/>
 <a href="http://www.speech.sri.com/projects/srilm/">http://www.speech.sri.com/projects/srilm/</a></p>
<p><a class="anchor" id="KenLM"></a>[KenLM] The KenLM Toolkit<br/>
 <a href="http://kheafield.com/code/kenlm/">http://kheafield.com/code/kenlm/</a></p>
<h1><a class="anchor" id="general"></a>
Overview</h1>
<h2><a class="anchor" id="build"></a>
Installation</h2>
<p>The code can be cloned from the following GitHub address: </p>
<pre class="fragment">&gt; git clone https://github.com/ucam-smt/ucam-smt.git
</pre><p>Once downloaded, go into the cloned directory and run this command: </p>
<pre class="fragment">&gt; ./build-tests.sh
</pre><p>This should download and install necessary dependencies, compile the code and run tests. The <code>README.md</code> in the cloned directory also contains useful information for the installation.</p>
<p>Files for this tutorial can be downloaded from the following GitHub address: </p>
<pre class="fragment">&gt; git clone https://github.com/ucam-smt/demo-files.git
&gt; gunzip wmaps/*.gz  ## Uncompress big wordmap files.
</pre><p>There are additional Supplementary Files which can be downloaded from <a href="http://mi.eng.cam.ac.uk/~wjb31/data/hifst.release.May14/">http://mi.eng.cam.ac.uk/~wjb31/data/hifst.release.May14/</a> .</p>
<h2><a class="anchor" id="paths"></a>
Paths and Environment Variables</h2>
<p>The following instructions are for the Bash shell.</p>
<p>In the following, <code>HiFSTROOT</code> designates the cloned directory, i.e. the following should be a complete path to the cloned directory </p>
<pre class="fragment">&gt; export HiFSTROOT=complete_path_to_hifst_cloned_directory
</pre><p>After HiFST is successfully built and tested, the file $HiFSTROOT/Makefile.inc will contain environment variable settings needed to run the HiFST binaries and the OpenFST tools using the HiFST libraries. To set these, simply run </p>
<pre class="fragment">&gt; source $HiFSTROOT/Makefile.inc
&gt; export PATH=$HiFSTROOT/bin:$OPENFST_BIN:$PATH
&gt; export LD_LIBRARY_PATH=$HiFSTROOT/bin:$OPENFST_LIB
</pre><p>You should make sure that $HiFSTBINDIR is added first on the path and the library path and that it preceeds the OpenFst directories. If the LD_LIBRARY_PATH variable is not set correctly, you will see the message </p>
<pre class="fragment">ERROR: GenericRegister::GetEntry : tropical_LT_tropical-arc.so: cannot open shared object file: No such file or directory
ERROR: ReadFst : unknown arc type "tropical_LT_tropical" : standard input
</pre><h2><a class="anchor" id="Setup_files"></a>
Directory Structure</h2>
<p>The following directories contain the data files, configuration files, and model files needed for this tutorial. </p>
<pre class="fragment"> ./
 |-configs/ # Configuration files
 |-EN/      # English reference text
 |-G/       # Translation grammars
 |-M/       # Language models 
 |-RU/      # Russian input text
 |-scripts/ # Scripts for these demonstration exercises
 |-wmaps/   # Word maps, to map English and Russian text to integers
</pre><p>The following directories will be created after running this tutorial. </p>
<pre class="fragment"> ./
 |-log/     # Translation process log files
 |-output/  # Translation output, as 1-best hypotheses and lattices
</pre><h2><a class="anchor" id="Setup_configs"></a>
Configuration Files and Command Line Options</h2>
<p>As you work through the tutorial, please read the comments in the config files which explain some of the processing options. The following configuration files are provided for the tutorial. </p>
<pre class="fragment"> # baseline configuration: 4-gram LM and Shallow-1 translation grammar
 configs/CF.baseline : HiFST with 4-gram language model and a Shallow-1 grammar
 configs/CF.baseline.lmbr : lattice Minimum Bayes' Risk (LMBR) rescoring on top of baseline system
 configs/CF.baseline.outputnoprune : lattice output without pruning
 configs/CF.baseline.outputnoprune.lmrescore : lattice rescoring with language models
 # full Hiero grammar with 4-gram LM
 configs/CF.hiero : full Hiero grammar without pruning in search
 configs/CF.hiero.chopping : methods for dealing with long source sentences
 configs/CF.hiero.localprune  : full Hiero grammar with pruning in search 
 configs/CF.hiero.pdt : full Hiero grammar, decoding with push-down automata (HiPDT)
 # full, iterative lattice MERT script 
 configs/CF.lmert.alilats : Lattice MERT example, alignment lattices
 configs/CF.lmert.hyps : Lattice MERT example, initial hypotheses
 configs/CF.lmert.vecfea : Lattice MERT example, vector feature lattices
 # example feature generation for MERT and LMERT
 configs/CF.mert.alilats.nbest : MERT features, derivation-to-translation transducers, restricted to N-Best lists 
 configs/CF.mert.hyps : MERT features, initial hypotheses
 configs/CF.mert.vecfea.nbest  : MERT features, N-Best feature lists for MERT
 # misc
 configs/CF.recaser : recasing examples
 configs/CF.baseline.client : HiFST client-server example, client
 configs/CF.baseline.server : HiFST client-server example, server
</pre><p>HiFST uses the Boost libraries which provide support for <a href="http://www.boost.org/doc/libs/1_55_0/doc/html/program_options/overview.html">configuration files</a>.</p>
<p>Parameters can be supplied either on the command line or in the config files. For example, the following options could be provided on the command line: </p>
<pre class="fragment"> --hifst.prune=9 --hifst.replacefstbyarc.nonterminals=X,V 
</pre><p>Alternatively, they could be specified in a configuration file either as </p>
<pre class="fragment"> hifst.prune=9
 hifst.replacefstbyarc.nonterminals=X,V 
</pre><p>or as </p>
<pre class="fragment"> [hifst]
 prune=9
 replacefstbyarc.nonterminals=X,V 
</pre><h2><a class="anchor" id="wmaps"></a>
Word Maps and Integer Mapped Files</h2>
<p>HiFST uses <a href="http://www.openfst.org/twiki/bin/view/FST/FstAdvancedUsage#Symbol_Tables">symbol tables</a> as provided by <a class="el" href="index.html#OpenFst">OpenFst</a> to map between source and target language text and the integer representation used internally by the decoder. See the <a class="el" href="index.html#OpenFst">OpenFst</a> <a href="http://www.openfst.org/twiki/bin/view/FST/FstQuickTour">Quick Tour</a> for a discussion of the use of symbol tables.</p>
<p>Integer mappings for English and Russian are in the directory wmaps/ : </p>
<pre class="fragment"> wmaps/wmt13.en.wmap 
 wmaps/wmt13.en.all.wmap (a much larger version of wmaps/wmt13.en.wmap)
 wmaps/wmt13.ru.wmap
 wmaps/wmt13.ru.all.wmap (a much larger version of wmaps/wmt13.ru.wmap)
</pre><p>Note that HiFST reserves the integers 1 and 2 for the sentence-start and sentence-end symbols. 0 is the OpenFST epsilon symbol.</p>
<p>The format of the wordmap files is straightforward, e.g. </p>
<pre class="fragment"> &gt; head wmaps/wmt13.en.wmap 
 &lt;epsilon&gt;                0
 &lt;s&gt;                      1
 &lt;/s&gt;                     2
 the                      3
 ,                        4
 .                        5
 of                       6
 to                       7
 and                      8
 in                       9
</pre><p>Source text files are provided in integer format : </p>
<pre class="fragment"> RU/RU.set1.idx : integer mapped Russian text

 &gt; head -2 RU/RU.set1.idx 
 1 20870 2447 5443 50916 78159 3621 2
 1 1716 20196 95123 154 1049 6778 996 9 239837 7 1799 4 2
</pre><p>The <a href="http://openfst.org/twiki/bin/view/FST/FstExtensions">FAR</a> tools can be used to generate Russian text from the integer mapped files (see the discussion on <a class="el" href="md_Tutorial.html#basic_latshyps">Translation Lattices and 1-Best Hypotheses</a>). </p>
<pre class="fragment"> &gt; farcompilestrings --entry_type=line RU/RU.set1.idx | farprintstrings --symbols=wmaps/wmt13.ru.wmap | head -2
 &lt;s&gt; республиканская стратегия сопротивления повторному избранию обамы &lt;/s&gt;
 &lt;s&gt; лидеры республиканцев оправдывали свою политику необходимостью борьбы с фальсификациями на выборах . &lt;/s&gt;
</pre><h2><a class="anchor" id="lms"></a>
Language Models</h2>
<p>English 3-gram and 4-gram language models are provided in both <a href="http://kheafield.com/code/kenlm/">KenLM</a> and <a href="http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html">ARPA</a> formats . See [<a class="el" href="index.html#Pino2013">Pino2013</a>] for a description of how these LMs are built.</p>
<p>The following language models have restricted vocabulary corresponding to the target side of the rules that apply to the first few sentences of the tuning set RU.set1.idx . This is done so that the LMs are small and quickly and easily loaded into memory. <em>Do not use these models except for the first few sentences in this tutorial.</em> </p>
<pre class="fragment"> M/lm.3g.arpa.gz : Kneser-Ney 3-gram language model in ARPA format 
 M/lm.3g.mmap : Kneser-Ney 3-gram language model in KenLM format
 M/lm.4g.arpa.gz : Kneser-Ney 4-gram language model in ARPA format 
 M/lm.4g.mmap : Kneser-Ney 4-gram language model in KenLM format
 M/lm.4g.eprnd.mmap : entropy pruned KN 4-gram LM in KenLM format
 M/lm.tc.gz : true-casing language model
</pre><p>The following large LMs are available from a separate download site (see <a class="el" href="index.html#build">Installation</a>). It covers the target-side vocabulary for the large translation grammars, and is suitable for running on the complete tune and test set included in this tutorial. In particular, this second LM must be downloaded and uncompressed into the M/ directory prior to running the MERT and LMERT scripts and examples (see <a class="el" href="md_Tutorial.html#mert">MERT - Features Only</a> and <a class="el" href="md_Tutorial.html#lmert">Lattice MERT</a>). </p>
<pre class="fragment"> M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap : KN 4gram LM in KenLM format
 M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap : quantized KN 4gram LM in KenLM format
</pre><p>Language models are in integer mapped format, e.g. for the ARPA files: </p>
<pre class="fragment"> &gt; zcat M/lm.3g.arpa.gz | grep . | head -15
 \data\
 ngram 1=1348
 ngram 2=130054
 ngram 3=785733
 \1-grams:
 -1.0615243 &lt;unk&gt;
 -inf               &lt;s&gt;     -1.0853117
 -1.5690455 &lt;/s&gt;
 -2.2388144 12      -1.0949439
 -2.682596  11      -0.872226
 -4.0860014 1547    -0.66412055
 -2.4807615 14      -0.8686333
 -2.9167347 25      -0.7014704
 -2.599824  22      -0.6987488
 -2.6652465 26      -0.7175091
</pre><p>We also provide an <em>entropy pruned</em> [<a class="el" href="index.html#SRILM">SRILM</a>] version of the 4-gram language model as used for decoding with Push-Down Automata [<a class="el" href="index.html#Allauzen2014">Allauzen2014</a>] ; this is described below in <a class="el" href="md_Tutorial.html#pda">Push-Down Automata</a> . </p>
<pre class="fragment"> M/lm.4g.eprnd.arpa.gz : Entropy-pruned Kneser-Ney 4-gram language model in ARPA format 
 M/lm.4g.eprnd.mmap : Entropy-pruned Kneser-Ney 4-gram language model in KenLM format
</pre><h2><a class="anchor" id="tgrammars"></a>
Translation Grammars</h2>
<p>HiFST uses Synchronous Context-Free Grammars (SCFGs) for translation. A full Hiero and a Shallow-1 translation grammar are provided in the <code>G/</code> directory: </p>
<pre class="fragment"> G/rules.hiero.gz : full hiero grammar with scalar translation scores
 G/rules.shallow.gz : Shallow-1 hiero grammar with scalar translation scores
</pre><p>We also provide versions of these grammars with raw, unweighted feature scores: </p>
<pre class="fragment"> G/rules.hiero.vecfea.gz : full hiero grammar with feature vectors
 G/rules.shallow.vecfea.gz : Shallow-1 hiero grammar with feature vectors
</pre><p>For the tutorial on optimization (<a class="el" href="md_Tutorial.html#mert">MERT - Features Only</a>), larger grammars corresponding to the entire <code>RU/RU.tune.idx</code> tune set are provided: </p>
<pre class="fragment"> G/rules.shallow.all.gz : larger Shallow-1 hiero grammar with scalar translation scores
 G/rules.shallow.vecfea.allgz : larger Shallow-1 hiero grammar with feature vectors
</pre><p>There is also a grammar provided for the true-casing example (<a class="el" href="md_Tutorial.html#true_casing">Fst-based True casing</a>) </p>
<pre class="fragment">  G/tc.unimap
</pre><h3><a class="anchor" id="rules"></a>
Grammar File Formats</h3>
<p>In the grammar file, each line represents a rule. The rule format is: </p>
<pre class="fragment"> LHS RHS_SOURCE RHS_TARGET FEA_1 [FEA_2 FEA_3 FEA_4 ...]
</pre><p>where </p>
<pre class="fragment"> LHS = the left hand side of the rule
 RHS_SOURCE = the source-language part of the right hand side of the rule
 RHS_TARGET = the target-language part of the right hand side of the rule
 FEA_i = the i-th component of the feature vector associated with the rule
</pre><p>The left hand side of a rule is a non-terminal symbol (in uppercase). The right hand side is a pair of terminal and non-terminal symbol sequences in the source and target languages.</p>
<h4><a class="anchor" id="tgrammars_formats_fea"></a>
Feature Vectors</h4>
<p>Scores are assigned to rules as the dot product of a rule-specific feature vector and a weight vector (see the discussion in <a class="el" href="md_Tutorial.html#mert">MERT - Features Only</a>). This computation can be done offline, in which case the feature for every rule in the grammar is a 1-dimensional scalar. Alternatively, the decoder can be provided with a weight vector which is applied to the feature vectors while loading the grammar.</p>
<p>For example, the grammar <code>G/rules.shallow.gz</code> provided in this tutorial the following set of weights was found via LMERT tuning (see <a class="el" href="md_Tutorial.html#mert">MERT - Features Only</a> ): </p>
<pre class="fragment">0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
</pre><p>These can be applied to the Shallow-1 grammar, as follows: </p>
<pre class="fragment">&gt; WV=0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
&gt; zcat G/rules.shallow.vecfea.gz | scripts/weightgrammar -w=$WV | head -3
V 3 4 -2.046860955276
V 3 4_3 -1.884009085882
V 3 8 1.857985226112
</pre><p>and this should agree with the scalar-valued version of the grammar: </p>
<pre class="fragment">&gt; zcat G/rules.shallow.gz | head -3
V 3 4 -2.046860955276
V 3 4_3 -1.884009085882
V 3 8 1.857985226112
</pre><h4><a class="anchor" id="tgrammars_formats_nt"></a>
Non-Terminals</h4>
<p>In translation, a non-terminal <code>X</code> on the right hand side can be rewritten by any rule whose left hand side is <code>X</code>. HiFST places no restrictions on the definition of terminal and non-terminal sequences in an SCFG rule; similarly, there are no constraints on how many non-terminal symbols can be used in a rule (i.e. there are no constraints on order of the SCFG). However using a full Hiero grammar can lead to slow translation, and this tutorial discusses several strategies for pruning in translation and for translation grammar pruning.</p>
<p>Here is a small sample translation grammar </p>
<pre class="fragment"> M 434_M 1462_8_M -1.81842                
 M 7_M 9_3_M -0.735445                    
 M V V -0                                 
 S S_X S_X 0.05768                        
 S X X -0                                 
 V 10806 1411 1.16623                     
 V 164_M_60 78_M_8 -0.226464              
 V 164_M2_60_M1 78_M1_8_M2 -0.226464      
 V 21_591 39_258_8 -0.510102              
 V 24 3_54 -2.50252                       
 V 274_M_4 709_9_3_M -0.589246            
 V 5 6 -1.81729                           
 V 7_1689 9_741_8 0.438945                
 V 8 23 -1.46604                          
 X 1 1 -2.5598                            
 X 2 2 -2.5598                            
 X V V -0                                 
 D 1775 &lt;dr&gt; 10.4327                      
 S S_D_X S_D_X 0.11536                    
</pre><p>This grammar has four non-terminal symbols: <code>S</code>, <code>M</code>, <code>V</code> and <code>D</code>, and a 1-dimensional weight. The terminal symbols are integers, corresponding to words in the source and target language word maps (<a class="el" href="index.html#wmaps">Word Maps and Integer Mapped Files</a>). The symbol <code>&lt;dr&gt;</code> is a special symbol used by HiFST to represent an empty word, to indicate deletion.</p>
<p>As an example, for rule </p>
<pre class="fragment"> V 164_M_60 78_M_8 -0.226464
</pre><p>we have </p>
<pre class="fragment"> LHS        = V
 RHS_SOURCE = 164_M_60
 RHS_TARGET = 78_M_8
 WEIGHT_1   = -0.226464
</pre><p>With this rule the decoder can rewrite the non-terminal <code>V</code> by replacing it by <code>164_M_60</code> in the source language and by <code>78_M_8</code> in the target language; the rule is applied with a score of -0.226464. Similarly, rule <code>V 5 6 -1.81729</code> replaces <code>V</code> by the word "5" in the source-language and with the word "6" in the target-language, with a score of -1.81729.</p>
<p>Indices on non-terminals indicate alignment within rules with more than one non-terminal. For example, for rule </p>
<pre class="fragment">V 164_M2_6_M1 78_M1_8_M2 -0.226464
</pre><p>the <code>M</code> non-terminals on both language sides are indexed by 1 or 2; that is, <code>M1</code> in the source language is linked with <code>M1</code> in the target language, and <code>M2</code> in the source language is linked with <code>M2</code> in the target. Strictly speaking, <code>M2</code> is not a non-terminal in the above example: it is the second instance of the non-terminal <code>M</code> in the rule. Obviously this is a reordering rule.</p>
<p>In general we use different types of rule to distinguish different translation cases and obtain a finer-grained model. In this example, the <code>S</code> rules are doing something rather similar to the glue rules used in Hiero-style systems; the <code>M</code> rules can be regarded as monotonic translation rules; the <code>V</code> rules can be regarded as reordering translation rules and phrasal translation rules; and the <code>D</code> rule can be regarded as an explicit operation of word deletion.</p>
<p>Note that there is a straight-forward correspondence between the Hiero-style rule notation introduced by [<a class="el" href="index.html#Chiang2007">Chiang2007</a>] and the HiFST rule file format (for rules with one-dimensional weights). For example, the HiFST grammar file entries </p>
<pre class="fragment">V 164_M2_6_M1 78_M1_8_M2 -0.226464
S S_X S_X 0.05768
</pre><p>can be written as </p>
<pre class="fragment">V -&gt; &lt; 164 M2 6 M1 ,  78 M1 8 M2 &gt; / -0.226464
S -&gt; &lt; S X , S X &gt; / 0.05768
</pre><h3><a class="anchor" id="tgrammars_shallow"></a>
Shallow-N Translation Grammars</h3>
<p>Shallow grammars [<a class="el" href="index.html#deGispert2010">deGispert2010</a>] can be used to control the degree of nesting allowed within an hierarchical grammar. For example, for a Shallow-1 Grammar, variables in a rule can be substituted only by phrases. That is, hierarchical rules can be used only once to generate words; once a hierarchical rule is used, translation relies on glue rules to cover longer source spans.</p>
<p>Formally, we use W to denote the set of terminals, and S and X to denote two non-terminals. A simple Hiero-style grammar can be defined to be: </p>
<pre class="fragment"> S -&gt; X, X                                           
 S -&gt; S X, S X                                       
 X -&gt; a, b        where a, b \in ({X} union W)^+
</pre><p>We can transform this into a Shallow-1 grammar as </p>
<pre class="fragment"> S -&gt; X, X                                           
 S -&gt; S X, S X                                       
 Y -&gt; a, b        where a, b \in {W}^+               
 X -&gt; u, v        where u, v \in {{Y} union W}^+ 
</pre><p>Here <code>Y</code> is introduced to handle phrasal translations. The variables in rule <code>X -&gt; u, v</code> can only be substituted with the <code>Y</code> rules.</p>
<p>In some cases it is desirable to allow more complex movement in translation, such as complex structure movements in Chinese-English translation. For this we can use a generalisation of the simple Shallow-1 grammar, called Shallow-N grammars. These grammars allow hierarchical rules to be applied up to N times. For example, below is the form of a Shallow-2 grammar. </p>
<pre class="fragment"> S -&gt; X, X                                           
 S -&gt; S X, S X                                       
 Y^0 -&gt; a^0, b^0  where a^0, b^0 \in {W}^+           
 Y^1 -&gt; a^1, b^1  where a^1, b^1 \in {{Y^0} union W}^+ 
 X -&gt; u, v        where u, v \in {{Y^1} union W}^+     
</pre><p>The Shallow-2 grammar introduces <code>Y^0</code> and <code>Y^1</code> to handle hierarchical rule application at two levels within a derivation. For more detailed description of Shallow-n grammars, please refer to the HiFST paper [<a class="el" href="index.html#deGispert2010">deGispert2010</a>]. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sun May 25 2014 23:58:08 for Cambridge SMT System by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.3
</small></address>
</body>
</html>
