<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.3"/>
<title>Cambridge SMT System: MERT - Features Only</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.3 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('mert.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">MERT - Features Only </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This section describes how to generate N-Best lists of features for use with MERT [<a href="http://aclweb.org/anthology/P/P03/P03-1021.pdf">Och 2003</a>].</p>
<h1><a class="anchor" id="mert_nblists"></a>
HiFST_nbestformert</h1>
<p>A script named <code>scripts/HiFST_nbestformert</code> is provided which can generate hypotheses and feature vectors that can be used by MERT. We use N-Best lists of depth N = 100 , set by the <code>prunereferenceshortestpath=</code> option in <code>configs/CF.mert.alilats.nbest</code>.</p>
<p>For this tutorial, the configuration files specify multithreading, and N-Best lists will be generated only for the first 2 sentences in RU/RU.tune.idx ; the script can be edited to process the entire file. The output is written to the file <code>output/exp.mert/nbest/nbest.list</code>. </p>
<pre class="fragment">&gt; scripts/HiFST_nbestformert
&gt; head -2 output/exp.mert/nbest/nbest.list; echo ....; tail -2 output/exp.mert/nbest/nbest.list
1 ||| parliament does not support the amendment , which gives you the freedom of tymoshenko |||         62.5442     10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000 -1.0000 0.0000 -7.0000 16.3076 40.5293
1 ||| the parliament does not support the amendment , which gives you the freedom of tymoshenko ||| 63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000 -1.0000 0.0000 -7.0000 17.0010 43.9482
....
2 ||| the amendment , which has led to the release of which is in jail , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . ||| 135.8217 24.0928 50.8281 -37.0000 -21.0000 -12.0000 0.0000 -1.0000 0.0000 -20.0000 100.6992 111.6357
2 ||| the amendment , which would have led to the release of which is in the jail of former prime minister , was rejected during the second reading of the bill to ease sentences for economic offences . |||        145.1169         23.7305  41.4756  -39.0000       -23.0000        -14.0000 0.0000 -1.0000 0.0000 -22.0000 93.0488 107.7188
</pre><p>The steps carried out in the <code>HiFST_nbestformert</code> script are described next.</p>
<h1><a class="anchor" id="mert_hyps"></a>
1. Hypotheses for MERT</h1>
<p>(note that this step is also done in <a class="el" href="lmert.html">Lattice MERT</a>)</p>
<ul>
<li>Input:<ul>
<li><code>RU/RU.tune.idx</code> &ndash; tuning set source language sentences</li>
<li><code>G/rules.shallow.vecfea.all.gz</code> &ndash; translation grammar</li>
<li><code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code> &ndash; target language model</li>
<li>language model and translation grammar feature weights (see configs/CF.mert.hyps)</li>
</ul>
</li>
<li>Output<ul>
<li><code>output/exp.mert/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized</li>
</ul>
</li>
</ul>
<p>This step runs HiFST in the usual way to generate a set of translation hypotheses which will be used in MERT. Note that M (the number of sentences to translate) is set to 5, just to make the tutorial steps run quickly. </p>
<pre class="fragment">&gt; M=5
# replace by M=1502 to process the entire tuning set
&gt; hifst.O2 --config=configs/CF.mert.hyps --range=1:$M &amp;&gt; log/log.mert.hyps
</pre><p>In this configuration, the grammar feature weights and the language model feature weights are applied on-the-fly to the grammar and language model as they are loaded. This allows feature vector weights to be changed at each iteration of MERT. This behaviour is specified through the following options in the CF.mert.hyps file, where we use the parameters from the baseline system: </p>
<pre class="fragment">[lm]
load=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap
featureweights=1.0
# Note that for only one language model, this parameter will always be set to 1.
# If there are multiple language models, the language model weights will be updated
# after each iteration of MERT

[grammar]
load=G/rules.shallow.vecfea.all.gz
featureweights=0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
# Note that this parameter vector should be updated after each iteration of MERT
# Updated versions can be provided via command line arguments
</pre><p>The translation grammar has its rules with unweighted feature vectors: </p>
<pre class="fragment">&gt; zcat G/rules.shallow.vecfea.all.gz | head -n 3
V 3 4 0.223527 0.116794 -1 -1 0 0 0 0 -1 1.268789 0.687159
V 3 4_3 3.333756 0.338107 -2 -1 0 0 0 0 -1 1.662178 3.363062
V 3 8 3.74095 3.279819 -1 -1 0 0 0 0 -1 3.741382 2.271445
</pre><p>The output lattices in <code>output/exp.mert/LATS</code> are acceptors containing word hypotheses, with weights in the form of the lexicographic semiring as described earlier. </p>
<pre class="fragment">&gt; zcat output/exp.mert/LATS/1.fst.gz | fstinfo | head -n 2
fst type                                          vector
arc type                                          tropical_LT_tropical

&gt; zcat output/exp.mert/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.all.wmap -w 2&gt;/dev/null
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      43.093,-19.4512
</pre><h1><a class="anchor" id="mert_nblist_derivations"></a>
2. Guided Translation / Forced Alignment</h1>
<ul>
<li>Input:<ul>
<li><code>RU/RU.tune.idx</code> &ndash; tuning set source language sentences</li>
<li><code>G/rules.shallow.all.gz</code> &ndash; translation grammar</li>
<li><code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code> &ndash; target language model</li>
<li><code>output/exp.mert/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized (from <a class="el" href="mert.html#mert_hyps">1. Hypotheses for MERT</a>)</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.mert/nbest/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations (e.g. Fig. 7, [<a class="el" href="index.html#deGispert2010">deGispert2010</a>])</li>
</ul>
</li>
</ul>
<p>Alignment is the task of finding the derivations (sequences of rules) that can produce a given translation. HiFST performs alignment via constrained translation (see Section 2.3, [<a class="el" href="index.html#deGispert2010">deGispert2010</a>] for a detailed description). This command runs HiFST in alignment mode: </p>
<pre class="fragment">&gt; hifst.O2 --config=configs/CF.mert.alilats.nbest --range=1:$M &amp;&gt; log/log.mert.alilats.nbest
</pre><p>In alignment mode, HiFST constructs <em>substring acceptors</em> (see Fig. 8, [<a class="el" href="index.html#deGispert2010">deGispert2010</a>]). These are constructed for each sentence as follows:</p>
<ul>
<li>the lattice from Step 1 is loaded by HiFST</li>
<li>an N-Best list in the form of a WFSA is extracted (using <code>fstshortestpath</code>) under the translation and language model score from Step 1</li>
<li>weights are removed from N-Best WFSA</li>
<li>the WFSA is transformed to a substring acceptor</li>
</ul>
<p>The translation grammar is applied in the usual way, but the translations are intersected with the substring acceptors so that only translations in the N-Best lists are retained. This generates every possible derivation of each N-Best list entry.</p>
<p>This behaviour is specified by the following config file parameters: </p>
<pre class="fragment">[referencefilter]
load=output/exp.mert/LATS/?.fst.gz
# perform alignment against these reference lattices containing initial hypotheses
prunereferenceshortestpath=100
# on loading the reference lattices, transform them to n-best lists prior to alignment.
# uses fstshortestpath
</pre><p>Note that application of the substring acceptors is very efficient; and this alignment step should be much faster than the translation operation of Step 1. The alignment lattices (referred to as ALILATS) map rule sequences (derivations) to translation hypotheses. Weights remain in lexicographic semiring form. </p>
<pre class="fragment">&gt; zcat output/exp.mert/nbest/ALILATS/1.fst.gz | fstinfo | head -n 2
fst type                                          vector
arc type                                          tropical_LT_tropical
</pre><p>Individual rules are identified by their line number in the translation grammar file. A rule map can be created as </p>
<pre class="fragment">&gt; zcat G/rules.shallow.all.gz | awk 'BEGIN{print "0\t0"}{printf "%s-&gt;&lt;%s,%s&gt;\t%d\n", $1, $2, $3, NR}'  &gt; G/rules.shallow.all.map
</pre><p>The ALILATS transducers are not determinised: they contain every possible derivation for each N-Best list entry. The following example prints some of the alternative derivations of the top-scoring hypothesis: </p>
<pre class="fragment">&gt; zcat output/exp.mert/LATS/1.fst.gz | fstshortestpath &gt; tmp/1.fst # properly, should remove arcweights
&gt; zcat output/exp.mert/nbest/ALILATS/1.fst.gz | fstcompose - tmp/1.fst | fstproject | printstrings.O2 --nbest=10 --semiring=lexstdarc -m G/rules.shallow.all.map 2&gt;/dev/null | head -n 2
S-&gt;&lt;1,1&gt; V-&gt;&lt;3526,50&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;28847,245&gt; X-&gt;&lt;10_1278_V,135_20_103_3_V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;3_64570,4_25_1145_48&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;1857,3_425_6&gt; X-&gt;&lt;V_7786,V_23899&gt; S-&gt;&lt;S_X,S_X&gt; X-&gt;&lt;2,&lt;/s&gt;&gt; S-&gt;&lt;S_X,S_X&gt;
S-&gt;&lt;1,1&gt; V-&gt;&lt;3526,50&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;28847,245&gt; X-&gt;&lt;10_1278_V,135_20_103_3_V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;3_64570,4_25_1145_48&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;1857,3_425_6&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;7786,23899&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; X-&gt;&lt;2,&lt;/s&gt;&gt; S-&gt;&lt;S_X,S_X&gt;
</pre><p>The order of the rules in these rule sequences correspond to HiFST's bottom-up (left-to-right) CYK grid structure. Rule IDs are added as input symbols to the component WFSTs in the RTN following the translation rule (with its non-terminals). This leads to the bottom-up ordering after Replacement.</p>
<h1><a class="anchor" id="mert_alilats"></a>
3. Hypotheses with Unweighted Feature Vectors</h1>
<ul>
<li>Input:<ul>
<li><code>G/rules.shallow.vecfea.all.gz</code> &ndash; translation grammar, rules with (unweighted) feature vectors</li>
<li><code>output/exp.mert/nbest/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations, for n-best entries (from <a class="el" href="mert.html#mert_nblist_derivations">2. Guided Translation / Forced Alignment</a>)</li>
<li>language model and translation grammar feature weights (see <code>configs/CF.mert.vecfea.nbest</code>)</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.mert/lats/VECFEA/?.nbest.gz</code> &ndash; N-best hypotheses</li>
<li><code>output/exp.mert/lats/VECFEA/?.vecfea.gz</code> &ndash; N-best unweighted features</li>
</ul>
</li>
</ul>
<p>The alilats2splats tool transforms ALILATS alignment lattices (transducers) to sparse vector weight lattices; see Section 2.3.1, [<a class="el" href="index.html#deGispert2010">deGispert2010</a>] for a detailed explanation. </p>
<pre class="fragment">&gt; alilats2splats.O2 --config=configs/CF.mert.vecfea.nbest --range=1:$M &amp;&gt; log/log.mert.nbest
</pre><p>The output is written to two sets of files:</p>
<p>N-best lists: </p>
<pre class="fragment"> &gt; head -n 2 output/exp.mert/nbest/VECFEA/1.nbest
 &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;     43.0904
 &lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 43.1757
</pre><p>Vecfea files: </p>
<pre class="fragment"> &gt; head -n 2 output/exp.mert/nbest/VECFEA/1.vecfea
 62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293
 63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482
</pre><ul>
<li>Line <code>n</code> in the nbest list is the `n-th' translation hypotheses, as ranked under the combined translation and language model scores.</li>
<li>Line <code>n</code> in the vecfea file is a vector obtained by summing the unweighted feature vectors of each rule in the best derivation of the <code>n-th</code> hypothesis</li>
</ul>
<p>The alilats2splats tool works as follows:</p>
<ul>
<li>The translation grammar with (unweighted) feature vectors is loaded</li>
<li>a Rule Flower acceptor, R, is created. This is an acceptor for rule sequences that applies vector weights (specifically, the feature vector for each rule). See Fig. 9 of [<a class="el" href="index.html#deGispert2010">deGispert2010</a>] for an example and an explanation.</li>
<li>For each source sentence, the ALILATS derivation-to-translation transducer from Step 2 is loaded, and its weights are removed. Call this T_u</li>
<li>The unweighted derivations-to-translation trandsducer T_u is composed with the Rule Flower acceptor R under the tropical sparse tuple weight semiring with the same feature vectors as are used to generate the translation.</li>
<li>The feature vector for the best scoring derivation for every translation is found as Determinise(Project_output(R o T_u) )</li>
<li>Language model scores M_1, ..., M_m are applied (again in the tropical sparse tuple weight semiring, so that each score ends up in a separate element in the vector) as Determinise(Project_output(R o T_u) ) o M_1 o ... o M_m</li>
</ul>
<p>Writing of N-Best lists and features is controlled by the <code>sparseweightvectorlattice</code> options <code>storenbestfile</code> and <code>storefeaturefile</code>: </p>
<pre class="fragment">[sparseweightvectorlattice]
loadalilats=output/exp.mert/nbest/ALILATS/?.fst.gz
storenbestfile=output/exp.mert/nbest/VECFEA/?.nbest
storefeaturefile=output/exp.mert/nbest/VECFEA/?.vecfea
wordmap=wmaps/wmt13.en.all.wmap
</pre><p>With the wordmap specified, the output of alilats2splats is in readable form in the target language. Note that the sentence boundary symbols and the combined translation and language model score appear in the nbest file. The N-best lists have the format</p>
<ul>
<li>wordindex1 wordindex2 ... translation_score</li>
</ul>
<p>The relationship of feature vectors and scores at the hypothesis level is as follows:</p>
<ul>
<li>Suppose there are m language models, with weights s_1 ...,s_m .<ul>
<li>These weights are specified by the HiFST parameters <code>lm.featureweights=s_1,s_2,..,s_m</code></li>
</ul>
</li>
<li>Suppose there are n dimensional feature vectors for each rule,<ul>
<li>The weights to be applied are specified by the HiFST parameters <code>grammar.featureweights=w_1,..,w_n</code></li>
</ul>
</li>
<li>A feature weight vector is formed as P = [s_1 ... s_m w_1 ... w_n]</li>
<li>A translation hypothesis e has a feature vector F(e) = [lm_1(e) ... lm_m(e) f_1(e) ... f_n(e)]<ul>
<li>lm_i(e): the i-th language model score for e</li>
<li>f_j(e): j-th grammar feature (see Section 3.2.1, [<a class="el" href="index.html#deGispert2010">deGispert2010</a>])</li>
</ul>
</li>
<li>The score of translation hypothesis e can be found as S(e) = F(e) . P (dot product)</li>
</ul>
<p>Each line k in the feature file has the format</p>
<ul>
<li>lm_1(e_k) ... lm_m(e_k) f_1(e_k) ... f_n(e_k)</li>
</ul>
<p>which are the unweighted feature values for the k-th hypothesis, e.g. </p>
<pre class="fragment"> &gt; head -n 2 output/exp.mert/nbest/VECFEA/1.vecfea
 62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293
 63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482
</pre><p>and the translation_score in line k is F(e_k) . P </p>
<pre class="fragment">&gt; head -n 2 output/exp.mert/nbest/VECFEA/1.nbest
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      43.0904
&lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;  43.1757</pre> </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Mon May 26 2014 02:37:40 for Cambridge SMT System by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.3 </li>
  </ul>
</div>
</body>
</html>
