<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Cambridge SMT System: Tutorial.080.bilingualmodels.md Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('Tutorial_8080_8bilingualmodels_8md.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Tutorial.080.bilingualmodels.md</div>  </div>
</div><!--header-->
<div class="contents">
<a href="Tutorial_8080_8bilingualmodels_8md.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;Rescoring with Bilingual Neural Network Models {#bilm}</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;=====</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;**Important Note:** In order to run this tutorial you will first need to clone \ref [our local copy of nplm](https://github.com/ucam-smt), and recompile ucam-smt package following the instructions in Makefile.inc.</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;In the following we describe how to rescore lattices</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;with bilingual models as described by [\ref Devlin2014].</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;Bilingual Neural Network models are simple feedforward models</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;with word embeddings on the first layer. They are trained with</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;bilingual samples extracted from the aligned parallel data.</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;The model M/nplm.s3t4 provided with this tutorial is trained</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;with [\ref NPLM] on samples that have 3 source words and 4 target words:</div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;</div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;     s1 s2 s3 t1 t2 t3 t4</div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;t1 t2 t3 is the target history; t4 is the prediction, which is _affiliated_ to s2, the center of</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;the window of 3 source words. The affiliation heuristic smooths</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;the word alignments so that there is exactly one link per target word</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;and a sample can be extracted for every target word.</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;More details in [\ref Devlin2014].</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;**Important Note:** In order to generate models that are consistent with ucam-smt tools, the training samples must be numberized. As numbers are mapped to different words in source and target, but the word embeddings are shared, we offset the source integers with an arbitrarily big number, so that NPLM word embeddings discriminate source words from target words. The running example uses an offset of 10000000. This must be consistent with the bilingual model application step.</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;During decoding or rescoring, we also need to determine the affiliations</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;for the words in each rule. These can be computed on-the-fly if internal</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;word alignments are available. We compute the affiliations offline</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;and define a grammar with affiliations for each rule.</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;     &gt; zcat G/grammar.affiliation.gz | grep _ | grep 1_0 | head -2</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;     V 3_939 1108_4 4.114964 3.335020 -2 -1 0 0 0 0 -1 5.198832 4.680031     1_0</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;     V 3_1600 5_569_14 4.179298 0.148420 -3 -1 0 0 0 0 -1 5.587708 6.762751  0_1_0</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;In rule V 3_939 1108_4,  1_0 means that word 1108 is aligned to 939 and 4 is aligned to 3.</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;In rule V 3_1600 5_569_14, 0_1_0 means that 5 and 14 are aligned to 3; 569 is aligned to 1600.</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;The following script contains a simple recipe that shows how to apply a bilingual</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;model to a translation lattice:</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;     &gt; scripts/runBiLMRescoring.bash</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;The script runs various steps necessary to achieve this goal, including the usual pipeline</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;comprising translation, alignment and feature vector lattice generation.</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;These are described respectively in:</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;* \ref basic_trans</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;* \ref mert_nblist_derivations</div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;* \ref mert_alilats</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;In the following we highlight the novel steps, specific to the bilingual model rescoring procedure.</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;**Running HiFST in affiliation mode**</div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;Running HiFST in affiliation mode is very similar to running in alignment mode, with the</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;difference that instead of rules we now have one source link for each target word.</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;With this additional information provided in the grammar, HiFST runs in affiliation mode as so:</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;     &gt; hifst.${TGTBINMK}.bin config/CF.hifst,config/CF.hifst-a,config/CF.lm --hifst.alilatsmode.type=affiliation --range=1:$M --featureweights=$FW --nthreads=15 --hifst.lattice.store=output/bilmexp/AFILATS/?.fst.gz</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;Affiliation Lattices have affiliation sequences on the input and words on the output. A translation hypothesis</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;has one or more affiliation sequences. For example:</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;     &gt; printstrings.O2 --input=output/bilmexp/AFILATS.STD/1.fst.gz  --print-input-output-labels -n 10 | grep &quot;1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2&quot;</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;     1 5 6 4 2 7 7 9 9 9 12 11 10 14         1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;     1 5 6 4 2 7 7 9 9 9 13 11 10 14         1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;For instance, word 9967 is linked to the 10th source word in both afiliation sequences.</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;But word 717 is linked to the 12th word in one case, and to the 13th word in the other.</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;**Affiliation Lattice Disambiguation**</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;We disambiguate these affiliation lattices with the `disambignffst` tool.</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;See \ref nfdisambiguation for more details on how this tool works.</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;**Composition of an Affiliation lattice with a Bilingual Neural Network Model.**</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;For the the bilingual model application we use the `applylm` tool described</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;in \ref rescoring_lm.</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;</div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;     &gt; applylm.${TGTBINMK}.bin --range=1:$M --nthreads=1 --lm.load=models/nplm.s3t4  --lm.featureweights=1  --lm.wps=0 --lattice.load=output/bilmexp/AFIDETLATS.0W/?.fst.gz --usebilm=yes --usebilm.sourcesize=3 --usebilm.sourcesentencefile=AR/mt02_05_tune.ara.special.idx --lattice.store=output/bilmexp/BLMONLY/?.fst.gz</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;The tool takes as input unweighted unambiguous lattices available in output/bilmexp/AFIDETLATS.0W/.</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;The output is a lattice with the bilingual model scores.</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;The program option `--usebilm=yes` enables bilingual composition. Note that an NPLM model is not aware of distinctions on the input between source and target; this has to be provided manually by the user with  `--usebilm.sourcesize=3`. Finally,  `--usebilm.sourcesentencefile=` points to a special version of the source file. Compare with the one used for HiFST:</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;    &gt; head -1 AR/mt02_05_tune.ara.special.10first.idx</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;    1 10000180 10000003 10000447 10000003 10001305 10006008 10000009 10001796 10006264 10022050 10000003 10000250 2</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;    &gt; head -1 AR/mt02_05_tune.ara.10first.idx</div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;    1 180 3 447 3 1305 6008 9 1796 6264 22050 3 250 2</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;The difference is the aforementioned offset to ensure that NPLM distinguishes source words from target words.</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;**Adding the bilingual feature to the feature vector lattices**</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;The script has a little function `addFeatureToVECFEA` that pipes OpenFst/ucam-smt tools to add the new feature</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;into the feature vector lattices. Recall that a feature vector lattice contains hypotheses with their individual feature contributions.</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;     &gt; FW=1.000000,0.820073,1.048347,0.798443,0.349793,0.286489,15.352371,-5.753633,-3.766533,0.052922,0.624889,-0.015877</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;     &gt; printstrings.O2 --input=output/bilmexp/VECFEA/1.fst.gz  --semiring=tuplearc --tuplearc.weights=$FW -w --sparseformat</div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;     1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2       0,9,1,55.5727,2,10.626,3,8.83496,4,-14,5,-8,6,-5,10,-8,11,29.29,12,31.3643</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;The highest feature index is 12. The new feature vector lattices contain the extra bilingual feature at position 13.</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;     &gt; printstrings.O2 --input=output/bilmexp/VECFEA+BLM/1.fst.gz  --semiring=tuplearc --tuplearc.weights=$FW,0 -w --sparseformat</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;     1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2       0,10,1,55.5727,2,10.626,3,8.83496,4,-14,5,-8,6,-5,10,-8,11,29.29,12,31.3643,13,33.4465</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;Note that the feature weights provided to the tool need an extra 0. Otherwise, the following message would appear:</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;     feature vector has a larger dimensionality than the parameters. Params: 12 Features: 13</div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;</div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;At this point, you only need to run `lmert` tool  to get a new set of weights, e.g. using as starting</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;parameters  $FW,0. See \ref lmert for more details.</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;Use `printstrings` tool to get the best hypotheses under the new weights.</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;</div></div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="Tutorial_8080_8bilingualmodels_8md.html">Tutorial.080.bilingualmodels.md</a></li>
    <li class="footer">Generated on Thu May 19 2016 11:21:36 for Cambridge SMT System by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
