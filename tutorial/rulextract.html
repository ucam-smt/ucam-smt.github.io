<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>Cambridge SMT System: Rule Extraction</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('rulextract.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Rule Extraction </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="rulextract_start_reminder"></a>
Getting started</h1>
<p>See (<a class="el" href="build.html#rulextract_install">Installation of the Hadoop-based Grammar Extraction Tools</a>)</p>
<h1><a class="anchor" id="rulextract_cluster_setup"></a>
Hadoop Cluster Setup</h1>
<p><b>Note</b>: a user already having access to a Hadoop cluster may wish to skip this section, after adding these dependencies to the <code>HADOOP_CLASSPATH</code> : <code>jcommander-1.35</code>, <code>hbase-0.92.0</code> and <code>guava-r09</code> .</p>
<p><b>Note</b>: we use Hadoop 1 as opposed to Hadoop 2 (see <a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">this discussion</a>). We also use the more recent API of Hadoop, which means that in general the import statements use the <code>org.apache.hadoop.mapreduce</code> package instead of the <code>org.apache.hadoop.mapred</code> package.</p>
<p>We give instructions on how to set up a single node Hadoop cluster. Specifically, we follow instructions for the pseudo-distributed single node setup from <a href="http://hadoop.apache.org/docs/r1.2.1/single_node_setup.html">http://hadoop.apache.org/docs/r1.2.1/single_node_setup.html</a> . More information is available at <a href="http://hadoop.apache.org">http://hadoop.apache.org</a> and in the book "Hadoop, The Definitive Guide" by Tom White.</p>
<p>First, choose a working directory, for example <code>$HOME/hadoopcluster</code>, then run the following commands: </p>
<pre class="fragment">&gt; mkdir -p $HOME/hadoopcluster
&gt; cd $HOME/hadoopcluster
&gt; $RULEXTRACT/scripts/hadoopClusterSetup.bash
</pre><p>This should install the cluster in the <code>$HOME/hadoopcluster/hadoop-1.2.1</code> directory. In the remainder of this tutorial, the <code>$HADOOP_ROOT</code> variable designates the Hadoop installation directory: </p>
<pre class="fragment">&gt; HADOOP_ROOT=$HOME/hadoopcluster/hadoop-1.2.1
</pre><p>We now detail the steps in the <code>hadoopClusterSetup.bash</code> script. You can also have a look at the commands and comments inside the script for more information.</p>
<ul>
<li>The java version is checked. If java 1.7+ is not installed, then a recent version of jdk is downloaded in the current directory, specifically jdk1.8.0_05 .</li>
<li>A recent version of Hadoop is downloaded, specifically version 1.2.1 .</li>
<li>Libraries on which the code is dependent are downloaded.</li>
<li>The configuration files in the Hadoop directory are modified to allow pseudo-distributed mode and point to the correct <code>JAVA_HOME</code> . The <code>HADOOP_CLASSPATH</code> is also modified to point to libraries that the code depends on.</li>
<li>Passwordless and passphraseless ssh is set. This is to make sure that the command <code>ssh localhost</code> works without any password or passphrase prompt.</li>
<li>The Hadoop Distributed File System (HDFS) is formatted.</li>
<li>Hadoop deamons are started. When this is done, you should be able to check the status of HDFS and MapReduce with a browser at the <code>localhost:50070</code> and <code>localhost:50030</code> respective addresses.</li>
<li>The HDFS <code>ls</code> command is tested.</li>
<li>The directory for your username (<code>/user/$USER</code>) is created. Is is better to store your HDFS data in that directory rather than the root directory or the <code>/tmp</code> directory.</li>
<li>The cluster is shut down to avoid having java processes lying around. You will need to restart the cluster to run MapReduce jobs with the following command: <pre class="fragment">&gt; $HADOOP_ROOT/bin/start-all.sh
</pre></li>
</ul>
<p>Once you are done with this tutorial, you can shut down the Hadoop cluster with this command: </p>
<pre class="fragment">  &gt; $HADOOP_ROOT/bin/stop-all.sh
</pre><p>When running rule extraction commands, if you see a similar looking log message: </p>
<pre class="fragment">  14/07/09 16:56:55 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
</pre><p>this means that the Hadoop cluster is not running and needs to be (re)started.</p>
<p>Note that this Hadoop cluster installation is for tutorial purposes. If you have a multi-core machine and enough memory (say 16G-32G), then this cluster may be sufficient for extracting relatively large grammars. However, a proper installation will use several nodes and a different username for the Hadoop administrator.</p>
<p>After running the installation script, if you still run into trouble while running the rule extraction commands, you may run the following commands as a last resort (note that this will delete all your HDFS data): </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/stop-all.sh
&gt; rm -rf /tmp/hadoop*
&gt; $HADOOP_ROOT/bin/hadoop namenode -format
    &gt; $HADOOP_ROOT/bin/start-all.sh
</pre><h1><a class="anchor" id="rulextract_pipeline_overview"></a>
Pipeline Overview</h1>
<p>Grammar extraction is composed of two main steps: extraction and retrieval.</p>
<p>Extraction consists in extracting all phrase-based and hierarchical rules from word-aligned parallel text and computing model scores for these rules. Extraction is itself decomposed into the following steps:</p>
<ul>
<li>training data loading: the training data, i.e. word aligned parallel text, is loaded onto HDFS.</li>
<li>extraction: rules are simply extracted, no scores are computed</li>
<li>model score computation: scores are computed for the extracted rules. Currently, source-to-target and target-to-source probabilities are computed.</li>
<li>merging: the various outputs from the previous step (score computation) are merged so that a rule can be associated to multiple scores.</li>
</ul>
<p>Rule retrieval, or rule filtering, consists in obtaining rules and scores that are only relevant to a given input test set or a given input sentence to be translated. Rule retrieval is decomposed into the following steps:</p>
<ul>
<li>launch lexical probability servers: lexical features are computed with a client/server architecture rather than with MapReduce because lexical models take a fair amount of memory.</li>
<li>rule retrieval: rules relevant to a test set are looked up in the HFiles produced by the extraction phase.</li>
<li>grammar formatting: a grammar with a format suitable for the HiFST decoder is produced.</li>
</ul>
<p>The next section details the various steps for grammar extraction.</p>
<h1><a class="anchor" id="rulextract_grammar_extraction"></a>
Grammar Extraction</h1>
<h2><a class="anchor" id="rulextract_commands"></a>
Running Commands</h2>
<p>In the remainder of this tutorial, it is assumed that commands are run from the <code>$DEMO</code> directory. Please change to that directory: </p>
<pre class="fragment">&gt; cd $DEMO
</pre><p>All commands look like: </p>
<pre class="fragment"> &gt; $HADOOP_ROOT/bin/hadoop jar $RULEXTRACTJAR class args
</pre><p>where <code>class</code> is a particular java class with a main method and <code>args</code> are the command line arguments. We use <a href="http://jcommander.org/">JCommander</a> for command line argument parsing. You can therefore put all arguments in a configuration file <code>config</code> and use the syntax <code>@config</code> to replace the command line arguments (see the <a href="http://jcommander.org/#Syntax">@-syntax</a>).</p>
<p>Create a directory to store logs: </p>
<pre class="fragment">&gt; mkdir -p logs
</pre><p>After running rule extraction commands that produce an output on HDFS, you can visualize the output using either the SequenceFile printer or the HFile printer provided. For example, after having run the extraction step (see below), you can see the extracted rules as follows: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.util.SequenceFilePrint \
    RUEN-WMT13/rules/part-r-00000 2&gt;/dev/null
</pre><p>This will print the first chunk of rules extracted. After the merging step, you can visualize rules, alignments and features as follows: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.util.HFilePrint \
    RUEN-WMT13/merge/part-r-00000.hfile 2&gt;/dev/null
</pre><h2><a class="anchor" id="rulextract_load_data"></a>
Data Loading</h2>
<p>The first step in grammar extraction is to load the training data onto HDFS. This is done via the following command: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.util.ExtractorDataLoader \
    @configs/CF.rulextract.load \
    &gt;&amp; logs/log.loaddata
</pre><p>You can see the following options in the <code>configs/CF.rulextract.load</code> configuration file:</p>
<ul>
<li><code>--source</code> : a gzipped text file with one source sentence per line.</li>
<li><code>--target</code> : a gzipped text file with one target sentence per line.</li>
<li><code>--alignment</code> : a gzipped text file with one sentence pair word alignment per line in Berkeley format.</li>
<li><code>--provenance</code> : a gzipped text file with one set of space separated provenances for a sentence pair per line. In general, each sentence pair has the 'main' provenance, unless you want to exclude some sentence pairs from the general source-to-target and target-to-source computation.</li>
<li><code>--hdfsout</code> : the location of the training data on HDFS.</li>
</ul>
<p>For <code>--hdfsout</code>, you can specify a relative or absolute path. The relative path is relative to your home directory on HDFS, that is <code>/user/$USER</code>. Once the command is completed, you will find the training data at the following location on HDFS: </p>
<pre class="fragment">/user/$USER/RUEN-WMT13/training_data
</pre><p>You can verify this by running the Hadoop ls command: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop fs -ls RUEN-WMT13
</pre><p><b>Note</b>: for this tutorial, we use a sample of the training data available for the Russian-English <a href="http://statmt.org/wmt13/translation-task.html">translation task at WMT13</a>. If you wish to test rule extraction with the entire data, modify <code>$DEMO/configs/CF.rulextract.load</code> with the following options:</p>
<ul>
<li><code>--source=train/ru.gz</code></li>
<li><code>--target=train/en.gz</code></li>
<li><code>--alignment=train/align.berkeley.gz</code></li>
<li><code>--provenance=train/provenance.gz</code></li>
</ul>
<p>For the full data, we give indicative timing measurements obtained on our cluster of 12 machines with 16 cores and 47G RAM each:</p>
<ul>
<li>extraction: 106m</li>
<li>s2t : 12m</li>
<li>t2s : 13m</li>
<li>merge : 41m</li>
<li>retrieval : 11m</li>
</ul>
<h2><a class="anchor" id="rulextract_extract"></a>
Rule Extraction</h2>
<p>Once the training data has been loaded onto HDFS, rules can be extracted. This is done via the following command: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.extraction.ExtractorJob \
    @configs/CF.rulextract.extract \
    &gt;&amp; logs/log.extract
</pre><p>You can see the following options in the <code>configs/CF.rulextract.extract</code> configuration file:</p>
<ul>
<li><code>--input</code> : the input training data on HDFS. This was the output from data loading.</li>
<li><code>--output</code> : the extracted rules on HDFS. This is a directory.</li>
<li><code>--remove_monotonic_repeats</code> : clips counts. For example, given a monotonically aligned phrase pair &lt;a b c, d e f&gt;, the hiero rule &lt;a X, d X&gt; can be extracted from &lt;a b, d e&gt; and from &lt;a b c, d e f&gt;, but the occurrence count is clipped to 1.</li>
<li><code>--max_source_phrase</code> : the maximum source phrase length for a phrase-based rule.</li>
<li><code>--max_source_elements</code> : the maximum number of source elements (terminal or nonterminal) for a hiero rule.</li>
<li><code>--max_terminal_length</code> : the maximum number of consecutive source terminals for a hiero rule.</li>
<li><code>--max_nonterminal_span</code> : the maximum number of terminals covered by a source nonterminal.</li>
<li><code>--provenance</code> : comma-separated list of provenances.</li>
</ul>
<p>Once the extraction is complete, you will find the rules in the <code>/user/$USER/RUEN-WMT13/rules/</code> HDFS directory. In this directory, you should see the following files:</p>
<ul>
<li><code>_SUCCESS</code> : this file indicates that the job successfully completed.</li>
<li><code>_logs</code> : this directory contains metatdata about the job that has been run.</li>
<li><code>part-r-*</code> : these files are the output of the Hadoop job.</li>
</ul>
<p>You can visualize the output by running the SequenceFile printing command: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.util.SequenceFilePrint \
    RUEN-WMT13/rules/part-r-00000 2&gt;/dev/null | head -n 3
</pre><p>You should see a similar looking output: </p>
<pre class="fragment">0 -1_10 -1_138_20  {0=1, 1=1}        {0-0 1-2=1}
0 -1_100 -1_4_77   {0=2, 1=2}        {0-0 1-2=2}
0 -1_100288_3 -1_1672_4    {0=1, 1=1}             {0-0 1-1 2-2=1}
</pre><p>The first three fields correspond to the left-hand side, source side and target side of a rule. The fourth field gives rule counts for various provenances. The last field gives the word alignment for the rule and its count.</p>
<h2><a class="anchor" id="rulextract_s2t"></a>
Source-to-target Probability</h2>
<p>The output of the previous job is the input to feature computation. We start by computing source-to-target rule probabilities for each provenance. This is done via the following command: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.features.phrase.Source2TargetJob \
    -D mapred.reduce.tasks=16 \
    @configs/CF.rulextract.s2t \
    &gt;&amp; logs/log.s2t
</pre><p>You can see the following options in the <code>configs/CF.rulextract.s2t</code> configuration file:</p>
<ul>
<li><code>--input</code> : the extracted rules on HDFS. This was the output from rule extraction.</li>
<li><code>--output</code> : the source-to-target probabilities on HDFS.</li>
<li><code>--provenance</code> : comma-separated list of provenances.</li>
<li><code>--mapreduce_features</code> : comma-separated list of features. This is important to give the correct index to each feature.</li>
</ul>
<p>Note that the command line also has the option <code>-D mapred.reduce.tasks=16</code> . This specifies the number of reducers at runtime. Unfortunately, the number of reducers is not determined automatically by the Hadoop framework. You can also specify the number of reducers in the <code>mapred-site.xml</code> Hadoop cluster configuration file with the <code>mapred.reduce.tasks</code> property. Because main classes all implement the <code>Tool</code> interface, you can specify generic options at the command line (see <a href="http://hadoopi.wordpress.com/2013/06/05/hadoop-implementing-the-tool-interface-for-mapreduce-driver/">this example</a> and the <a href="https://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/util/Tool.html">API documentation</a> for more detail).</p>
<p>Once the job is complete, you will find the source-to-target probabilities in the <code>/user/$USER/RUEN-WMT13/s2t</code> HDFS directory.</p>
<p>You can visualize the output by running the SequenceFile printing command: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.util.SequenceFilePrint \
    RUEN-WMT13/s2t/part-r-00000 2&gt;/dev/null | head -n 3
</pre><p>You should see a similar looking output: </p>
<pre class="fragment">0 -1_10003_1428_3_1752 -1_6_3_1106_784_4_911      {0=1.0, 1=1.0, 4=1.0, 5=1.0}
0 -1_100521_277 -1_3191_471_151                                   {0=1.0, 1=1.0, 4=1.0, 5=1.0}
0 -1_100529_360070_9 -1_8906_41685_40                     {0=1.0, 1=1.0, 4=1.0, 5=1.0}
</pre><p>The first three fields correspond to the left-hand side, source side and target side of a rule. The last field represents the computed features. In this example, the index 0 corresponds to the source-to-target probability, the index 1 corresponds to the rule count, the index 4 correponds to the "cc" provenance specific probability and the index 5 corresponds to the "cc" provenance specific rule count. In the sample provided, "cc" is the only provenance so features for indices 0, 1, 4 and 5 should be the same.</p>
<h2><a class="anchor" id="rulextract_t2s"></a>
Target-to-source Probability</h2>
<p>Computation of other features can be done simultaneously. Computing target-to-source probabilities for each provenance can be done as follows: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.features.phrase.Target2SourceJob \
    -D mapred.reduce.tasks=16 \
    @configs/CF.rulextract.t2s \
    &gt;&amp; logs/log.t2s
</pre><p>You can see the following options in the <code>configs/CF.rulextract.t2s</code> configuration file:</p>
<ul>
<li><code>--input</code> : the extracted rules on HDFS.</li>
<li><code>--output</code> : the source-to-target probabilities on HDFS.</li>
<li><code>--provenance</code> : comma-separated list of provenances.</li>
<li><code>--mapreduce_features</code> : comma-separated list of features.</li>
</ul>
<p>Once the job is complete, you will find the target-to-source probabilities in the <code>/user/$USER/RUEN-WMT13/t2s</code> HDFS directory. The output is very similar to the output of the source-to-target job but the features have indices 2, 3, 12 and 13: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.util.SequenceFilePrint \
    RUEN-WMT13/t2s/part-r-00000 2&gt;/dev/null | head -n 3
0 -1_1985 -1_100090                         {2=1.0, 3=1.0, 12=1.0, 13=1.0}
0 -1_1985_8_6 -1_100090_215_8               {2=1.0, 3=1.0, 12=1.0, 13=1.0}
0 -1_234783_5289 -1_100148_3442             {2=1.0, 3=1.0, 12=1.0, 13=1.0}
</pre><h2><a class="anchor" id="rulextract_merge"></a>
Feature Merging</h2>
<p>Once all features have been computed, rules and features are merged into a single output. This can be done via the following command: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.merge.MergeJob \
    -D mapred.reduce.tasks=10 \
    @configs/CF.rulextract.merge \
    &gt;&amp; logs/log.merge
</pre><p>You can see the following options in the <code>configs/CF.rulextract.merge</code> configuration file:</p>
<ul>
<li><code>--input_features</code> : comma separated list of output from feature computation</li>
<li><code>--input_rules</code> : the extracted rules on HDFS</li>
<li><code>--output</code> : merged output</li>
</ul>
<p>We need both rules and features as input because the merge job adds word alignment information into the output using the rules. Once this step is completed, there will be 10 output hfiles in the <code>/user/$USER/RUEN-WMT13/merge</code> HDFS directory. For backup purposes and in order to make the subsequent retrieval step faster, copy these hfiles to local disk as follows: </p>
<pre class="fragment">&gt; mkdir -p hfile
&gt; $HADOOP_ROOT/bin/hadoop fs -copyToLocal RUEN-WMT13/merge/*.hfile hfile/
</pre><p>If you are using NFS, it's better to copy the hfiles to local disk, e.g. <code>/tmp</code> or <code>/scratch</code> .</p>
<p>To visualize the output, run the HFile printing command: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.util.HFilePrint \
    RUEN-WMT13/merge/part-r-00000.hfile 2&gt;/dev/null | head -n 3
0 12 3      {0-0=7}                                                         {0=0.01728395061728395, 1=7.0, 2=0.002145922746781116, 3=7.0, 4=0.01728395061728395, 5=7.0, 12=0.002145922746781116, 13=7.0}
0 12 6      {0-0=5}                                                         {0=0.012345679012345678, 1=5.0, 2=0.010438413361169102, 3=5.0, 4=0.012345679012345678, 5=5.0, 12=0.010438413361169102, 13=5.0}
0 12 7      {0-0=84}                                                        {0=0.2074074074074074, 1=84.0, 2=0.10218978102189781, 3=84.0, 4=0.2074074074074074, 5=84.0, 12=0.10218978102189781, 13=84.0}
</pre><p>The first three fields correspond to the rule, the fourth field to the word alignment and the last field to the computed features.</p>
<h1><a class="anchor" id="rulextract_retrieval"></a>
Grammar Filtering</h1>
<h2><a class="anchor" id="lex_model"></a>
Lexical Models Download</h2>
<p>Lexical models are available as a separate download as they take a fair amount of disk space. Run these commands: </p>
<pre class="fragment">&gt; wget http://mi.eng.cam.ac.uk/~jmp84/share/giza_ibm_model1_filtered.tar.gz
&gt; tar -xvf giza_ibm_model1_filtered.tar.gz
</pre><p>These lexical models were filtered with the source vocabulary and target vocabulary of the test set for this tutorial to obtain a reasonable size for these models (the source vocabulary is easily obtained from the test set, the target vocabulary is obtained by taking target words from relevant translation rules for that test set). If you wish, you can also download the <a href="http://mi.eng.cam.ac.uk/~jmp84/share/giza_ibm_model1_filtered.tar.gz">full models</a> but you will require a machine with about 30G RAM to load the servers.</p>
<h2><a class="anchor" id="lex_prob_server"></a>
Lexical Probability Servers</h2>
<p>A preliminary step prior to grammar filtering is to launch lexical probability servers. These servers load IBM Model 1 probabilities for various provenances and for source-to-target and target-to-source directions. These probabilities have been obtained by the GIZA++ toolkit. For convenience, for this tutorial, we provide pretrained models. The source-to-target and target-to-source servers are launched as follows: </p>
<pre class="fragment">&gt; export HADOOP_HEAPSIZE=3000
&gt; export HADOOP_OPTS="-XX:+UseConcMarkSweepGC -verbose:gc -server -Xms3000M"
&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.features.lexical.TTableServer \
    @configs/CF.rulextract.lexserver \
    --ttable_direction=s2t \
    --ttable_language_pair=en2ru \
    --min_lex_prob=0.001
&gt; $HADOOP_ROOT/bin/hadoop \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.extraction.hadoop.features.lexical.TTableServer \
    @configs/CF.rulextract.lexserver \
    --ttable_direction=t2s \
    --ttable_language_pair=ru2en \
    --min_lex_prob=0.001
</pre><p>Both servers should be launched in a separate terminal and without trailing ampersand to the commands. Once the servers are ready, a message similar to <code>"TTable server ready on port: 4949"</code> will be printed out.</p>
<p>You can see the following options in the <code>configs/CF.rulextract.lexserver</code> configuration file:</p>
<ul>
<li><code>--ttable_s2t_server_port</code> : the port for the source-to-target server.</li>
<li><code>--ttable_s2t_host</code> : the host for the source-to-target server.</li>
<li><code>--ttable_t2s_server_port</code> : the port for the target-to-source server.</li>
<li><code>--ttable_t2s_host</code> : the host for the target-to-source server.</li>
<li><code>--ttable_server_template</code> : indicates a templated path to the lexical model where the templates <code>$GENRE</code> and <code>$DIRECTION</code> are replaced by their actual value.</li>
<li><code>--provenance</code> : comma-separated provenances. This is used to search for the lexical models in the template.</li>
<li><code>--min_lex_prob</code> : this option is used to keep the memory usage relatively low. Probabilities in the Model 1 table that are lower than a certain threshold are discarded. By default, no entry is discarded.</li>
</ul>
<h2><a class="anchor" id="hadoop_local_conf"></a>
Hadoop Local Configuration</h2>
<p>We have mentioned that retrieval is faster from local disk than from HDFS. In order to run Hadoop commands locally, we need to use a local configuration. This can be done by modifying the Hadoop configuration file that was prepared when setting up the Hadoop cluster: </p>
<pre class="fragment">&gt; cp -r $HADOOP_ROOT/conf configs/hadoopLocalConf
&gt; cat $HADOOP_ROOT/conf/mapred-site.xml | \
    $RULEXTRACT/scripts/makeHadoopLocalConfig.pl \
    &gt; configs/hadoopLocalConf/mapred-site.xml
&gt; cat $HADOOP_ROOT/conf/hdfs-site.xml | \
    $RULEXTRACT/scripts/makeHadoopLocalConfig.pl \
    &gt; configs/hadoopLocalConf/hdfs-site.xml
&gt; cat $HADOOP_ROOT/conf/core-site.xml | \
    $RULEXTRACT/scripts/makeHadoopLocalConfig.pl \
    &gt; configs/hadoopLocalConf/core-site.xml
</pre><h2><a class="anchor" id="retrieval"></a>
Grammar Filtering</h2>
<p>Once the lexical probability servers are up, the Hadoop local configuration has been prepared, one can proceed to actual grammar filtering. This is done via the following command: </p>
<pre class="fragment">&gt; $HADOOP_ROOT/bin/hadoop \
    --config configs/hadoopLocalConf \
    jar $RULEXTRACTJAR \
    uk.ac.cam.eng.rule.retrieval.RuleRetriever \
    @configs/CF.rulextract.retrieval \
    &gt;&amp; logs/log.retrieval
</pre><p>You can see the following options in the <code>configs/CF.rulextract.retrieval</code> configuration file:</p>
<ul>
<li><code>--max_source_phrase</code> : the maximum source phrase length for a phrase-based rule. This option is used to control how source patterns are generated from the test file. The value for this option should be at most the value chosen when extracting rules. Otherwise, no rules will be found for certain patterns.</li>
<li><code>--max_source_elements</code> : the maximum number of source elements (terminal or nonterminal) for a hiero rule. Same remarks as for <code>--max_source_phrase</code> apply.</li>
<li><code>--max_terminal_length</code> : the maximum number of consecutive source terminals for a hiero rule. Same remarks as for <code>--max_source_phrase</code> apply.</li>
<li><code>--max_nonterminal_span</code> : the maximum number of terminals covered by a source nonterminal. Usually we set the value for this option to be equal to the one used in the extraction phase but it's possible to choose any value smaller that the value of <code>--hr_max_height</code> .</li>
<li><code>--hr_max_height</code> : the maximum number of terminals covered by the entire source side of a rule. The value for this option should be at most the value chosen for the <code>--cykparser.hrmaxheight</code> option in the HiFST decoder, otherwise some rules will never be used in decoding.</li>
<li><code>--mapreduce_features</code> : comma-separated list of mapreduce features. Note that for the value for this option, we have used the value from the extraction phase and added lexical features.</li>
<li><code>--provenance</code> : comma-separated list of provenances.</li>
<li><code>--features</code>: comma-separated list of features.</li>
<li><code>--pass_through_rules</code> : file containing special translation rules that copy source words or source word sequences to the target.</li>
<li><code>--filter_config</code> : file with additional filter options. This configuration file determines what patterns are allowed, minimum source-to-target and target-to-source probabilities for phrase-based and hierarchical rules, etc. See the comments in <code>$DEMO/configs/CF.rulextract.filter</code> for details.</li>
<li><code>--source_patterns</code> : list of source patterns to be used in order to generate source pattern instances.</li>
<li><code>--ttable_s2t_server_port</code> : the port for the source-to-target server. The value for this option should be same as the one used to launch the servers.</li>
<li><code>--ttable_s2t_host</code> : the host for the source-to-target server.</li>
<li><code>--ttable_t2s_server_port</code> : the port for the target-to-source server.</li>
<li><code>--ttable_t2s_host</code> : the host for the target-to-source server.</li>
<li><code>--retrieval_threads</code> : the number of threads. The value for this option should be equal to the number of HFiles obtained in the merge step.</li>
<li><code>--hfile</code> : the directory containing the HFiles.</li>
<li><code>--test_file</code> : the test set to be translated.</li>
<li><code>--rules</code> : the output file containing relevant rules for the test set.</li>
</ul>
<p>Once this step is completed, you should obtain a rule file at this location: <code>$DEMO/G/rules.RU.tune.idx.gz</code></p>
<h2><a class="anchor" id="grammar_conversion"></a>
Grammar Formatting</h2>
<p>In order to obtain a shallow grammar ready to use by the HiFST decoder, a postprocessing step is needed. This can be achieved via the following command: </p>
<pre class="fragment">&gt; zcat -f G/rules.RU.tune.idx.gz | \
    $RULEXTRACT/scripts/prepareShallow.pl | \
    $RULEXTRACT/scripts/shallow2hifst.pl | \
    $RULEXTRACT/scripts/sparse2nonsparse.pl 27 | \
    gzip &gt; G/rules.shallow.vecfea.sample.prov.gz
</pre><p>The <code>G/rules.shallow.vecfea.sample.prov.gz</code> file should be ready to be used by the HiFST decoder with the <code>--grammar.load</code> option. If you've changed the <code>configs/CF.rulextract.load</code> configuration file to use the entire training data, and if you have used the option <code>--min_lex_prob</code> set to zero for the lexical probability servers, then you should obtain a file with the same rules as in <code>G/rules.shallow.vecfea.all.gz</code> with the same first 11 features and additional provenance features.</p>
<h1><a class="anchor" id="Development"></a>
Development</h1>
<h2><a class="anchor" id="ide"></a>
IDE Development</h2>
<p>In order to generate a project file for Eclipse, please follow these <a href="https://github.com/typesafehub/sbteclipse">instructions</a>. For IntelliJ IDEA, follow these <a href="https://github.com/mpeltonen/sbt-idea">instructions</a>.</p>
<h2><a class="anchor" id="local_feature"></a>
Adding a Local Feature</h2>
<p>We give instructions on how to add a local feature to rule extraction. In order to compute a local feature for a rule, we only need to consider that rule. For example, the number of target terminals in a rule is a local feature. Let's add that feature:</p>
<ul>
<li>In the <code>uk.ac.cam.eng.rulebuilding.features</code> package, create a new class called <code>NumberTargetElements</code> . Its featureName field can be for example "number_target_elements".</li>
<li>This class should implement the <code>Feature</code> class from the same package.</li>
<li>Implement the required methods for that class. You can look at the WordInsertionPenalty class in the same package for an example.</li>
<li>In the constructor of the FeatureCreator class in the same package, add the following line: <pre class="fragment">features.put("number_target_elements", new NumberTargetElements());
</pre></li>
<li>Modify the <code>--features</code> option to include <code>number_target_elements</code> in the comma-separated list of features. If you follow the tutorial commands, you can modify the <code>configs/CF.rulextract.retrieval</code> configuration file.</li>
</ul>
<p>If you get stuck, you can see what modifications are needed <a href="https://github.com/ucam-smt/ucam-smt/commit/09f697d1e7e6f35e9e04c37e3f00b0c7780b6d67">here</a></p>
<h2><a class="anchor" id="mapreduce_feature"></a>
Adding a MapReduce Feature</h2>
<p>We now give instructions on how to add a MapReduce feature. In order to compute a MapReduce feature, we need to consider all rules extracted from the training data rather than a single rule at a time. For example, let's add source-to-target and provenance source-to-target probabilities with add-one smoothing:</p>
<ul>
<li>In the <code>uk.ac.cam.eng.extraction.hadoop.features.phrase</code> package, create a new class called <code>Source2TargetAddOneSmoothedJob</code> . This class is very similar to <code>Source2TargetJob</code> except that the mapper adds one to the rule counts.</li>
<li>Modify the <code>MapReduceFeature</code> class to add the new feature.</li>
<li>Follow the steps to add a local feature.</li>
<li><p class="startli">Change the <code>--mapreduce_feature</code> option to be the following:</p>
<p class="startli">&ndash;mapreduce_features=source2target_probability,target2source_probability,provenance_source2target_probability,provenance_target2source_probability,source2target_addonesmoothed_probability,provenance_source2target_addonesmoothed_probability</p>
</li>
<li><p class="startli">For merging, change the <code>--input_features</code> option to be the following:</p>
<p class="startli">&ndash;input_features=RUEN-WMT13/s2t,RUEN-WMT13/t2s,RUEN-WMT13/s2taddone</p>
</li>
<li>Since there is a new feature, you need to run a command analogous to the one run to obtain source-to-target probabilities</li>
<li>For retrieval, modify the <code>--mapreduce_features</code> and the <code>--features</code> options to be as follows: <pre class="fragment">--mapreduce_features=source2target_probability,target2source_probability,provenance_source2target_probability,provenance_target2source_probability,source2target_addonesmoothed_probability,provenance_source2target_addonesmoothed_probability,source2target_lexical_probability,target2source_lexical_probability,provenance_source2target_lexical_probability,provenance_target2source_lexical_probability
--features=source2target_probability,target2source_probability,word_insertion_penalty,rule_insertion_penalty,glue_rule,insert_scale,rule_count_1,rule_count_2,rule_count_greater_than_2,source2target_lexical_probability,target2source_lexical_probability,provenance_source2target_probability,provenance_target2source_probability,provenance_source2target_lexical_probability,provenance_target2source_lexical_probability,source2target_addonesmoothed_probability,provenance_source2target_addonesmoothed_probability
</pre></li>
</ul>
<p>The code modifications are <a href="https://github.com/ucam-smt/ucam-smt/commit/7cba14a596f5e428ce69fe2620e411ecfe0e8d71">here</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Fri Jul 31 2015 15:30:38 for Cambridge SMT System by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.6 </li>
  </ul>
</div>
</body>
</html>
