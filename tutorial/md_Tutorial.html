<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.3"/>
<title>Cambridge SMT System: Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.3 -->
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="Intro"></a>
Introduction</h1>
<p>This tutorial presents various tools and techniques developed in the <a href="http://divf.eng.cam.ac.uk/smt">Statistical Machine Translation</a> group at the <a href="http://eng.cam.ac.uk">Cambridge University Engineering Department</a>.</p>
<p>The tutorial is intended to serve as a guide for the use of the tools, but our research publications contain the best descriptions of the algorithms and modelling techniques described here. The most relevant publications for this tutorial are listed below (<a class="el" href="md_Tutorial.html#Refs">Reading Material</a>). In particular, this tutorial is based on the Russian-English SMT system developed for the WMT 2013 evaluation - we suggest reading the system description [<a class="el" href="md_Tutorial.html#Pino2013">Pino2013</a>] before starting on the tutorial.</p>
<p>Our complete publications can be found at <a href="http://divf.eng.cam.ac.uk/smt/Main/SmtPapers">http://divf.eng.cam.ac.uk/smt/Main/SmtPapers</a>.</p>
<p>HiFST grew out of the Ph.D. thesis work of Gonzalo Iglesias.</p>
<p>Contributors to this release are:</p>
<ul>
<li>Graeme Blackwood</li>
<li>Bill Byrne</li>
<li>Adria de Gispert</li>
<li>Federico Flego</li>
<li>Gonzalo Iglesias</li>
<li>Juan Pino</li>
<li>Rory Waite</li>
<li>Tong Xiao</li>
</ul>
<p>with thanks to Cyril Allauzen and Michael Riley.</p>
<h2><a class="anchor" id="intro_features"></a>
Features Included in this Release</h2>
<ul>
<li>HiFST &ndash; Hierarchical phrase-based statistical machine translation system based on <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a></li>
<li>Direct production of translation lattices as Weighted Finite State Automata</li>
<li>Efficient WFSA rescoring procedures</li>
<li><a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> wrappers for direct inclusion of <a class="el" href="md_Tutorial.html#KenLM">KenLM</a> and ARPA language models as WFSAs</li>
<li>Lattice Minimum Bayes Risk decoding</li>
<li>Lattice Minimum Error Rate training</li>
<li>Tutorial for Hiero translation using Recursive Transition Networks and Pushdown Transducers</li>
<li>Client/Server mode</li>
<li>Shallow-N translation grammars</li>
<li>Source-sentence `chopping' procedures</li>
<li>WFSA true-casing</li>
<li>...</li>
</ul>
<h1><a class="anchor" id="Refs"></a>
Reading Material</h1>
<h2><a class="anchor" id="Refs_decoding"></a>
HiFST, HiPDT and Hierarchical Phrase-Based Decoding</h2>
<p><a class="anchor" id="deGispert2010"></a>[deGispert2010] <em>Hierarchical phrase-based translation with weighted finite state transducers and Shallow-N grammars</em>. <br/>
 A. de Gispert, G. Iglesias, G. Blackwood, E. R. Banga, and W. Byrne. Computational Linguistics, 36(3). 2010. <br/>
 <a href="http://aclweb.org/anthology/J/J10/J10-3008.pdf">http://aclweb.org/anthology/J/J10/J10-3008.pdf</a></p>
<p><a class="anchor" id="Allauzen2014"></a>[Allauzen2014] <em>Pushdown automata in statistical machine translation</em>. <br/>
 C. Allauzen, W. Byrne, A. de Gispert, G. Iglesias, and M. Riley. Computational Linguistics. 2014. To appear.<br/>
 <a href="http://mi.eng.cam.ac.uk/~wjb31/ppubs/cl2013.final.pdf">http://mi.eng.cam.ac.uk/~wjb31/ppubs/cl2013.final.pdf</a></p>
<p><a class="anchor" id="Iglesias2009"></a>[Iglesias2009] <em>Hierarchical phrase-based translation with weighted finite state transducers.</em><br/>
 G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne. Proceedings of HLT. 2009.<br/>
 <a href="http://aclweb.org/anthology//N/N09/N09-1049.pdf">http://aclweb.org/anthology//N/N09/N09-1049.pdf</a> <br/>
 <a href="http://mi.eng.cam.ac.uk/~wjb31/ppubs/naaclhlt2009presentation.pdf">http://mi.eng.cam.ac.uk/~wjb31/ppubs/naaclhlt2009presentation.pdf</a></p>
<p><a class="anchor" id="Iglesias2011"></a>[Iglesias2011] <em>Hierarchical Phrase-based Translation Representations</em>. <br/>
 G. Iglesias, C. Allauzen, W. Byrne, A. de Gispert, M. Riley. Proceedings of EMNLP. 2011. <br/>
 <a href="http://aclweb.org/anthology/D/D11/D11-1127.pdf">http://aclweb.org/anthology/D/D11/D11-1127.pdf</a></p>
<p><a class="anchor" id="Iglesias2009"></a>[Iglesias2009] <em>Rule filtering by pattern for efficient hierarchical translation</em>.<br/>
 G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne. Proceedings of EACL. 2009. <br/>
 <a href="http://aclweb.org/anthology/E/E09/E09-1044.pdf">http://aclweb.org/anthology/E/E09/E09-1044.pdf</a> <br/>
</p>
<p><a class="anchor" id="Chiang2007"></a>[Chiang2007] <em>Hierarchical phrase-based translation</em>.<br/>
 Computational Linguistics. 2007 <br/>
 <a href="http://aclweb.org/anthology/J07-2003.pdf">http://aclweb.org/anthology/J07-2003.pdf</a></p>
<h2><a class="anchor" id="Refs_systems"></a>
CUED SMT System Descriptions</h2>
<p><a class="anchor" id="Pino2013"></a>[Pino2013] <em>The University of Cambridge Russian-English System at WMT13</em>. <br/>
 J. Pino, A. Waite, T. Xiao, A. de Gispert, F. Flego, and W. Byrne. Proceedings of the Eighth Workshop on Statistical Machine Translation. 2013. <br/>
 <a href="http://aclweb.org/anthology//W/W13/W13-2225.pdf">http://aclweb.org/anthology//W/W13/W13-2225.pdf</a></p>
<h2><a class="anchor" id="Refs_fsts"></a>
OpenFST and Related Modelling Techniques</h2>
<p><a class="anchor" id="OpenFst"></a>[OpenFst] The OpenFST Toolkit <a href="http://www.openfst.org/">http://www.openfst.org/</a></p>
<p><a class="anchor" id="Roark2011"></a>[Roark2011] <em>Lexicographic semirings for exact automata encoding of sequence models.</em> <br/>
 B. Roark, R. Sproat, and I. Shafran. Proceedings of ACL-HLT. 2011. <br/>
 <a href="http://aclweb.org/anthology/P/P11/P11-2001.pdf">http://aclweb.org/anthology/P/P11/P11-2001.pdf</a></p>
<h2><a class="anchor" id="Refs_lmbr"></a>
Lattice Minimum Bayes Risk Decoding using WFSAs</h2>
<p><a class="anchor" id="BlackwoodPhD"></a>[BlackwoodPhD] <em>Lattice rescoring methods for statistical machine translation</em>.<br/>
 G. Blackwood. Ph.D. Thesis. Cambridge University Engineering Department and Clare College. 2010. <br/>
 <a href="http://mi.eng.cam.ac.uk/~gwb24/publications/phd.thesis.pdf">http://mi.eng.cam.ac.uk/~gwb24/publications/phd.thesis.pdf</a></p>
<p><a class="anchor" id="Blackwood2010"></a>[Blackwood2010] <em>Efficient path counting transducers for minimum Bayes-risk decoding of statistical machine translation lattices</em>.<br/>
 G. Blackwood, A. de Gispert, W. Byrne. Proceedings of ACL Short Papers. 2010. <br/>
 <a href="http://aclweb.org/anthology//P/P10/P10-2006.pdf">http://aclweb.org/anthology//P/P10/P10-2006.pdf</a></p>
<p><a class="anchor" id="Allauzen2010"></a>[Allauzen2010] <em>Expected Sequence Similarity Maximization</em>. <br/>
 C. Allauzen, S. Kumar, W. Macherey, M. Mohri, M Riley. Proceedings of HLT-NAACL, 2010. <br/>
 <a href="http://aclweb.org/anthology//N/N10/N10-1139.pdf">http://aclweb.org/anthology//N/N10/N10-1139.pdf</a></p>
<h2><a class="anchor" id="lmert_refs"></a>
Lattice Mert</h2>
<p><a class="anchor" id="Macherey2008"></a>[Macherey2008] <em>Lattice-based Minimum Error Rate Training for Statistical Machine Translation</em>. <br/>
 W. Macherey, F. Och, I. Thayer, J. Uszkoreit. Proceedings of EMNLP, 2008. <br/>
 <a href="http://aclweb.org/anthology/D/D08/D08-1076.pdf">http://aclweb.org/anthology/D/D08/D08-1076.pdf</a></p>
<p><a class="anchor" id="Waite2012"></a>[Waite2012] <em>Lattice-based minimum error rate training using weighted finite-state transducers with tropical polynomial weights.</em> <br/>
 A. Waite, G. Blackwood, and W. Byrne. Proceedings of FSMNLP, 2012.<br/>
 <a href="http://aclweb.org/anthology-new/W/W12/W12-6219.pdf">http://aclweb.org/anthology-new/W/W12/W12-6219.pdf</a></p>
<h2><a class="anchor" id="othertools"></a>
Language Modelling Toolkits</h2>
<p><a class="anchor" id="SRILM"></a>[SRILM] SRI Language Model Toolkit<br/>
 <a href="http://www.speech.sri.com/projects/srilm/">http://www.speech.sri.com/projects/srilm/</a></p>
<p><a class="anchor" id="KenLM"></a>[KenLM] The KenLM Toolkit<br/>
 <a href="http://kheafield.com/code/kenlm/">http://kheafield.com/code/kenlm/</a></p>
<h1><a class="anchor" id="general"></a>
Overview</h1>
<h2><a class="anchor" id="build"></a>
Installation</h2>
<p>The code can be cloned from the following GitHub address: </p>
<pre class="fragment">&gt; git clone https://github.com/ucam-smt/ucam-smt.git
</pre><p>Once downloaded, go into the cloned directory and run this command: </p>
<pre class="fragment">&gt; ./build-tests.sh
</pre><p>This should download and install necessary dependencies, compile the code and run tests. The <code>README.md</code> in the cloned directory also contains useful information for the installation.</p>
<p>Files for this tutorial can be downloaded from the following GitHub address: </p>
<pre class="fragment">&gt; git clone https://github.com/ucam-smt/demo-files.git
&gt; gunzip wmaps/*.gz  ## Uncompress big wordmap files.
</pre><p>There are additional Supplementary Files which can be downloaded from <a href="http://mi.eng.cam.ac.uk/~wjb31/data/hifst.release.May14/">http://mi.eng.cam.ac.uk/~wjb31/data/hifst.release.May14/</a> .</p>
<h2><a class="anchor" id="paths"></a>
Paths and Environment Variables</h2>
<p>The following instructions are for the Bash shell.</p>
<p>In the following, <code>HiFSTROOT</code> designates the cloned directory, i.e. the following should be a complete path to the cloned directory </p>
<pre class="fragment">&gt; export HiFSTROOT=complete_path_to_hifst_cloned_directory
</pre><p>After HiFST is successfully built and tested, the file $HiFSTROOT/Makefile.inc will contain environment variable settings needed to run the HiFST binaries and the OpenFST tools using the HiFST libraries. To set these, simply run </p>
<pre class="fragment">&gt; source $HiFSTROOT/Makefile.inc
&gt; export PATH=$HiFSTROOT/bin:$OPENFST_BIN:$PATH
&gt; export LD_LIBRARY_PATH=$HiFSTROOT/bin:$OPENFST_LIB
</pre><p>You should make sure that $HiFSTBINDIR is added first on the path and the library path and that it preceeds the OpenFst directories. If the LD_LIBRARY_PATH variable is not set correctly, you will see the message </p>
<pre class="fragment">ERROR: GenericRegister::GetEntry : tropical_LT_tropical-arc.so: cannot open shared object file: No such file or directory
ERROR: ReadFst : unknown arc type "tropical_LT_tropical" : standard input
</pre><h2><a class="anchor" id="Setup_files"></a>
Directory Structure</h2>
<p>The following directories contain the data files, configuration files, and model files needed for this tutorial. </p>
<pre class="fragment"> ./
 |-configs/ # Configuration files
 |-EN/      # English reference text
 |-G/       # Translation grammars
 |-M/       # Language models 
 |-RU/      # Russian input text
 |-scripts/ # Scripts for these demonstration exercises
 |-wmaps/   # Word maps, to map English and Russian text to integers
</pre><p>The following directories will be created after running this tutorial. </p>
<pre class="fragment"> ./
 |-log/     # Translation process log files
 |-output/  # Translation output, as 1-best hypotheses and lattices
</pre><h2><a class="anchor" id="Setup_configs"></a>
Configuration Files and Command Line Options</h2>
<p>As you work through the tutorial, please read the comments in the config files which explain some of the processing options. The following configuration files are provided for the tutorial. </p>
<pre class="fragment"> # baseline configuration: 4-gram LM and Shallow-1 translation grammar
 configs/CF.baseline : HiFST with 4-gram language model and a Shallow-1 grammar
 configs/CF.baseline.lmbr : lattice Minimum Bayes' Risk (LMBR) rescoring on top of baseline system
 configs/CF.baseline.outputnoprune : lattice output without pruning
 configs/CF.baseline.outputnoprune.lmrescore : lattice rescoring with language models
 # full Hiero grammar with 4-gram LM
 configs/CF.hiero : full Hiero grammar without pruning in search
 configs/CF.hiero.chopping : methods for dealing with long source sentences
 configs/CF.hiero.localprune  : full Hiero grammar with pruning in search 
 configs/CF.hiero.pdt : full Hiero grammar, decoding with push-down automata (HiPDT)
 # full, iterative lattice MERT script 
 configs/CF.lmert.alilats : Lattice MERT example, alignment lattices
 configs/CF.lmert.hyps : Lattice MERT example, initial hypotheses
 configs/CF.lmert.vecfea : Lattice MERT example, vector feature lattices
 # example feature generation for MERT and LMERT
 configs/CF.mert.alilats.nbest : MERT features, derivation-to-translation transducers, restricted to N-Best lists 
 configs/CF.mert.hyps : MERT features, initial hypotheses
 configs/CF.mert.vecfea.nbest  : MERT features, N-Best feature lists for MERT
 # misc
 configs/CF.recaser : recasing examples
 configs/CF.baseline.client : HiFST client-server example, client
 configs/CF.baseline.server : HiFST client-server example, server
</pre><p>HiFST uses the Boost libraries which provide support for <a href="http://www.boost.org/doc/libs/1_55_0/doc/html/program_options/overview.html">configuration files</a>.</p>
<p>Parameters can be supplied either on the command line or in the config files. For example, the following options could be provided on the command line: </p>
<pre class="fragment"> --hifst.prune=9 --hifst.replacefstbyarc.nonterminals=X,V 
</pre><p>Alternatively, they could be specified in a configuration file either as </p>
<pre class="fragment"> hifst.prune=9
 hifst.replacefstbyarc.nonterminals=X,V 
</pre><p>or as </p>
<pre class="fragment"> [hifst]
 prune=9
 replacefstbyarc.nonterminals=X,V 
</pre><h2><a class="anchor" id="wmaps"></a>
Word Maps and Integer Mapped Files</h2>
<p>HiFST uses <a href="http://www.openfst.org/twiki/bin/view/FST/FstAdvancedUsage#Symbol_Tables">symbol tables</a> as provided by <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> to map between source and target language text and the integer representation used internally by the decoder. See the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://www.openfst.org/twiki/bin/view/FST/FstQuickTour">Quick Tour</a> for a discussion of the use of symbol tables.</p>
<p>Integer mappings for English and Russian are in the directory wmaps/ : </p>
<pre class="fragment"> wmaps/wmt13.en.wmap 
 wmaps/wmt13.en.all.wmap (a much larger version of wmaps/wmt13.en.wmap)
 wmaps/wmt13.ru.wmap
 wmaps/wmt13.ru.all.wmap (a much larger version of wmaps/wmt13.ru.wmap)
</pre><p>Note that HiFST reserves the integers 1 and 2 for the sentence-start and sentence-end symbols. 0 is the OpenFST epsilon symbol.</p>
<p>The format of the wordmap files is straightforward, e.g. </p>
<pre class="fragment"> &gt; head wmaps/wmt13.en.wmap 
 &lt;epsilon&gt;                0
 &lt;s&gt;                      1
 &lt;/s&gt;                     2
 the                      3
 ,                        4
 .                        5
 of                       6
 to                       7
 and                      8
 in                       9
</pre><p>Source text files are provided in integer format : </p>
<pre class="fragment"> RU/RU.set1.idx : integer mapped Russian text

 &gt; head -2 RU/RU.set1.idx 
 1 20870 2447 5443 50916 78159 3621 2
 1 1716 20196 95123 154 1049 6778 996 9 239837 7 1799 4 2
</pre><p>The <a href="http://openfst.org/twiki/bin/view/FST/FstExtensions">FAR</a> tools can be used to generate Russian text from the integer mapped files (see the discussion on <a class="el" href="md_Tutorial.html#basic_latshyps">Translation Lattices and 1-Best Hypotheses</a>). </p>
<pre class="fragment"> &gt; farcompilestrings --entry_type=line RU/RU.set1.idx | farprintstrings --symbols=wmaps/wmt13.ru.wmap | head -2
 &lt;s&gt; республиканская стратегия сопротивления повторному избранию обамы &lt;/s&gt;
 &lt;s&gt; лидеры республиканцев оправдывали свою политику необходимостью борьбы с фальсификациями на выборах . &lt;/s&gt;
</pre><h2><a class="anchor" id="lms"></a>
Language Models</h2>
<p>English 3-gram and 4-gram language models are provided in both <a href="http://kheafield.com/code/kenlm/">KenLM</a> and <a href="http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html">ARPA</a> formats . See [<a class="el" href="md_Tutorial.html#Pino2013">Pino2013</a>] for a description of how these LMs are built.</p>
<p>The following language models have restricted vocabulary corresponding to the target side of the rules that apply to the first few sentences of the tuning set RU.set1.idx . This is done so that the LMs are small and quickly and easily loaded into memory. <em>Do not use these models except for the first few sentences in this tutorial.</em> </p>
<pre class="fragment"> M/lm.3g.arpa.gz : Kneser-Ney 3-gram language model in ARPA format 
 M/lm.3g.mmap : Kneser-Ney 3-gram language model in KenLM format
 M/lm.4g.arpa.gz : Kneser-Ney 4-gram language model in ARPA format 
 M/lm.4g.mmap : Kneser-Ney 4-gram language model in KenLM format
 M/lm.4g.eprnd.mmap : entropy pruned KN 4-gram LM in KenLM format
 M/lm.tc.gz : true-casing language model
</pre><p>The following large LMs are available from a separate download site (see <a class="el" href="md_Tutorial.html#build">Installation</a>). It covers the target-side vocabulary for the large translation grammars, and is suitable for running on the complete tune and test set included in this tutorial. In particular, this second LM must be downloaded and uncompressed into the M/ directory prior to running the MERT and LMERT scripts and examples (see <a class="el" href="md_Tutorial.html#mert">MERT - Features Only</a> and <a class="el" href="md_Tutorial.html#lmert">Lattice MERT</a>). </p>
<pre class="fragment"> M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap : KN 4gram LM in KenLM format
 M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap : quantized KN 4gram LM in KenLM format
</pre><p>Language models are in integer mapped format, e.g. for the ARPA files: </p>
<pre class="fragment"> &gt; zcat M/lm.3g.arpa.gz | grep . | head -15
 \data\
 ngram 1=1348
 ngram 2=130054
 ngram 3=785733
 \1-grams:
 -1.0615243 &lt;unk&gt;
 -inf               &lt;s&gt;     -1.0853117
 -1.5690455 &lt;/s&gt;
 -2.2388144 12      -1.0949439
 -2.682596  11      -0.872226
 -4.0860014 1547    -0.66412055
 -2.4807615 14      -0.8686333
 -2.9167347 25      -0.7014704
 -2.599824  22      -0.6987488
 -2.6652465 26      -0.7175091
</pre><p>We also provide an <em>entropy pruned</em> [<a class="el" href="md_Tutorial.html#SRILM">SRILM</a>] version of the 4-gram language model as used for decoding with Push-Down Automata [<a class="el" href="md_Tutorial.html#Allauzen2014">Allauzen2014</a>] ; this is described below in <a class="el" href="md_Tutorial.html#pda">Push-Down Automata</a> . </p>
<pre class="fragment"> M/lm.4g.eprnd.arpa.gz : Entropy-pruned Kneser-Ney 4-gram language model in ARPA format 
 M/lm.4g.eprnd.mmap : Entropy-pruned Kneser-Ney 4-gram language model in KenLM format
</pre><h2><a class="anchor" id="tgrammars"></a>
Translation Grammars</h2>
<p>HiFST uses Synchronous Context-Free Grammars (SCFGs) for translation. A full Hiero and a Shallow-1 translation grammar are provided in the <code>G/</code> directory: </p>
<pre class="fragment"> G/rules.hiero.gz : full hiero grammar with scalar translation scores
 G/rules.shallow.gz : Shallow-1 hiero grammar with scalar translation scores
</pre><p>We also provide versions of these grammars with raw, unweighted feature scores: </p>
<pre class="fragment"> G/rules.hiero.vecfea.gz : full hiero grammar with feature vectors
 G/rules.shallow.vecfea.gz : Shallow-1 hiero grammar with feature vectors
</pre><p>For the tutorial on optimization (<a class="el" href="md_Tutorial.html#mert">MERT - Features Only</a>), larger grammars corresponding to the entire <code>RU/RU.tune.idx</code> tune set are provided: </p>
<pre class="fragment"> G/rules.shallow.all.gz : larger Shallow-1 hiero grammar with scalar translation scores
 G/rules.shallow.vecfea.allgz : larger Shallow-1 hiero grammar with feature vectors
</pre><p>There is also a grammar provided for the true-casing example (<a class="el" href="md_Tutorial.html#true_casing">Fst-based True casing</a>) </p>
<pre class="fragment">  G/tc.unimap
</pre><h3><a class="anchor" id="rules"></a>
Grammar File Formats</h3>
<p>In the grammar file, each line represents a rule. The rule format is: </p>
<pre class="fragment"> LHS RHS_SOURCE RHS_TARGET FEA_1 [FEA_2 FEA_3 FEA_4 ...]
</pre><p>where </p>
<pre class="fragment"> LHS = the left hand side of the rule
 RHS_SOURCE = the source-language part of the right hand side of the rule
 RHS_TARGET = the target-language part of the right hand side of the rule
 FEA_i = the i-th component of the feature vector associated with the rule
</pre><p>The left hand side of a rule is a non-terminal symbol (in uppercase). The right hand side is a pair of terminal and non-terminal symbol sequences in the source and target languages.</p>
<h4><a class="anchor" id="tgrammars_formats_fea"></a>
Feature Vectors</h4>
<p>Scores are assigned to rules as the dot product of a rule-specific feature vector and a weight vector (see the discussion in <a class="el" href="md_Tutorial.html#mert">MERT - Features Only</a>). This computation can be done offline, in which case the feature for every rule in the grammar is a 1-dimensional scalar. Alternatively, the decoder can be provided with a weight vector which is applied to the feature vectors while loading the grammar.</p>
<p>For example, the grammar <code>G/rules.shallow.gz</code> provided in this tutorial the following set of weights was found via LMERT tuning (see <a class="el" href="md_Tutorial.html#mert">MERT - Features Only</a> ): </p>
<pre class="fragment">0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
</pre><p>These can be applied to the Shallow-1 grammar, as follows: </p>
<pre class="fragment">&gt; WV=0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
&gt; zcat G/rules.shallow.vecfea.gz | scripts/weightgrammar -w=$WV | head -3
V 3 4 -2.046860955276
V 3 4_3 -1.884009085882
V 3 8 1.857985226112
</pre><p>and this should agree with the scalar-valued version of the grammar: </p>
<pre class="fragment">&gt; zcat G/rules.shallow.gz | head -3
V 3 4 -2.046860955276
V 3 4_3 -1.884009085882
V 3 8 1.857985226112
</pre><h4><a class="anchor" id="tgrammars_formats_nt"></a>
Non-Terminals</h4>
<p>In translation, a non-terminal <code>X</code> on the right hand side can be rewritten by any rule whose left hand side is <code>X</code>. HiFST places no restrictions on the definition of terminal and non-terminal sequences in an SCFG rule; similarly, there are no constraints on how many non-terminal symbols can be used in a rule (i.e. there are no constraints on order of the SCFG). However using a full Hiero grammar can lead to slow translation, and this tutorial discusses several strategies for pruning in translation and for translation grammar pruning.</p>
<p>Here is a small sample translation grammar </p>
<pre class="fragment"> M 434_M 1462_8_M -1.81842                
 M 7_M 9_3_M -0.735445                    
 M V V -0                                 
 S S_X S_X 0.05768                        
 S X X -0                                 
 V 10806 1411 1.16623                     
 V 164_M_60 78_M_8 -0.226464              
 V 164_M2_60_M1 78_M1_8_M2 -0.226464      
 V 21_591 39_258_8 -0.510102              
 V 24 3_54 -2.50252                       
 V 274_M_4 709_9_3_M -0.589246            
 V 5 6 -1.81729                           
 V 7_1689 9_741_8 0.438945                
 V 8 23 -1.46604                          
 X 1 1 -2.5598                            
 X 2 2 -2.5598                            
 X V V -0                                 
 D 1775 &lt;dr&gt; 10.4327                      
 S S_D_X S_D_X 0.11536                    
</pre><p>This grammar has four non-terminal symbols: <code>S</code>, <code>M</code>, <code>V</code> and <code>D</code>, and a 1-dimensional weight. The terminal symbols are integers, corresponding to words in the source and target language word maps (<a class="el" href="md_Tutorial.html#wmaps">Word Maps and Integer Mapped Files</a>). The symbol <code>&lt;dr&gt;</code> is a special symbol used by HiFST to represent an empty word, to indicate deletion.</p>
<p>As an example, for rule </p>
<pre class="fragment"> V 164_M_60 78_M_8 -0.226464
</pre><p>we have </p>
<pre class="fragment"> LHS        = V
 RHS_SOURCE = 164_M_60
 RHS_TARGET = 78_M_8
 WEIGHT_1   = -0.226464
</pre><p>With this rule the decoder can rewrite the non-terminal <code>V</code> by replacing it by <code>164_M_60</code> in the source language and by <code>78_M_8</code> in the target language; the rule is applied with a score of -0.226464. Similarly, rule <code>V 5 6 -1.81729</code> replaces <code>V</code> by the word "5" in the source-language and with the word "6" in the target-language, with a score of -1.81729.</p>
<p>Indices on non-terminals indicate alignment within rules with more than one non-terminal. For example, for rule </p>
<pre class="fragment">V 164_M2_6_M1 78_M1_8_M2 -0.226464
</pre><p>the <code>M</code> non-terminals on both language sides are indexed by 1 or 2; that is, <code>M1</code> in the source language is linked with <code>M1</code> in the target language, and <code>M2</code> in the source language is linked with <code>M2</code> in the target. Strictly speaking, <code>M2</code> is not a non-terminal in the above example: it is the second instance of the non-terminal <code>M</code> in the rule. Obviously this is a reordering rule.</p>
<p>In general we use different types of rule to distinguish different translation cases and obtain a finer-grained model. In this example, the <code>S</code> rules are doing something rather similar to the glue rules used in Hiero-style systems; the <code>M</code> rules can be regarded as monotonic translation rules; the <code>V</code> rules can be regarded as reordering translation rules and phrasal translation rules; and the <code>D</code> rule can be regarded as an explicit operation of word deletion.</p>
<p>Note that there is a straight-forward correspondence between the Hiero-style rule notation introduced by [<a class="el" href="md_Tutorial.html#Chiang2007">Chiang2007</a>] and the HiFST rule file format (for rules with one-dimensional weights). For example, the HiFST grammar file entries </p>
<pre class="fragment">V 164_M2_6_M1 78_M1_8_M2 -0.226464
S S_X S_X 0.05768
</pre><p>can be written as </p>
<pre class="fragment">V -&gt; &lt; 164 M2 6 M1 ,  78 M1 8 M2 &gt; / -0.226464
S -&gt; &lt; S X , S X &gt; / 0.05768
</pre><h3><a class="anchor" id="tgrammars_shallow"></a>
Shallow-N Translation Grammars</h3>
<p>Shallow grammars [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>] can be used to control the degree of nesting allowed within an hierarchical grammar. For example, for a Shallow-1 Grammar, variables in a rule can be substituted only by phrases. That is, hierarchical rules can be used only once to generate words; once a hierarchical rule is used, translation relies on glue rules to cover longer source spans.</p>
<p>Formally, we use W to denote the set of terminals, and S and X to denote two non-terminals. A simple Hiero-style grammar can be defined to be: </p>
<pre class="fragment"> S -&gt; X, X                                           
 S -&gt; S X, S X                                       
 X -&gt; a, b        where a, b \in ({X} union W)^+
</pre><p>We can transform this into a Shallow-1 grammar as </p>
<pre class="fragment"> S -&gt; X, X                                           
 S -&gt; S X, S X                                       
 Y -&gt; a, b        where a, b \in {W}^+               
 X -&gt; u, v        where u, v \in {{Y} union W}^+ 
</pre><p>Here <code>Y</code> is introduced to handle phrasal translations. The variables in rule <code>X -&gt; u, v</code> can only be substituted with the <code>Y</code> rules.</p>
<p>In some cases it is desirable to allow more complex movement in translation, such as complex structure movements in Chinese-English translation. For this we can use a generalisation of the simple Shallow-1 grammar, called Shallow-N grammars. These grammars allow hierarchical rules to be applied up to N times. For example, below is the form of a Shallow-2 grammar. </p>
<pre class="fragment"> S -&gt; X, X                                           
 S -&gt; S X, S X                                       
 Y^0 -&gt; a^0, b^0  where a^0, b^0 \in {W}^+           
 Y^1 -&gt; a^1, b^1  where a^1, b^1 \in {{Y^0} union W}^+ 
 X -&gt; u, v        where u, v \in {{Y^1} union W}^+     
</pre><p>The Shallow-2 grammar introduces <code>Y^0</code> and <code>Y^1</code> to handle hierarchical rule application at two levels within a derivation. For more detailed description of Shallow-n grammars, please refer to the HiFST paper [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>].</p>
<h1><a class="anchor" id="basic"></a>
Basic Translation and Lattice Generation</h1>
<p>The first demonstration exercise is to generate translations of integer-mapped Russian text using the translation grammar and English n-gram language model provided. HiFST is configured to generate one-best translation hypotheses as well as translation lattices.</p>
<p>The baseline configuration file is </p>
<pre class="fragment">configs/CF.baseline 
</pre><p>See the comments in that file for brief explanations of the HiFST options.</p>
<p>The following command will translate the first 2 lines in the Russian integer-mapped file <code>RU/RU.set1.idx</code>: </p>
<pre class="fragment"># Run HiFST
&gt; mkdir log
&gt; hifst.O2 --config=configs/CF.baseline &amp;&gt; log/log.baseline
</pre><p>The log file output can be viewed as: </p>
<pre class="fragment">&gt; tail -n 11 log/log.baseline
Fri May  9 11:04:37 2014: run.INF:=====Translate sentence 1:1 20870 2447 5443 50916 78159 3621 2
Fri May  9 11:04:37 2014: run.INF:Loading hierarchical grammar: G/rules.shallow.gz
Fri May  9 11:04:37 2014: run.INF:loading LM=M/lm.4g.mmap
Fri May  9 11:04:37 2014: run.INF:Stats for Sentence 1: local pruning, number of times=0
Fri May  9 11:04:37 2014: run.INF:End Sentence ******************************************************
Fri May  9 11:04:37 2014: run.INF:Translation 1best is: 1 9121 384 6 2756 7 3 4144 6 1458528 1341 2 
Fri May  9 11:04:37 2014: run.INF:=====Translate sentence 2:1 1716 20196 95123 154 1049 6778 996 9 239837 7 1799 4 2
Fri May  9 11:04:37 2014: run.INF:Stats for Sentence 2: local pruning, number of times=0
Fri May  9 11:04:38 2014: run.INF:End Sentence ******************************************************
Fri May  9 11:04:38 2014: run.INF:Translation 1best is: 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2 
Fri May  9 11:04:38 2014: main.INF:hifst.O2 ends!
</pre><p>The best scoring translation hypotheses are given in integer-mapped form, e.g. for the second Russian sentence, the best-scoring translation hypothesis is </p>
<pre class="fragment">run.INF:Translation 1best is: 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2
</pre><h2><a class="anchor" id="basic_latshyps"></a>
Translation Lattices and 1-Best Hypotheses</h2>
<p>The baseline configuration file instructs HiFST to write its 1-best translations to the output file <code>output/exp.baseline/hyps</code> (see the <code>target.store=</code> specification in the config file). The contents of this file should agree with the <em>Translation 1best</em> entries in the log file (compare these results to above): </p>
<pre class="fragment"> &gt; cat output/exp.baseline/hyps
 1 9121 384 6 2756 7 3 4144 6 1458528 1341 2 
 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2 
</pre><p>The configuration file also directs HiFST to write translation lattices to <code>output/exp.baseline/LATS/?.fst.gz</code> (see the <code>hifst.lattice.store=</code> specification in the config file). Note the use of the placeholder '<code>?</code>' in the argument <code>.../LATS/?.fst.gz</code> . The placeholder is replaced by the line number of sentence being translated, e.g. so that <code>.../LATS/2.fst.gz</code> is a weighted finite state transducer (WFST) containing translations of the second line in the source text file. Note also the use of the '<code>.gz</code>' extension: when this is provided, lattices are written as gzipped files.</p>
<p>The shortest path through each of these output lattices should agree with the top-scoring hypotheses in the hyps and log files : </p>
<pre class="fragment">&gt; echo `zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint | awk '{print $3}'`
 1 9121 384 6 2756 7 3 4144 6 1458528 1341 2
&gt; echo `zcat output/exp.baseline/LATS/2.fst.gz | fstshortestpath | fsttopsort | fstprint | awk '{print $3}'`
 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2
</pre><p>The English wordmap can be supplied to fstprint to convert from integer mapped strings to English: </p>
<pre class="fragment">&gt; echo `zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint --isymbols=wmaps/wmt13.en.wmap | awk '{print $3}'`
 &lt;s&gt; republican strategy of resistance to the renewal of obamas election &lt;/s&gt;

&gt; echo `zcat output/exp.baseline/LATS/2.fst.gz | fstshortestpath | fsttopsort | fstprint --isymbols=wmaps/wmt13.en.wmap | awk '{print $3}'`
  &lt;s&gt; the leaders of the republican justified their policies need to deal with the spin on the elections . &lt;/s&gt;
</pre><p>The above operations do the following:</p>
<ol type="1">
<li><code>zcat</code> pipes the HiFST output lattice to the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/ShortestPathDoc">Shortest Path</a> tool which produces an FST containing only the shortest path in the lattice</li>
<li>the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/TopSortDoc">Topological Sort</a> operation renumbers the state IDs so that all arcs are links from lower to higher state IDs</li>
<li>the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/FstQuickTour#Printing_Drawing_and_Summarizing">fstprint</a> operation reads the English wordmap, for the arc input symbols, and traverses the input fst, writing each arc as it is encountered; TopSort ensures these arcs are written in the correct order</li>
<li>the awk operation prints only the words on the arcs</li>
<li>wrapping everything inside echo generates a single string</li>
</ol>
<p>To see what is produced at the various steps in the pipeline:</p>
<p>Input lattice: </p>
<pre class="fragment"> &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstinfo | head -6
 fst type                                          vector
 arc type                                          tropical_LT_tropical
 input symbol table                                none
 output symbol table                               none
 # of states                                       489
 # of arcs                                         1104
</pre><p>Shortest Path: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fstprint
12      11      1       1       -2.609375,-2.609375
0
1       0       2       2       2.33047056,-2.34277344
2       1       1341    1341    6.04568958,1.55957031
3       2       1458528 1458528 13.4981985,2.22167969
4       3       6       6       0.201819927,0
5       4       4144    4144    9.78138161,0
6       5       3       3       -0.395056069,-1.23925781
7       6       7       7       1.79730964,0
8       7       2756    2756    9.45967484,0.288085938
9       8       6       6       0.892110586,-2.04589844
10      9       384     384     7.13530731,-2.609375
11      10      9121    9121    9.33318996,-1.26074219
</pre><p>(the paired weights are described in the next section (<a class="el" href="md_Tutorial.html#basic_scores">Scores, Costs, and Semirings</a>))</p>
<p>Topologically Sorted Shortest Path: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint
0       1       1       1       -2.609375,-2.609375
1       2       9121    9121    9.33318996,-1.26074219
2       3       384     384     7.13530731,-2.609375
3       4       6       6       0.892110586,-2.04589844
4       5       2756    2756    9.45967484,0.288085938
5       6       7       7       1.79730964,0
6       7       3       3       -0.395056069,-1.23925781
7       8       4144    4144    9.78138161,0
8       9       6       6       0.201819927,0
9       10      1458528 1458528 13.4981985,2.22167969
10      11      1341    1341    6.04568958,1.55957031
11      12      2       2       2.33047056,-2.34277344
12
</pre><p>Toplogically Sorted Shortest Path, with English words replacing the arc input symbols </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint --isymbols=wmaps/wmt13.en.wmap
0       1       &lt;s&gt;     1       -2.609375,-2.609375
1       2       republican      9121    9.33318996,-1.26074219
2       3       strategy        384     7.13530731,-2.609375
3       4       of      6       0.892110586,-2.04589844
4       5       resistance      2756    9.45967484,0.288085938
5       6       to      7       1.79730964,0
6       7       the     3       -0.395056069,-1.23925781
7       8       renewal 4144    9.78138161,0
8       9       of      6       0.201819927,0
9       10      obamas  1458528 13.4981985,2.22167969
10      11      election        1341    6.04568958,1.55957031
11      12      &lt;/s&gt;    2       2.33047056,-2.34277344
12
</pre><p>Note that loading the English wordmap can be time consuming due to its size. Ideally, in processing multiple translation hypotheses, the wordmap should be loaded only once, rather than once for each sentence. The FST Archive (FAR) command line tools (in the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> FAR <a href="http://openfst.org/twiki/bin/view/FST/FstExtensions">extensions</a>) will do this: </p>
<pre class="fragment">&gt; farcompilestrings --entry_type=line output/exp.baseline/hyps | farprintstrings --symbols=wmaps/wmt13.en.wmap
&lt;s&gt; republican strategy of resistance to the renewal of obamas election &lt;/s&gt;
&lt;s&gt; the leaders of the republican justified their policies need to deal with the spin on the elections . &lt;/s&gt;
</pre><h2><a class="anchor" id="basic_nbest"></a>
N-Best Lists</h2>
<p>The <code>printstrings</code> tool provided with this tutorial combines the above operations into a single programme. It also can print the top-N hypotheses, as the following example shows </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc --nbest=10 --unique -w 2&gt;/dev/null
1 9121 384 6 2756 7 3 4144 6 1458528 1341 2             57.4705,-8.03809
1 3 9121 384 6 2756 7 3 4144 6 1458528 1341 2                   57.5366,-8.66992
1 9121 384 6 2756 7 3 4144 6 159312 42 1341 2                   57.7029,-8.49512
1 3 9121 384 6 2756 7 3 4144 6 159312 42 1341 2         57.769,-9.12695
1 9121 384 2756 7 3 4144 6 1458528 1341 2     59.5391,-6.32422
1 3 9121 384 2756 7 3 4144 6 1458528 1341 2   59.6052,-6.95605
1 9121 384 2756 7 3 4144 6 159312 42 1341 2   59.7715,-6.78125
1 3 9121 1132 384 4144 6 1458528 1341 2   59.8094,1.74512
1 3 9121 384 2756 7 3 4144 6 159312 42 1341 2       59.8376,-7.41309
1 3 9121 1132 384 3 4144 6 1458528 1341 2   59.8382,-0.804688
</pre><p>With the English wordmap, <code>printstrings</code> will map the integer representation to English text: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc --nbest=10 --unique -w -m wmaps/wmt13.en.wmap 2&gt;/dev/null
&lt;s&gt; republican strategy of resistance to the renewal of obamas election &lt;/s&gt;              57.4705,-8.03809
&lt;s&gt; the republican strategy of resistance to the renewal of obamas election &lt;/s&gt;      57.5366,-8.66992
&lt;s&gt; republican strategy of resistance to the renewal of obama 's election &lt;/s&gt;            57.7029,-8.49512
&lt;s&gt; the republican strategy of resistance to the renewal of obama 's election &lt;/s&gt;    57.769,-9.12695
&lt;s&gt; republican strategy resistance to the renewal of obamas election &lt;/s&gt;     59.5391,-6.32422
&lt;s&gt; the republican strategy resistance to the renewal of obamas election &lt;/s&gt;       59.6052,-6.95605
&lt;s&gt; republican strategy resistance to the renewal of obama 's election &lt;/s&gt;         59.7715,-6.78125
&lt;s&gt; the republican opposition strategy renewal of obamas election &lt;/s&gt;              59.8094,1.74512
&lt;s&gt; the republican strategy resistance to the renewal of obama 's election &lt;/s&gt;             59.8376,-7.41309
&lt;s&gt; the republican opposition strategy the renewal of obamas election &lt;/s&gt;          59.8382,-0.804688
</pre><h2><a class="anchor" id="basic_scores"></a>
Scores, Costs, and Semirings</h2>
<p>HiFST follows the formalism in which rule probabilities are represented as arc weights (see Section 2 of [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>]).</p>
<p>A rule with probability <em>p</em> is represented as a negative log probability, i.e. </p>
<pre class="fragment"> X -&gt; &lt; A , B &gt; / - log(p)
</pre><p>with n-gram language model scores encoded similarly, i.e. as costs -log P(w|h) for word <em>w</em> with LM history <em>h</em>. Costs are accumulated at the path level, so that the shortest path through the output FSA accepts the highest scoring hypothesis under the translation grammar and the language model. Hence the use of <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/ShortestPathDoc">ShortestPath</a> to extract the best scoring hypothesis under the tropical semiring.</p>
<p>The <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/PushDoc">Push</a> operation can be used to accumulate weights at the path level within the shortest path fst: </p>
<pre class="fragment"> &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstpush --push_weights --to_final | fstprint --isymbols=wmaps/wmt13.en.wmap
 0       1       &lt;s&gt;     1
 1       2       republican      9121
 2       3       strategy        384
 3       4       of      6
 4       5       resistance      2756
 5       6       to      7
 6       7       the     3
 7       8       renewal 4144
 8       9       of      6
 9       10      obamas  1458528
 10      11      election        1341
 11      12      &lt;/s&gt;    2
 12      57.4707222,-8.03808594
</pre><p>HiFST uses the lexicographic semiring (see [<a class="el" href="md_Tutorial.html#Roark2011">Roark2011</a>]) of two tropical weights to keep track of the translation score and the language model score.</p>
<p>The lexicographic semiring is a pair of real-valued weights, and in this application the first component, 54.4707222, contains the sum of the translation grammar scores and the language model. The second component, -8.03808594, is the translation grammar score alone; see Section 5.1 of [<a class="el" href="md_Tutorial.html#Allauzen2014">Allauzen2014</a>]. The lexicographic semiring is such that these scores are computed correctly at the path level:</p>
<ul>
<li>The cost of the shortest path found by <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ShortestPathDoc">ShortestPath</a> is that of the best hypothesis under the sum of the translation grammar and language model scores</li>
<li>In the lexicographic semiring, when the path weight is pushed to the final state:<ul>
<li>the first weight component is the correct combined translation and language model score</li>
<li>the second weight component is the best translation score over all possible derivations that could have generated this hypothesis</li>
</ul>
</li>
</ul>
<p>One advantage of this representation is that it is easy to remove the language model score prior to <a href="#lmrescore">rescoring the lattice with a new language model</a> simply by mapping the second component of the lexicographic weight to a plain tropical weight prior to composition with a weighted finite state automaton (WFSA) containing the new language model scores.</p>
<ul>
<li>N.B. Also see (<a class="el" href="md_Tutorial.html#lmert_veclats_tst">Notes on Tropical Sparse Tuple Vector Weights</a>)</li>
</ul>
<h2><a class="anchor" id="basic_toplevelpruning"></a>
Admissible pruning / top-level pruning</h2>
<p>HiFST can prune translation lattices prior to saving them to disk. Pruning is done using the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/PruneDoc">Prune</a> operation. Pruning in this case is admissible, since it is performed after translation and language model scores have been completely applied. Low-scoring hypotheses are discarded, but no search errors are introduced by this pruning. Top-level pruning is described in detail in Section 2.2.2, [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>].</p>
<p>Top-level pruning is controlled by the <code>--hifst.prune</code> option. In the previous examples, <code>--hifst.prune</code> was set to 9. If we use the default (3.40282347e+38), then the output lattice size becomes very large. For example, compare lattices in <code>exp1/</code> generated with <code>prune=9</code> vs. unpruned lattices in <code>exp2/</code>: </p>
<pre class="fragment">&gt; hifst.O2 --config=configs/CF.baseline.outputnoprune &amp;&gt; log/log.baseline.outputnoprune
&gt; du -sh output/exp.baseline/LATS/1.fst.gz output/exp.baseline.outputnoprune/LATS/1.fst.gz
8.0K  output/exp.baseline/LATS/1.fst.gz
56K   output/exp.baseline.outputnoprune/LATS/1.fst.gz
&gt; du -sh output/exp.baseline/LATS/2.fst.gz output/exp.baseline.outputnoprune/LATS/2.fst.gz
84K output/exp.baseline/LATS/2.fst.gz
3.8M        output/exp.baseline.outputnoprune/LATS/2.fst.gz
</pre><p>The <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/FstQuickTour#Printing_Drawing_and_Summarizing">fstinfo</a> command also indicates much larger outputs: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/2.fst.gz | fstinfo | grep \#
# of states                                       3312
# of arcs                                         11365
# of final states                                 1
# of input/output epsilons                        0
# of input epsilons                               0
# of output epsilons                              0
# of accessible states                            3312
# of coaccessible states                          3312
# of connected states                             3312
# of connected components                         1
# of strongly conn components                     3312

&gt; zcat output/exp.baseline.outputnoprune/LATS/2.fst.gz | fstinfo | grep \#
# of states                                       39270
# of arcs                                         584446
# of final states                                 1
# of input/output epsilons                        0
# of input epsilons                               0
# of output epsilons                              0
# of accessible states                            39270
# of coaccessible states                          39270
# of connected states                             39270
# of connected components                         1
# of strongly conn components                     39270
</pre><p>The unpruned lattices are much bigger, and contain many translation hypotheses, although the top scoring hypotheses should be unchanged by this form of pruning, as is the case in this example: </p>
<pre class="fragment">&gt; head output/exp.baseline/hyps output/exp.baseline.outputnoprune/hyps
==&gt; output/exp1/hyps &lt;==
1 9121 384 6 2756 7 3 4144 6 1458528 1341 2 
1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2 

==&gt; output/exp.baseline.outputnoprune/hyps &lt;==
1 9121 384 6 2756 7 3 4144 6 1458528 1341 2 
1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2 
</pre><h2><a class="anchor" id="multithread"></a>
Multithreading</h2>
<p>(* note that the timing results here are illustrative only.)</p>
<p>HiFST uses <a href="http://www.boost.org/doc/libs/1_38_0/doc/html/thread.html">Boost.Thread</a> to enable multithreading. This is disabled by default, but can enabled using the flag <code>--nthreads=N</code> . If set, each source language sentence is translated simultaneously on its own thread (trimmed to the number of CPUs available). The translation grammar and language model are kept in shared memory.</p>
<p>To see the effects of multithreading on speed and memory use, the baseline configuration is run over the first twenty sentences without multithreading: </p>
<pre class="fragment"> &gt; time hifst.O2 --config=configs/CF.baseline --range=1:20 
</pre><p>Processing time is 140 seconds and maximum memory use is about 0.3GB. In the same decoder configuration but with 2 threads </p>
<pre class="fragment"> &gt; time hifst.O2 --config=configs/CF.baseline --range=1:20 --nthreads=2
</pre><p>processing time is reduced to 90 seconds with maximum memory use of about 0.5GB.</p>
<p>In these examples, both the LM and translation grammar are relatively small, and so there is not a great deal of gain from keeping them in shared memory. But in larger tasks, multithreading can be a significant advantage.</p>
<h2><a class="anchor" id="server"></a>
Client-Server Mode (Experimental)</h2>
<p>HiFST can run in server mode. </p>
<pre class="fragment"> &gt; hifst.O2 --config=configs/CF.baseline.server &amp;&gt; log/log.server &amp;
 &gt; pid=$! # catch the server pid 
</pre><p>Note that in this particular configuration, both source and target wordmaps are loaded, so Hifst can read tokenized Russian text and produce tokenized English translations (see options <code>--prepro.wordmap.load</code> and <code>--postpro.wordmap.load</code>). Also, to ensure that CYK parser never fails, out of vocabulary (OOV) words must be detected (<code>--ssgrammar.addoovs.enable</code>) and sentence markers (<code>&lt;s&gt;</code>,<code>&lt;/s&gt;</code>) have to be added on the fly, as the shallow grammar relies on them (i.e. <code>S 1 1</code>).</p>
<p>With the <code>hifst-client.O2</code> binary, we can read Russian tokenized text (<code>RU/RU.tune</code>) and submit translation requests to the server. The output is stored in a file specified by the client tool (<code>--target.store</code>). </p>
<pre class="fragment">&gt; sleep 60 # make sure to wait for the server to finish loading, otherwise clients will fail
&gt; hifst-client.O2 --config=configs/CF.baseline.client --range=200:5:300 --target.store=output/exp.clientserver/translation1.txt &amp;&gt; log/log.client1 &amp;
# Connect to localhost, port=1205 and translate a bunch of sentences. Lets do this in background, just for fun
# Note that the localhost setting is in the config file; this can point to another machine, of course
&gt; pid2=$! 

&gt; hifst-client.O2 --config=configs/CF.baseline.client --range=1:50,100,1300 --target.store=output/exp.clientserver/translation2.txt &amp;&gt; log/log.client2 &amp;
# In the meantime, we request another 52 translations...
&gt; wait $pid2

&gt; kill -9 $pid      
# We are finished -- kill the server 

&gt; head -5 output/exp.clientserver/translation2.txt
parliament supports amendment giving freedom tymoshenko
amendment , which led to a liberation located imprisoned former prime minister was rejected during second reading bill mitigating sentences for economic offences .
sentence still ultimate ; the court will review appeal tymoshenko in december .
proposal cancel article 365 criminal-procedural codex whereby former prime minister was convicted was supported 147 members parliament .
winning libya</pre><h2><a class="anchor" id="lpruning"></a>
Local pruning / pruning in search</h2>
<p>Local pruning controls processing speed and memory use during translation. Only enough details are reviewed here to describe how HiFST performs pruning in search; for a detailed discussion of local pruning and pruning in search, see Section 2.2.2 of [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>].</p>
<p>Given a translation grammar and a source language sentence, HiFST first constructs a Recursive Transition Network (RTN) representing the translation hypotheses [<a class="el" href="md_Tutorial.html#Iglesias2009">Iglesias2009</a>, <a class="el" href="md_Tutorial.html#Iglesias2011">Iglesias2011</a>]. This is done as part of a modified CYK algorithm used to parse the source sentence under the translation grammar. The RTN is then <em>expanded</em> to an equivalent WFSA via the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation. This WFSA contains the translation hypotheses along with their scores under the translation grammar. We refer to this as the `top-level' WFSA, because it is associated with the top-most cell in the CYK grid. This top-level WFSA can be pruned after composition with the language model, as described in the discussion of <a class="el" href="md_Tutorial.html#basic_toplevelpruning">Admissible pruning / top-level pruning</a>. We refer to this as <em>exact search</em> or <em>exact translation</em>. In exact translation, no translation hypotheses are discarded prior to applying the complete translation and language model scores.</p>
<p>Exact translation can be done under some combinations of translation grammars, language models, and language pairs. In particular, the <a class="el" href="md_Tutorial.html#tgrammars_shallow">Shallow-N Translation Grammars</a> were designed for exact search. However attempting exact translation under many translation grammars would cause either the <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation or the subsequent language model composition to become computationally intractable. We therefore have developed a pruning strategy that prunes the RTN during its construction.</p>
<p>The RTN created by HiFST can be described as follows:</p>
<ul>
<li><img class="formulaInl" alt="$X$" src="form_0.png"/> is the set of non-terminals in the translation grammar, with <img class="formulaInl" alt="$S$" src="form_1.png"/> as the root</li>
<li><img class="formulaInl" alt="$\Sigma$" src="form_2.png"/> is the target language vocabulary, i.e. the terminals in the target language</li>
<li><img class="formulaInl" alt="$I$" src="form_3.png"/> is the length of the source sentence <img class="formulaInl" alt="$s$" src="form_4.png"/>, i.e. <img class="formulaInl" alt="$s = s_0...s_{I - 1}$" src="form_5.png"/></li>
<li>A new set of non-terminals is defined as <img class="formulaInl" alt="$N = \{ (x,i,j) : x \in X , 0 <= i <= j < I \}$" src="form_6.png"/><ul>
<li>Note that <img class="formulaInl" alt="$(S,0,I-1) \in N$" src="form_7.png"/></li>
</ul>
</li>
<li><img class="formulaInl" alt="$(T_u)_{u \in N}$" src="form_8.png"/>, is a family of WFSAs with input alphabet <img class="formulaInl" alt="$\Sigma \cup N$" src="form_9.png"/><ul>
<li>Each <img class="formulaInl" alt="$T_u$" src="form_10.png"/> with <img class="formulaInl" alt="$u = (x, i, j)$" src="form_11.png"/>, is a WFSA that describes all applications of translation rules with left-hand side non-terminal <img class="formulaInl" alt="$x$" src="form_12.png"/> that span the substring <img class="formulaInl" alt="$s_i ... s_j$" src="form_13.png"/></li>
<li><img class="formulaInl" alt="$T_u$" src="form_10.png"/> is associated with the CYK grid cell associated with source space <img class="formulaInl" alt="$[i,j]$" src="form_14.png"/> and headed by non-terminal <img class="formulaInl" alt="$x$" src="form_12.png"/></li>
</ul>
</li>
<li>The top-level RTN is defined as <img class="formulaInl" alt="$R_{(S,0,I-1)} = (N, \Sigma, (T_u)_{u \in N}, (S,0,I-1))$" src="form_15.png"/>.<ul>
<li>The root symbol of this RTN is <img class="formulaInl" alt="$(S,0,I-1)$" src="form_16.png"/>.</li>
<li>The WFSA <img class="formulaInl" alt="$T_{(S,0,I-1)}$" src="form_17.png"/> represents all applications of translation rules that span the entire sentence and are rooted with non-terminal <img class="formulaInl" alt="$S$" src="form_1.png"/>.</li>
</ul>
</li>
</ul>
<p>Exact translation is achieved if every <img class="formulaInl" alt="$T_u$" src="form_10.png"/> is complete (i.e. if no pruning is done) prior to the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation on the RTN <img class="formulaInl" alt="$R_{(S,0,I-1)}$" src="form_18.png"/>. This produces a WFSA that contains all translations that can be produced under the translation grammar.</p>
<p>The RTN pruning strategy relies on noting that each of the WFSAs <img class="formulaInl" alt="$T_{u'}$" src="form_19.png"/>, <img class="formulaInl" alt="$u' = (x', i', j')$" src="form_20.png"/>, also defines an RTN <img class="formulaInl" alt="$R_{u'}$" src="form_21.png"/>, as follows:</p>
<ul>
<li>Define a subset of non-terminals <img class="formulaInl" alt="$N' = \{ (x,i,j) : x \in X , i' <= i <=j < j' \}$" src="form_22.png"/> , i.e. <img class="formulaInl" alt="$N' \subset N$" src="form_23.png"/></li>
<li><img class="formulaInl" alt="$R_{u'} = (N', (T_u)_{u \in N'}, (x', i', j') )$" src="form_24.png"/><ul>
<li>The root symbol of this RTN is <img class="formulaInl" alt="$(x', i', j')$" src="form_25.png"/></li>
</ul>
</li>
</ul>
<p>The <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation can be applied to the RTNs <img class="formulaInl" alt="$R_{u'}$" src="form_21.png"/> to produce an WFSA containing all translations of the source string <img class="formulaInl" alt="$S_{i'} ... S_{j'}$" src="form_26.png"/> using derivations rooted in the non-terminal <img class="formulaInl" alt="$x'$" src="form_27.png"/>. This WFSA can be pruned and used in place of the original <img class="formulaInl" alt="$T_{u'}$" src="form_19.png"/>.</p>
<p>Because of the possibility of search errors we refer to this as 'local pruning' or inadmissible pruning. There is the possibility that pruning any of the <img class="formulaInl" alt="$T_u$" src="form_10.png"/> may possibly cause some good translations to be discarded. For this reason it is important to tune the pruning strategy for the translation grammar and language model. Once pruning has been set, the benefits are</p>
<ul>
<li>faster creation of the top-level WFSA via the <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation</li>
<li>faster composition of the translation WFSA with the language model</li>
<li>less memory used in RTN construction and language model composition</li>
</ul>
<p>Local pruning should be done under the combined translation and the language model scores, rather than under the translation grammar scores alone alone. However, the LM used in local pruning can be relatively weak. For example, if the main language model used in translation is a 4-gram, perhaps a 3-gram or even a bigram language model could be used in local pruning. Using a smaller language model will make pruning faster, as will an efficient scheme to remove the scores of the language models used in pruning. The lexicographic semiring, see <a class="el" href="md_Tutorial.html#basic_scores">Scores, Costs, and Semirings</a>, makes this last operation easy.</p>
<h3><a class="anchor" id="local_prune"></a>
Local Pruning Algorithm</h3>
<p>HiFST monitors the size of the <img class="formulaInl" alt="$T_u$" src="form_10.png"/> during translation. Any of these automata that exceed specified thresholds are converted to WFSAs and pruned. Subsequent expansion of the RTN <img class="formulaInl" alt="$R_{(S,0,I-1)}$" src="form_18.png"/> is then done with respect to the pruned versions of <img class="formulaInl" alt="$T_u$" src="form_10.png"/>.</p>
<p>Local pruning is controlled via the following HiFST parameters: </p>
<pre class="fragment">hifst.localprune.enable=yes # must be set to activate local pruning
hifst.localprune.conditions=NT_1,span_1,size_1,threshold_1,...,NT_N,span_N,size_N,threshold_N
hifst.localprune.lm.load=lm_1,...lm_K 
hifst.localprune.lm.featureweights=scale_1,...,scale_K 
hifst.localprune.lm.wps=wp_1,...,wp_K 
</pre><p>In the above, an arbitrary number N of tuples (<code>NT_n</code>, <code>span_n</code>, <code>size_n</code>, <code>threshold_n</code>) can be provided; similarly, an arbitrary number K of language model parameters (<code>lm_k</code>, <code>scale_k</code>, <code>wp_k</code>) can also be used in pruning.</p>
<p>Pruning is applied during construction of the RTN, as follows:</p>
<ul>
<li>If any <img class="formulaInl" alt="$T_u$" src="form_10.png"/> satisfies the following conditions for any parameter set (<code>NT_n</code>, <code>span_n</code>, <code>size_n</code>, <code>threshold_n</code>), n=1,...,N<ul>
<li>NT_n = X</li>
<li>span_n &lt;= j-i</li>
<li>size_n &lt;= number of states of <img class="formulaInl" alt="$T_u$" src="form_10.png"/>, computed via <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <code>NumStates()</code></li>
</ul>
</li>
<li>then <img class="formulaInl" alt="$T_u$" src="form_10.png"/> is pruned as follows:<ul>
<li>OpenFst <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> converts <img class="formulaInl" alt="$R_u$" src="form_28.png"/> to a WFSA</li>
<li><a href="http://www.openfst.org/twiki/bin/view/FST/RmEpsilonDoc">RmEpsilon</a>,<a href="http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc">Deteminize</a>, and <a href="http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc">Minimize</a> generate a compacted WFSA</li>
<li><a href="http://www.openfst.org/twiki/bin/view/FST/ComposeDoc">Composition</a> with K language model(s) WFSAs<ul>
<li>The parameters (<code>lm_k</code>, <code>scale_k</code>, <code>wp_k</code>) specify the language models, language model scale factors, and word penalties to be applied</li>
</ul>
</li>
<li>OpenFst <a href="http://www.openfst.org/twiki/bin/view/FST/PruneDoc">Prune</a> is applied with threshold <code>threshold_n</code></li>
<li>Language model scores are removed by copying component weights in the lexicographic semiring, see <a class="el" href="md_Tutorial.html#basic_scores">Scores, Costs, and Semirings</a></li>
<li><a href="http://www.openfst.org/twiki/bin/view/FST/RmEpsilonDoc">RmEpsilon</a>,<a href="http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc">Deteminize</a>, and <a href="http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc">Minimize</a>, yielding a pruned WFSA <img class="formulaInl" alt="$T_u$" src="form_10.png"/> with only translation scores and target language symbols</li>
</ul>
</li>
<li>The pruned version of <img class="formulaInl" alt="$T_u$" src="form_10.png"/> is then used in place of the original version in the RTN</li>
</ul>
<h3><a class="anchor" id="lpruning_effects"></a>
Effect on Speed, Memory, Scores</h3>
<p>Pruning in search is particularly important when running HiFST with grammars that are more powerful than the shallow grammar used in earlier examples.</p>
<p>For example, HiFST can be run with a full Hiero grammar, while monitoring memory consumption via the UNIX top command: </p>
<pre class="fragment"> &gt; (time hifst.O2 --config=configs/CF.hiero) &amp;&gt; log/log.hiero
</pre><p>The memory use is approximately 2GB and translation takes approximately 1m45s. (The resource consumption may vary depending on your hardware, we provide these numbers to illustrate the effect of local pruning.)</p>
<p>If translation is performed with the same grammar and language model, but with local pruning, </p>
<pre class="fragment"> &gt; (time hifst.O2 --config=configs/CF.hiero.localprune) &amp;&gt; log/log.hiero.localprune
</pre><p>then the memory consumption is reduced to under 300MB and the processing time to approximately 25s. Inspecting the log file indicates that local pruning was applied to 18 sublattices for the second sentence: </p>
<pre class="fragment"> &gt; tail -n 16 log/log.hiero.localprune | head -n 12
 Fri May  9 15:20:38 2014: run.INF:=====Translate sentence 1:1 20870 2447 5443 50916 78159 3621 2
 Fri May  9 15:20:38 2014: run.INF:Loading hierarchical grammar: G/rules.hiero.gz
 Fri May  9 15:20:38 2014: run.INF:loading LM=M/lm.4g.mmap
 Fri May  9 15:20:38 2014: run.INF:loading LM=M/lm.3g.mmap
 Fri May  9 15:20:38 2014: run.INF:Stats for Sentence 1: local pruning, number of times=0
 Fri May  9 15:20:38 2014: run.INF:End Sentence ******************************************************
 Fri May  9 15:20:38 2014: run.INF:Translation 1best is: 1 3 9121 384 6 2756 7 3 4144 6 159312 42 1341 2 
 Fri May  9 15:20:38 2014: run.INF:=====Translate sentence 2:1 1716 20196 95123 154 1049 6778 996 9 239837 7 1799 4 2
 Fri May  9 15:20:47 2014: run.INF:Stats for Sentence 2: local pruning, number of times=18
 Fri May  9 15:20:55 2014: run.INF:End Sentence ******************************************************
 Fri May  9 15:20:56 2014: run.INF:Translation 1best is: 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2 
 Fri May  9 15:20:56 2014: main.INF:hifst.O2 ends!
</pre><p>In this case, local pruning has no effect on the translations produced: </p>
<pre class="fragment"> &gt; head output/exp.hiero.localprune/hyps output/exp.hiero/hyps
 ==&gt; output/exp.hiero.localprune/hyps &lt;==
 1 3 9121 384 6 2756 7 3 4144 6 159312 42 1341 2 
 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2 

 ==&gt; output/exp.hiero/hyps &lt;==
 1 3 9121 384 6 2756 7 3 4144 6 159312 42 1341 2 
 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2 
</pre><p>The effect of pruning can be more dramatic on longer, more difficult to translate sentences. For example, the third sentence in this set is difficult to translate under the full Hiero grammar without pruning, although it can be translated using local pruning as </p>
<pre class="fragment"> &gt; (time hifst.O2 --config=configs/CF.hiero.localprune --range=3:3) &amp;&gt; log/log.hiero.localprune2
</pre><p>Even with local pruning, the processing time for this one sentence is over 4 minutes.</p>
<p>By comparison, translation is much faster with much more aggressive local pruning, which we introduce via command line options to override the settings in the configuration file: </p>
<pre class="fragment"> &gt; (time hifst.O2 --config=configs/CF.hiero.localprune --range=3:3 --hifst.lattice.store=output/exp.hiero.localprunemore/LATS/?.fst.gz --target.store=output/exp.hiero.localprunemore/hyps --hifst.localprune.conditions=X,3,10,1,V,3,10,1) &amp;&gt; log/log.hiero.localprune3
</pre><p>Translation finishes in less than 6 seconds, but this more aggressive local pruning changes the translation hypothesis: </p>
<pre class="fragment"> &gt; zcat output/exp.hiero.localprune/LATS/3.fst.gz | printstrings.O2 -w --semiring=lexstdarc -m wmaps/wmt13.en.wmap 2&gt;/dev/null
 &lt;s&gt; however , in the heart of the take the last myth , arguing that the rare cases of fraud in elections in the united states , the deaths of a lightning strike . &lt;/s&gt;  128.842,-0.150391

 &gt; zcat output/exp.hiero.localprunemore/LATS/3.fst.gz | printstrings.O2 -w --semiring=lexstdarc -m wmaps/wmt13.en.wmap 2&gt;/dev/null
 &lt;s&gt; however , in the heart of the take the last myth , arguing that a rare cases of fraud in elections in the united states , the deaths of a lightning strike . &lt;/s&gt;  130.054,-0.943359
</pre><p>The best hypothesis generated with less local pruning in <code>exp.hiero.localprune/</code> has a combined translation and language model score of 128.842 . This hypothesis does not survive more local pruning in <code>exp.hiero.localprunemore/</code> , where the best hypothesis has a higher combined score of 130.054 .</p>
<h1><a class="anchor" id="chopping"></a>
Source Sentence Chopping</h1>
<p>Long source sentences make the translation process slow and expensive in memory consumption; see [<a class="el" href="md_Tutorial.html#Allauzen2014">Allauzen2014</a>] for a discussion of how source sentence length affects computational complexity and memory use by HiFST and HiPDT. There are various strategies for controlling translation complexity; pruning has been discussed (<a class="el" href="md_Tutorial.html#lpruning">Local pruning / pruning in search</a>), and it is also possible to set the maximum span and gap spans allowed in translation so as to control computational complexity. However translation quality can be affected if pruning is too heavy or if span constraints are set too aggressively.</p>
<p>An alternative approach is to 'chop' long sentences into shorter segements which can then be translated separately. If the sentence chopping is done carefully, the impact on the translation quality can be minimized. The benefits to chopping are faster translation that consumes less memory. The potential drawbacks are twofold: chopping can prevent the search procedure from finding good hypothesis under the grammar, and care must be taken to correctly apply the target language model at the sentence level.</p>
<h2><a class="anchor" id="chopping_gb"></a>
Grammar-based Sentence Chopping</h2>
<p>Chopping can be done by inserting the special 'chop' symbol '0' in the source sentence, and then translating with a modified grammar. The chopping grammar is constructed so that translation rules are not applied across the chopping points, thus limiting the space of translation that are generated. Conceptually, translation proceeds as:</p>
<ol type="1">
<li>the translation grammar is applied separately to source sentence segments demarcated by chop symbols</li>
<li>local pruning can be applied to the translations of these segments</li>
<li>the resulting WFSAs containing translations of the segments are concatenated under the chopping grammar, possibly with local pruning</li>
<li>the language model is applied</li>
<li>top-level, admissible pruning is done under the translation and languaage model scores</li>
</ol>
<p>In this way the FSTs produced by translating the segments are concatenated prior to application of the target language model; in this way the language model context is not broken by the source sentence chopping.</p>
<p>As an example, a grammar modified for chopping contains the following rules (without weights): </p>
<pre class="fragment"> R 1 1
 R R_D_X R_D_X
 R R_X R_X

 T 0 0
 T T_D_X T_D_X
 T T_X T_X

 S S_U S_U
 S Q Q
 Q R R
 U T T
</pre><p>The rules in the first block above are similar to those used in the usual Hiero grammar, with the original '<code>S</code>' changed to '<code>R</code>'. These rules are responsible for concatenating the partial translations of the source sentence, starting from the sentence-start symbol '1', up to but not including the first instance of the chopping symbol '0'.</p>
<p>Each subsequent sequence of source words starting with symbol '0', is handled in a similar way by the second block of rules above. Note that the only rule that can be applied to the input symbol '0' is '<code>T 0 0</code>', making the translation of each chopped segment independent. This makes use of the OpenFST convention of mapping 0 to epsilon: the 0's in the input are parsed as regular symbols by HiFST, while 0's on the output side are mapped to epsilons and ignored in composition with the language model.</p>
<p>The third block of rules above will join together the results obtained for each chopped segment. As with the glue rule '<code>S</code>' in the usual Hiero grammar, it is necessary to allow this new set of rules to be applied to any span. This is done by setting </p>
<pre class="fragment">  cykparser.ntexceptionsmaxspan=S,Q,R,T,U
</pre><p>The additional mapping provided by the last two rules controls the pruning applied to the top CYK cell relative to each chopped segment: </p>
<pre class="fragment">  hifst.localprune.conditions=Q,1,100,12,U,1,100,12,X,5,10000,9,V,3,20000,9
</pre><p>In the above example tighter parameters are chosen for '<code>Q</code>' and '<code>U</code>' to force pruning. In this way the final lattice obtained by concatenation is prevented from growing too large. However a wider beam (12) with respect to the other cell types is used, to avoid discarding too many potentially useful hypotheses.</p>
<p>It is possible to specify explicitly that FSTs generated for rules with LHS '<code>X</code>' or '<code>V</code>' can be kept as pointers rather then expanded in the FST (RTN) that is built for a higher CYK cell. This is achieved setting </p>
<pre class="fragment">  hifst.replacefstbyarc=X,V 
  hifst.replacefstbyarc.exceptions=S,R,T
</pre><p>The second line above prevents substitution for rules with LHS '<code>S</code>', '<code>R</code>' and '<code>T</code>'. It is better to have a fully expanded FST for these rules for more effective optimisation (Determinization and Minimisation).</p>
<h3><a class="anchor" id="chopping_eg"></a>
Converting Grammars and Input Text for Chopping</h3>
<p>The usual Hiero grammar can be converted for chopping, as follows; note that no-cost, 0 valued, weights are added to rules :</p>
<p>First, create the chopping and glue rules: </p>
<pre class="fragment"> &gt; (echo "T T_D_X T_D_X 0" ; echo "T T_X T_X 0" ; echo "T 0 0 0") &gt; G/rules.hiero.chop
 &gt; (echo "S S_U S_U 0" ; echo "U T T 0" ; echo "Q R R 0" ; echo "S Q Q 0") &gt;&gt; G/rules.hiero.chop
 &gt; cat G/rules.hiero.chop
 T T_D_X T_D_X 0
 T T_X T_X 0
 T 0 0 0
 S S_U S_U 0
 U T T 0
 Q R R 0
 S Q Q 0
</pre><p>Next, append all rules, mapping glue rules with LHS S to LHS R: </p>
<pre class="fragment"> &gt; zcat G/rules.hiero.gz | sed 's,S,R,g' &gt;&gt; G/rules.hiero.chop
 &gt; gzip G/rules.hiero.chop
</pre><p>The source text (<code>RU/RU.set1.chop.idx</code>) will be chopped simply inserting the chopping marker '0' after each comma (integer mapped to 3 in the Russian wordmap); this is a simplistic approach that is easily implemented for this demonstration: </p>
<pre class="fragment"> &gt; sed 's, 3 , 3 0 ,g' RU/RU.set1.idx &gt; RU/RU.set1.chopping.idx

 &gt; diff RU/RU.set1.idx RU/RU.set1.chopping.idx | head -n4
 3c3
 &lt; 1 109 5 458 756435 1225 1358 60145 3 12725 3 11 3678 66369 7 1799 5 1317 2946 45 32023 3 75 3678 1102 24 10272 28960 4 2
 ---
 &gt; 1 109 5 458 756435 1225 1358 60145 3 0 12725 3 0 11 3678 66369 7 1799 5 1317 2946 45 32023 3 0 75 3678 1102 24 10272 28960 4 2
</pre><p>The following command will translate lines 3,12,19 in the Russian integer-mapped file RU/RU.set1.chop.idx : </p>
<pre class="fragment"> # Run HiFST, with chopping.  Input is the chopped source RU/RU.set1.chopping.idx.
 # hypotheses are written to output/exp.chopping/chop/hyps and lattices to output/exp.chopping/chop/LATS/
 &gt; (time hifst.O2 --config=configs/CF.hiero.chopping) &amp;&gt; log/log.chopping.chop
</pre><p>Now we decode again keeping the same configuration (same grammar and language model), but with the non-chopped version of the input: </p>
<pre class="fragment"> # Run HiFST, without chopping.  Input is the original, unchopped source RU/RU.set1.idx
 # hypotheses are written to output/exp.chopping/nochop/hyps and lattices to output/exp.chopping/nochop/LATS/
 &gt; (time hifst.O2 --config=configs/CF.hiero.chopping --source.load=RU/RU.set1.idx --target.store=output/exp.chopping/nochop/hyps --hifst.lattice.store=output/exp.chopping/nochop/LATS/?.fst.gz) &amp;&gt; log/log.chopping.nochop
</pre><p>Comparing the time and memory consumption of the two experiments shows that source-sentence chopping is significantly faster and uses far less memory; in particular, local pruning is required less often under the chopping grammar: </p>
<pre class="fragment">                                         Number of local prunings
 Input      Tot time      Max memory    Sent 3    Sent 12   Sent 19
 --------   --------      ----------    ------    -------   -------
 Unchopped  22m 37s         5.4Gb        92        106       168
 Chopped     3m 57s         0.8Gb        34         46        20                                  
</pre><p>However, chopping restricts the space of translations. Looking at the scores of the best translation hypotheses, chopping the source sentence prevents the decoder from finding the best scoring hypothesis under the grammar; for the third sentence, the hypothesis produced without chopping has a lower (i.e. better) combined cost (128.842) than the hypothesis produced with chopping (130.309): </p>
<pre class="fragment"> # find the score of the best hypothesis for the 3rd sentence, without chopping
 &gt; zcat output/exp.chopping/nochop/LATS/3.fst.gz | printstrings.O2 --semiring=lexstdarc
 1 106 4 9 3 1552 6 3 96 3 200 8072 4 5452 10 3 4143 535 6 1206 9 628 9 3 232 56 4 3 2723 6 11 21441 2645 5 2       128.842,-0.150391
 # find the score of the best hypothesis for the 3rd sentence, with chopping
 &gt; zcat output/exp.chopping/chop/LATS/3.fst.gz | printstrings.O2 --semiring=lexstdarc
 1 106 4 9 3 1552 6 3 96 3 200 8072 4 5452 10 3 4143 535 6 1206 9 628 9 3 232 56 4 3 2723 6 11 21441 2645 5 2       130.309,1.31641
</pre><h2><a class="anchor" id="chopping_sseg"></a>
Chopping by Explicit Source Sentence Segmentation</h2>
<p>It is also possible to segment and translate each segment completely independently, as shown in this example for sentence 3. Here, the original Russian sentence is chopped into three shorter sentences which are to be translated independently, as follows: </p>
<pre class="fragment"> # split sentence 3 into 4 separate sentences
 &gt; awk 'NR==3' RU/RU.set1.idx | sed 's, 3 , 3 2\n1 ,g'  &gt; RU/RU.sent3.chopped
 &gt; cat RU/RU.sent3.chopped
 1 109 5 458 756435 1225 1358 60145 3 2
 1 12725 3 2
 1 11 3678 66369 7 1799 5 1317 2946 45 32023 3 2
 1 75 3678 1102 24 10272 28960 4 2

 # run HiFST over all segments
 &gt; hifst.O2 --config=configs/CF.hiero.chopping --source.load=RU/RU.sent3.chopped --target.store=output/exp.chopping/sent3/hyps --hifst.lattice.store=output/exp.chopping/sent3/LATS/seg.?.fst --range=1:4

 # concatenate output lattices
 &gt; fstconcat output/exp.chopping/sent3/LATS/seg.1.fst output/exp.chopping/sent3/LATS/seg.2.fst | fstconcat - output/exp.chopping/sent3/LATS/seg.3.fst | fstconcat - output/exp.chopping/sent3/LATS/seg.4.fst &gt; output/exp.chopping/sent3/LATS/sent.fst

 # print output string
 &gt; cat output/exp.chopping/sent3/LATS/sent.fst | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
 &lt;s&gt; however , in the middle of the last myth , believe &lt;/s&gt; &lt;s&gt; by saying , &lt;/s&gt; &lt;s&gt; the cases of fraud in elections in the united states , a rare &lt;/s&gt; &lt;s&gt; the deaths of a lightning strike . &lt;/s&gt;  141.166,-12.5742
</pre><p>In the above example, the language model is applied separately to the translations of each segment leading to the substrings "&lt;/s&gt; &lt;s&gt;" in the hypothesis. A transducer can be built to remove these from the output lattice, as follows </p>
<pre class="fragment"> &gt; mkdir tmp
 &gt; echo -e "0\t1\t1\t1" &gt; tmp/strip_1_2.txt
 &gt; echo -e "1\t2\t2\t0" &gt;&gt; tmp/strip_1_2.txt
 &gt; echo -e "2\t1\t1\t0" &gt;&gt; tmp/strip_1_2.txt
 &gt; echo -e "2\t2\t0\t0" &gt;&gt; tmp/strip_1_2.txt
 &gt; echo -e "1\t3\t2\t2" &gt;&gt; tmp/strip_1_2.txt
 &gt; echo -e "3" &gt;&gt; tmp/strip_1_2.txt
 &gt; awk '$2 != 1 &amp;&amp; $2 != 2 {printf "1\t1\t%d\t%d\n", $2,$2}' wmaps/wmt13.en.wmap &gt;&gt; tmp/strip_1_2.txt   
 &gt; fstcompile  --arc_type=tropical_LT_tropical tmp/strip_1_2.txt | fstarcsort &gt; tmp/strip_1_2.fst

 # apply the strip_1_2.fst transducer to the fst for sentence 3
 &gt; fstcompose output/exp.chopping/sent3/LATS/sent.fst tmp/strip_1_2.fst | fstproject --project_output | fstrmepsilon &gt; output/exp.chopping/sent3/LATS/sent_no12.fst

 # look at output
 &gt; cat output/exp.chopping/sent3/LATS/sent_no12.fst | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
 &lt;s&gt; however , in the middle of the last myth , believe by saying , the cases of fraud in elections in the united states , a rare the deaths of a lightning strike . &lt;/s&gt;  141.166,-12.5742
</pre><p>Note however that the language model score for this hypothesis are not correct, since the language model histories cannot span translations across the chopped segments.</p>
<p>To fix this, the applylm tool (see <a class="el" href="md_Tutorial.html#rescoring_lm">Efficient Language Model Rescoring</a>) can be used to remove and reapply the language model so that it spans the source segment translations. The following example simply removes the language model scores from output/exp.chopping/sent3/LATS/sent_no12.fst and then reapplies them via composition </p>
<pre class="fragment"> &gt; applylm.O2 --lm.load=M/lm.4g.mmap --lm.featureweights=1 --lm.wps=0.0 --semiring=lexstdarc --lattice.load=output/exp.chopping/sent3/LATS/sent_no12.fst --lattice.store=output/exp.chopping/sent?/LATS/sent_no12_rescore.fst --lattice.load.deletelmcost --range=3:3     
</pre><p>The rescored output is written to <code>output/exp.chopping/sent3/LATS/sent_no12_rescore.fst</code> with correctly applied language model scores. The total translation cost is much lower (better) than when segment hypotheses are simply combined (i.e. 115.855 vs. 141.166): </p>
<pre class="fragment"> &gt; cat output/exp.chopping/sent3/LATS/sent_no12_rescore.fst | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
 &lt;s&gt; however , in the heart of the take the last myth , arguing that the rare cases of fraud in elections in the united states , the deaths of a lightning strike . &lt;/s&gt;       115.855,-13.1377</pre><h1><a class="anchor" id="mert"></a>
MERT - Features Only</h1>
<p>This section describes how to generate N-Best lists of features for use with MERT [<a href="http://aclweb.org/anthology/P/P03/P03-1021.pdf">Och 2003</a>].</p>
<h2><a class="anchor" id="mert_nblists"></a>
HiFST_nbestformert</h2>
<p>A script named <code>scripts/HiFST_nbestformert</code> is provided which can generate hypotheses and feature vectors that can be used by MERT. We use N-Best lists of depth N = 100 , set by the <code>prunereferenceshortestpath=</code> option in <code>configs/CF.mert.alilats.nbest</code>.</p>
<p>For this tutorial, the configuration files specify multithreading, and N-Best lists will be generated only for the first 2 sentences in RU/RU.tune.idx ; the script can be edited to process the entire file. The output is written to the file <code>output/exp.mert/nbest/nbest.list</code>. </p>
<pre class="fragment">&gt; scripts/HiFST_nbestformert
&gt; head -2 output/exp.mert/nbest/nbest.list; echo ....; tail -2 output/exp.mert/nbest/nbest.list
1 ||| parliament does not support the amendment , which gives you the freedom of tymoshenko |||         62.5442     10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000 -1.0000 0.0000 -7.0000 16.3076 40.5293
1 ||| the parliament does not support the amendment , which gives you the freedom of tymoshenko ||| 63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000 -1.0000 0.0000 -7.0000 17.0010 43.9482
....
2 ||| the amendment , which has led to the release of which is in jail , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . ||| 135.8217 24.0928 50.8281 -37.0000 -21.0000 -12.0000 0.0000 -1.0000 0.0000 -20.0000 100.6992 111.6357
2 ||| the amendment , which would have led to the release of which is in the jail of former prime minister , was rejected during the second reading of the bill to ease sentences for economic offences . |||        145.1169         23.7305  41.4756  -39.0000       -23.0000        -14.0000 0.0000 -1.0000 0.0000 -22.0000 93.0488 107.7188
</pre><p>The steps carried out in the <code>HiFST_nbestformert</code> script are described next.</p>
<h2><a class="anchor" id="mert_hyps"></a>
1. Hypotheses for MERT</h2>
<p>(note that this step is also done in <a class="el" href="md_Tutorial.html#lmert">Lattice MERT</a>)</p>
<ul>
<li>Input:<ul>
<li><code>RU/RU.tune.idx</code> &ndash; tuning set source language sentences</li>
<li><code>G/rules.shallow.vecfea.all.gz</code> &ndash; translation grammar</li>
<li><code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code> &ndash; target language model</li>
<li>language model and translation grammar feature weights (see configs/CF.mert.hyps)</li>
</ul>
</li>
<li>Output<ul>
<li><code>output/exp.mert/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized</li>
</ul>
</li>
</ul>
<p>This step runs HiFST in the usual way to generate a set of translation hypotheses which will be used in MERT. Note that M (the number of sentences to translate) is set to 5, just to make the tutorial steps run quickly. </p>
<pre class="fragment">&gt; M=5
# replace by M=1502 to process the entire tuning set
&gt; hifst.O2 --config=configs/CF.mert.hyps --range=1:$M &amp;&gt; log/log.mert.hyps
</pre><p>In this configuration, the grammar feature weights and the language model feature weights are applied on-the-fly to the grammar and language model as they are loaded. This allows feature vector weights to be changed at each iteration of MERT. This behaviour is specified through the following options in the CF.mert.hyps file, where we use the parameters from the baseline system: </p>
<pre class="fragment">[lm]
load=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap
featureweights=1.0
# Note that for only one language model, this parameter will always be set to 1.
# If there are multiple language models, the language model weights will be updated
# after each iteration of MERT

[grammar]
load=G/rules.shallow.vecfea.all.gz
featureweights=0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
# Note that this parameter vector should be updated after each iteration of MERT
# Updated versions can be provided via command line arguments
</pre><p>The translation grammar has its rules with unweighted feature vectors: </p>
<pre class="fragment">&gt; zcat G/rules.shallow.vecfea.all.gz | head -n 3
V 3 4 0.223527 0.116794 -1 -1 0 0 0 0 -1 1.268789 0.687159
V 3 4_3 3.333756 0.338107 -2 -1 0 0 0 0 -1 1.662178 3.363062
V 3 8 3.74095 3.279819 -1 -1 0 0 0 0 -1 3.741382 2.271445
</pre><p>The output lattices in <code>output/exp.mert/LATS</code> are acceptors containing word hypotheses, with weights in the form of the lexicographic semiring as described earlier. </p>
<pre class="fragment">&gt; zcat output/exp.mert/LATS/1.fst.gz | fstinfo | head -n 2
fst type                                          vector
arc type                                          tropical_LT_tropical

&gt; zcat output/exp.mert/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.all.wmap -w 2&gt;/dev/null
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      43.093,-19.4512
</pre><h2><a class="anchor" id="mert_nblist_derivations"></a>
2. Guided Translation / Forced Alignment</h2>
<ul>
<li>Input:<ul>
<li><code>RU/RU.tune.idx</code> &ndash; tuning set source language sentences</li>
<li><code>G/rules.shallow.all.gz</code> &ndash; translation grammar</li>
<li><code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code> &ndash; target language model</li>
<li><code>output/exp.mert/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized (from <a class="el" href="md_Tutorial.html#mert_hyps">1. Hypotheses for MERT</a>)</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.mert/nbest/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations (e.g. Fig. 7, [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>])</li>
</ul>
</li>
</ul>
<p>Alignment is the task of finding the derivations (sequences of rules) that can produce a given translation. HiFST performs alignment via constrained translation (see Section 2.3, [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>] for a detailed description). This command runs HiFST in alignment mode: </p>
<pre class="fragment">&gt; hifst.O2 --config=configs/CF.mert.alilats.nbest --range=1:$M &amp;&gt; log/log.mert.alilats.nbest
</pre><p>In alignment mode, HiFST constructs <em>substring acceptors</em> (see Fig. 8, [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>]). These are constructed for each sentence as follows:</p>
<ul>
<li>the lattice from Step 1 is loaded by HiFST</li>
<li>an N-Best list in the form of a WFSA is extracted (using <code>fstshortestpath</code>) under the translation and language model score from Step 1</li>
<li>weights are removed from N-Best WFSA</li>
<li>the WFSA is transformed to a substring acceptor</li>
</ul>
<p>The translation grammar is applied in the usual way, but the translations are intersected with the substring acceptors so that only translations in the N-Best lists are retained. This generates every possible derivation of each N-Best list entry.</p>
<p>This behaviour is specified by the following config file parameters: </p>
<pre class="fragment">[referencefilter]
load=output/exp.mert/LATS/?.fst.gz
# perform alignment against these reference lattices containing initial hypotheses
prunereferenceshortestpath=100
# on loading the reference lattices, transform them to n-best lists prior to alignment.
# uses fstshortestpath
</pre><p>Note that application of the substring acceptors is very efficient; and this alignment step should be much faster than the translation operation of Step 1. The alignment lattices (referred to as ALILATS) map rule sequences (derivations) to translation hypotheses. Weights remain in lexicographic semiring form. </p>
<pre class="fragment">&gt; zcat output/exp.mert/nbest/ALILATS/1.fst.gz | fstinfo | head -n 2
fst type                                          vector
arc type                                          tropical_LT_tropical
</pre><p>Individual rules are identified by their line number in the translation grammar file. A rule map can be created as </p>
<pre class="fragment">&gt; zcat G/rules.shallow.all.gz | awk 'BEGIN{print "0\t0"}{printf "%s-&gt;&lt;%s,%s&gt;\t%d\n", $1, $2, $3, NR}'  &gt; G/rules.shallow.all.map
</pre><p>The ALILATS transducers are not determinised: they contain every possible derivation for each N-Best list entry. The following example prints some of the alternative derivations of the top-scoring hypothesis: </p>
<pre class="fragment">&gt; zcat output/exp.mert/LATS/1.fst.gz | fstshortestpath &gt; tmp/1.fst # properly, should remove arcweights
&gt; zcat output/exp.mert/nbest/ALILATS/1.fst.gz | fstcompose - tmp/1.fst | fstproject | printstrings.O2 --nbest=10 --semiring=lexstdarc -m G/rules.shallow.all.map 2&gt;/dev/null | head -n 2
S-&gt;&lt;1,1&gt; V-&gt;&lt;3526,50&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;28847,245&gt; X-&gt;&lt;10_1278_V,135_20_103_3_V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;3_64570,4_25_1145_48&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;1857,3_425_6&gt; X-&gt;&lt;V_7786,V_23899&gt; S-&gt;&lt;S_X,S_X&gt; X-&gt;&lt;2,&lt;/s&gt;&gt; S-&gt;&lt;S_X,S_X&gt;
S-&gt;&lt;1,1&gt; V-&gt;&lt;3526,50&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;28847,245&gt; X-&gt;&lt;10_1278_V,135_20_103_3_V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;3_64570,4_25_1145_48&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;1857,3_425_6&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;7786,23899&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; X-&gt;&lt;2,&lt;/s&gt;&gt; S-&gt;&lt;S_X,S_X&gt;
</pre><p>The order of the rules in these rule sequences correspond to HiFST's bottom-up (left-to-right) CYK grid structure. Rule IDs are added as input symbols to the component WFSTs in the RTN following the translation rule (with its non-terminals). This leads to the bottom-up ordering after Replacement.</p>
<h2><a class="anchor" id="mert_alilats"></a>
3. Hypotheses with Unweighted Feature Vectors</h2>
<ul>
<li>Input:<ul>
<li><code>G/rules.shallow.vecfea.all.gz</code> &ndash; translation grammar, rules with (unweighted) feature vectors</li>
<li><code>output/exp.mert/nbest/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations, for n-best entries (from <a class="el" href="md_Tutorial.html#mert_nblist_derivations">2. Guided Translation / Forced Alignment</a>)</li>
<li>language model and translation grammar feature weights (see <code>configs/CF.mert.vecfea.nbest</code>)</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.mert/lats/VECFEA/?.nbest.gz</code> &ndash; N-best hypotheses</li>
<li><code>output/exp.mert/lats/VECFEA/?.vecfea.gz</code> &ndash; N-best unweighted features</li>
</ul>
</li>
</ul>
<p>The alilats2splats tool transforms ALILATS alignment lattices (transducers) to sparse vector weight lattices; see Section 2.3.1, [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>] for a detailed explanation. </p>
<pre class="fragment">&gt; alilats2splats.O2 --config=configs/CF.mert.vecfea.nbest --range=1:$M &amp;&gt; log/log.mert.nbest
</pre><p>The output is written to two sets of files:</p>
<p>N-best lists: </p>
<pre class="fragment"> &gt; head -n 2 output/exp.mert/nbest/VECFEA/1.nbest
 &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;     43.0904
 &lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 43.1757
</pre><p>Vecfea files: </p>
<pre class="fragment"> &gt; head -n 2 output/exp.mert/nbest/VECFEA/1.vecfea
 62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293
 63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482
</pre><ul>
<li>Line <code>n</code> in the nbest list is the `n-th' translation hypotheses, as ranked under the combined translation and language model scores.</li>
<li>Line <code>n</code> in the vecfea file is a vector obtained by summing the unweighted feature vectors of each rule in the best derivation of the <code>n-th</code> hypothesis</li>
</ul>
<p>The alilats2splats tool works as follows:</p>
<ul>
<li>The translation grammar with (unweighted) feature vectors is loaded</li>
<li>a Rule Flower acceptor, R, is created. This is an acceptor for rule sequences that applies vector weights (specifically, the feature vector for each rule). See Fig. 9 of [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>] for an example and an explanation.</li>
<li>For each source sentence, the ALILATS derivation-to-translation transducer from Step 2 is loaded, and its weights are removed. Call this T_u</li>
<li>The unweighted derivations-to-translation trandsducer T_u is composed with the Rule Flower acceptor R under the tropical sparse tuple weight semiring with the same feature vectors as are used to generate the translation.</li>
<li>The feature vector for the best scoring derivation for every translation is found as Determinise(Project_output(R o T_u) )</li>
<li>Language model scores M_1, ..., M_m are applied (again in the tropical sparse tuple weight semiring, so that each score ends up in a separate element in the vector) as Determinise(Project_output(R o T_u) ) o M_1 o ... o M_m</li>
</ul>
<p>Writing of N-Best lists and features is controlled by the <code>sparseweightvectorlattice</code> options <code>storenbestfile</code> and <code>storefeaturefile</code>: </p>
<pre class="fragment">[sparseweightvectorlattice]
loadalilats=output/exp.mert/nbest/ALILATS/?.fst.gz
storenbestfile=output/exp.mert/nbest/VECFEA/?.nbest
storefeaturefile=output/exp.mert/nbest/VECFEA/?.vecfea
wordmap=wmaps/wmt13.en.all.wmap
</pre><p>With the wordmap specified, the output of alilats2splats is in readable form in the target language. Note that the sentence boundary symbols and the combined translation and language model score appear in the nbest file. The N-best lists have the format</p>
<ul>
<li>wordindex1 wordindex2 ... translation_score</li>
</ul>
<p>The relationship of feature vectors and scores at the hypothesis level is as follows:</p>
<ul>
<li>Suppose there are m language models, with weights s_1 ...,s_m .<ul>
<li>These weights are specified by the HiFST parameters <code>lm.featureweights=s_1,s_2,..,s_m</code></li>
</ul>
</li>
<li>Suppose there are n dimensional feature vectors for each rule,<ul>
<li>The weights to be applied are specified by the HiFST parameters <code>grammar.featureweights=w_1,..,w_n</code></li>
</ul>
</li>
<li>A feature weight vector is formed as P = [s_1 ... s_m w_1 ... w_n]</li>
<li>A translation hypothesis e has a feature vector F(e) = [lm_1(e) ... lm_m(e) f_1(e) ... f_n(e)]<ul>
<li>lm_i(e): the i-th language model score for e</li>
<li>f_j(e): j-th grammar feature (see Section 3.2.1, [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>])</li>
</ul>
</li>
<li>The score of translation hypothesis e can be found as S(e) = F(e) . P (dot product)</li>
</ul>
<p>Each line k in the feature file has the format</p>
<ul>
<li>lm_1(e_k) ... lm_m(e_k) f_1(e_k) ... f_n(e_k)</li>
</ul>
<p>which are the unweighted feature values for the k-th hypothesis, e.g. </p>
<pre class="fragment"> &gt; head -n 2 output/exp.mert/nbest/VECFEA/1.vecfea
 62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293
 63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482
</pre><p>and the translation_score in line k is F(e_k) . P </p>
<pre class="fragment">&gt; head -n 2 output/exp.mert/nbest/VECFEA/1.nbest
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      43.0904
&lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;  43.1757</pre><h1><a class="anchor" id="lmert"></a>
Lattice MERT</h1>
<p>This section describes how to:</p>
<ul>
<li>generate lattices for use with LMERT [<a class="el" href="md_Tutorial.html#Macherey2008">Macherey2008</a>]</li>
<li>run the HiFST implementations of LMERT for iterative parameter estimation [<a class="el" href="md_Tutorial.html#Waite2012">Waite2012</a>]</li>
</ul>
<h2><a class="anchor" id="lmert_run"></a>
HiFST_lmert</h2>
<p>This HiFST release includes an implementation of LMERT [<a class="el" href="md_Tutorial.html#Waite2012">Waite2012</a>]. The script <code>HiFST_lmert</code> runs several iterations of lattice generation and parameter estimation using the <code>lmert</code> tool.</p>
<p>Prior to running the script, make sure to download and uncompress the LM <code>interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap</code> into the <code>M/</code> directory (see <a class="el" href="md_Tutorial.html#build">Installation</a>). </p>
<pre class="fragment">&gt; scripts/HiFST_lmert
</pre><p>This script runs 4 iterations of LMERT, with each iteration consisting of the four steps that follow in the sections below. In our experience, on a X86_64 Linux computer with 4 2.8GHz CPUs and 24GB RAM, each iteration takes ca. 3 hours.</p>
<p>Output from each iteration is written to files <code>log/log.lmert.[1,2,3,4]</code>. For example, </p>
<pre class="fragment">&gt; tail -8 log/log.lmert.bak/log.lmert.1 
Thu May 15 14:33:52 2014 optimization result:
Thu May 15 14:33:52 2014   start: 0.310724 0.996479
Thu May 15 14:33:52 2014   final: 0.316637
Thu May 15 14:33:52 2014 writing final parameters to file: output/exp.lmert/params.1
Thu May 15 14:33:52 2014   1.000000,1.080643,0.516691,1.694245,0.096786,-0.436836,24.789537,-4.856992,-3.419495,0.488006,0.121445,-0.064946
==Params
Thu May 15 14:33:52 BST 2014
1.000000,1.080643,0.516691,1.694245,0.096786,-0.436836,24.789537,-4.856992,-3.419495,0.488006,0.121445,-0.064946

&gt; tail -8 log/log.lmert.bak/log.lmert.2 
Thu May 15 19:08:00 2014 optimization result:
Thu May 15 19:08:00 2014   start: 0.316392 0.968629
Thu May 15 19:08:00 2014   final: 0.322178
Thu May 15 19:08:00 2014 writing final parameters to file: output/exp.lmert/params.2
Thu May 15 19:08:00 2014   1.000000,1.471600,0.534356,2.443535,-1.373008,0.631498,27.626422,-5.952960,-2.638954,0.699155,0.182260,-0.018291
==Params
Thu May 15 19:08:00 BST 2014
1.000000,1.471600,0.534356,2.443535,-1.373008,0.631498,27.626422,-5.952960,-2.638954,0.699155,0.182260,-0.018291
</pre><p>This indicates:</p>
<ul>
<li>The initial set of parameters yields a tuning set BLEU score of 0.310724, with brevity penalty 0.996479</li>
<li>The first iteration of LMERT improves the tuning set BLEU score to 0.316637 over the tuning set lattices generated with the initial parameters</li>
<li>Retranslation with the parameters found at iteration 1 yields a tuning set BLEU score of 0.316392, with brevity penalty 0.968629</li>
<li>The second iteration of LMERT improves the tuning set BLEU score to 0.322178 over the tuning set lattices generated with the parameters from iteration 1</li>
</ul>
<p>Note that the n-gram language model is set by default to <code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap</code>. This is a quantized version of <code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code>. Slightly higher tuning set BLEU scores can be gotten with the unquantized LM, although memory use in tuning will be higher.</p>
<p>Note also that the script <code>HiFST_lmert</code> can be modified to perform LMERT over only the first (e.g.) 100 tuning set sentences. This can be done for debugging / demonstration, in that processing will be much faster, although the estimated parameters will not be as robust.</p>
<h2><a class="anchor" id="lmert_hyps"></a>
1. Hypotheses for LMERT</h2>
<p>Note that this step is also done in MERT (<a class="el" href="md_Tutorial.html#mert">MERT - Features Only</a>), although the settings here are slightly different.</p>
<p>The following command is run at iteration <code>$it</code> , and will generate lattices for <code>$M</code> files.</p>
<ul>
<li>Input:<ul>
<li><code>$it</code> &ndash; lmert iteration (1, 2, ...)</li>
<li><code>$M</code> &ndash; number of sentences to process</li>
<li><code>RU/RU.tune.idx</code> &ndash; tuning set source language sentences</li>
<li><code>G/rules.shallow.vecfea.all.gz</code> &ndash; translation grammar</li>
<li><code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code> &ndash; target language model</li>
<li>language model and translation grammar feature weights, provided via command line option</li>
</ul>
</li>
<li>Output<ul>
<li><code>output/exp.lmert/$it/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized</li>
<li><code>output/exp.lmert/$it/hyps</code> &ndash; translation hyps (discarded)</li>
</ul>
</li>
</ul>
<p>The feature weights are gathered into a single vector, and set via the <code>--featureweights</code> command line option</p>
<ul>
<li>The first parameter is the grammar scale factor (in the MERT demo, this is set via <code>lm.featureweights</code>)</li>
<li>The remaining parameters are weights for the grammar features (in the MERT demo, these are set via <code>grammar.featureweights</code>)</li>
</ul>
<p>The initial parameters are </p>
<pre class="fragment">FW=1.0,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
</pre><p>HiFST is run in translation mode as </p>
<pre class="fragment">&gt; hifst.O2 --config=configs/CF.lmert.hyps --range=1:$M --featureweights=$FW --target.store=output/exp.lmert/$it/hyps --hifst.lattice.store=output/exp.lmert/$it/LATS/?.fst.gz      
</pre><h2><a class="anchor" id="lmert_veclats"></a>
2. Guided Translation / Forced Alignment</h2>
<ul>
<li>Input:<ul>
<li><code>$it</code> &ndash; lmert iteration (1, 2, ...)</li>
<li><code>$M</code> &ndash; number of sentences to process</li>
<li><code>output/exp.lmert/$it/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized (from <a class="el" href="md_Tutorial.html#lmert_hyps">1. Hypotheses for LMERT</a>)</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.lmert/$it/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations (e.g. Fig. 7, [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>])</li>
<li><code>output/exp.lmert/$it/hyps</code> &ndash; translation hyps (discarded)</li>
</ul>
</li>
</ul>
<p>HiFST is run in alignment mode. Lattices (from <code>output/exp.lmert/$it/LATS/?.fst.gz</code>) read and transformed into substring acceptors used to constrain the space of alignments </p>
<pre class="fragment">&gt; hifst.O2 --config=configs/CF.lmert.alilats --range=1:$M --referencefilter.load=output/exp.lmert/$it/LATS/?.fst.gz --target.store=output/exp.lmert/$it/hyps --hifst.lattice.store=output/exp.lmert/$it/ALILATS/?.fst.gz 
</pre><p>Note the following parameters in the configuration file: </p>
<pre class="fragment">[referencefilter]
prunereferenceweight=4 
# pruning threshold to be applied to input lattice prior to alignment
prunereferenceshortestpath=10000
# extract n-best list from input lattice to use as hypotheses
</pre><p>Reference lattices are read from <code>output/exp.lmert/$it/LATS/?.fst.gz</code>. For each lattice:</p>
<ul>
<li>an N-Best list of depth 10000 is extracted using <code>fstshortestpath</code></li>
<li>the lattices are pruned with threshold <code>prunereferenceweight=4</code></li>
<li>the pruned lattice is unioned with the N-Best list</li>
<li>the resulting WFSA is transformed (after removing weights, minimization and determinization) into a substring acceptor to be used in alignment</li>
</ul>
<p>A simpler approach could be simply to prune the reference lattices. However in practice it can be difficult to find a global pruning threshold that always yields a reference lattice that is big enough, but not too big. Including the n-best list ensures that there will always be a rich set of candidate hypotheses.</p>
<h2><a class="anchor" id="lmert_alilats"></a>
3. WFSAs with Unweighted Feature Vectors</h2>
<ul>
<li>Input:<ul>
<li><code>$it</code> &ndash; lmert iteration (1, 2, ...)</li>
<li><code>$M</code> &ndash; number of sentences to process</li>
<li>language model and translation grammar feature weights, provided via command line options</li>
<li><code>output/exp.lmert/$it/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations</li>
</ul>
</li>
<li>Output:<ul>
<li>output/exp.lmert/$it/VECFEA/?.fst.gz &ndash; translation lattices (WFSAs) with unweighted feature vectors</li>
</ul>
</li>
</ul>
<p><code>alilats2splats</code> transforms ALILATS alignment lattices to sparse vector weight lattices; see Section 2.3.1, [<a class="el" href="md_Tutorial.html#deGispert2010">deGispert2010</a>] for a detailed explanation. A single output WFSA with sparse vector weights is written for each translation. Note that this is different from the MERT case, where two N-best lists of hypotheses and features are written for each translation.</p>
<p>The HiFST <code>alilats2splats</code> command is </p>
<pre class="fragment">&gt; alilats2splats.O2 --config=configs/CF.lmert.vecfea --range=1:$M --featureweights=$FW --sparseweightvectorlattice.loadalilats=output/exp.lmert/$it/ALILATS/?.fst.gz --sparseweightvectorlattice.store=output/exp.lmert/$it/VECFEA/?.fst.gz 
</pre><h2><a class="anchor" id="lmert_lmert"></a>
4. LMERT</h2>
<ul>
<li>Input:<ul>
<li><code>$it</code> &ndash; lmert iteration (1, 2, ...)</li>
<li><code>$M</code> &ndash; number of sentences to process</li>
<li><code>output/exp.lmert/$it/VECFEA/?.fst.gz</code> &ndash; translation lattices (WFSAs) with unweighted feature vectors (from <a class="el" href="md_Tutorial.html#lmert_alilats">3. WFSAs with Unweighted Feature Vectors</a>)</li>
<li><code>EN/EN.tune.idx</code> &ndash; target language references in integer format</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.lmert/params.$it</code> &ndash; reestimated feature vector under LMERT with BLEU</li>
</ul>
</li>
</ul>
<p><code>latmert</code> runs as follows </p>
<pre class="fragment">&gt; latmert.O2 --search=random --random_axes --random_directions=28 --direction=axes --threads=24 --cache_lattices --error_function=bleu --algorithm=lmert --idxlimits=1:$M --print_precision=6 --lats=output/exp.lmert/$it/VECFEA/%idx%.fst.gz --lambda=$FW --write_parameters=output/exp.lmert/params.$it  EN/EN.tune.idx 
</pre><h2><a class="anchor" id="lmert_veclats_tst"></a>
Notes on Tropical Sparse Tuple Vector Weights</h2>
<p>The output lattices in <code>output/exp.lmert/$it/lats/VECFEA</code> are <code>tropicalsparsetuple</code> vector weight lattices. </p>
<pre class="fragment">&gt; zcat output/exp.mert/1/lats/VECFEA/1.fst.gz | fstinfo | head -n 2
fst type                                          vector
arc type                                          tropicalsparsetuple
</pre><p>The scores in these lattices are unweighted by the feature vector weights, i.e. they are the raw feature scores against which L/MERT finds the optimal parameter vector values. Distances under these unweighted vectors do not agree with the initial translation hypotheses, e.g. the shortest-path does not agree with the best translation: </p>
<pre class="fragment">&gt; unset TUPLEARC_WEIGHT_VECTOR
&gt; zcat output/exp.mert/1/lats/VECFEA/1.fst.gz | fstshortestpath | fsttopsort | fstpush --to_final --push_weights | fstprint -isymbols=wmaps/wmt13.en.all.wmap 
Warning: cannot find parameter vector. Defaulting to flat parameters
Warning: cannot find parameter vector. Defaulting to flat parameters
0       1       &lt;s&gt;     1
1       2       parliament      50
2       3       not     20
3       4       supports        1463
4       5       amendment       245
5       6       ,       4
6       7       gives   1145
7       8       freedom 425
8       9       tymoshenko      23899
9       10      &lt;/s&gt;    2
10      0,10,1,35.6919899,2,6.59277344,3,14.2285156,4,-10,5,-10,6,-6,8,-1,10,-9,11,8.2109375,12,13.7412109,
</pre><p>The sparse vector weight format is </p>
<pre class="fragment">0,N,idx_1,fea_1,...,idx_N,fea_N
</pre><p>where N is the number of non-zero elements in that weight vector.</p>
<p>To compute semiring costs correctly, the <code>TUPLEARC_WEIGHT_VECTOR</code> environment variable should be set to contain the correct feature vector weight; this should be the same feature vector weight applied in translation in steps 1 and 2: </p>
<pre class="fragment">TUPLEARC_WEIGHT_VECTOR=[s_1 ... s_m w_1 ... w_n]
</pre><p>which in this particular example is </p>
<pre class="fragment">&gt; export TUPLEARC_WEIGHT_VECTOR="1,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136"
</pre><p>The shortest path found through the vector lattice is then the same hypothesis produced under the initial parameter settings: </p>
<pre class="fragment">&gt; zcat output/exp.mert/1/lats/VECFEA/1.fst.gz | fstshortestpath | fsttopsort | fstpush --to_final --push_weights | fstprint -isymbols=wmaps/wmt13.en.all.wmap 
0       1       &lt;s&gt;     1
1       2       parliament      50
2       3       supports        1463
3       4       amendment       245
4       5       giving  803
5       6       freedom 425
6       7       tymoshenko      23899
7       8       &lt;/s&gt;    2
8       0,10,1,20.7773838,2,7.80957031,3,17.8671875,4,-8,5,-8,6,-5,9,-1,10,-7,11,20.0175781,12,18.2978516,
</pre><p>Note that printstrings can be used to extract n-best lists from the vector lattices, if the TUPLEARC_WEIGHT_VECTOR is correctly set: </p>
<pre class="fragment">&gt; zcat output/exp.mert/1/lats/VECFEA/1.fst.gz | printstrings.O2 --semiring=tuplearc --nbest=10 --unique -w -m wmaps/wmt13.en.all.wmap --tuplearc.weights=$TUPLEARC_WEIGHT_VECTOR 2&gt;/dev/null
&lt;s&gt; parliament supports amendment giving freedom tymoshenko &lt;/s&gt;        20.7778,7.80957,17.8672,-8,-8,-5,0,0,-1,-7,20.0176,18.2979
&lt;s&gt; parliament supports amendment gives freedom tymoshenko &lt;/s&gt;         20.7773,8.48828,20.9248,-8,-8,-4,0,-1,0,-7,14.1162,14.1016
&lt;s&gt; parliament supports amendment giving freedom timoshenko &lt;/s&gt;        20.7778,9.70703,17.7393,-8,-8,-5,0,0,-1,-7,22.166,18.2529
&lt;s&gt; parliament supports correction giving freedom tymoshenko &lt;/s&gt;       20.7768,9.15527,18.6689,-8,-8,-4,0,0,-1,-7,22.4062,20.7334
&lt;s&gt; parliament supports amendment giving liberty tymoshenko &lt;/s&gt;        20.7768,10.2051,18.3838,-8,-8,-5,0,0,-1,-7,22.6582,19.4707
&lt;s&gt; parliament supports amendment gives freedom timoshenko &lt;/s&gt;         20.7773,10.1602,21.0596,-8,-8,-4,0,-1,0,-7,16.2646,14.0566
&lt;s&gt; parliament supports amendment enables freedom tymoshenko &lt;/s&gt;       20.7768,8.48828,20.0742,-8,-8,-4,0,-1,0,-7,50.2627,17.0137
&lt;s&gt; parliament supports amendment enable freedom tymoshenko &lt;/s&gt;        20.7768,8.48828,20.6904,-8,-8,-4,0,-1,0,-7,50.2627,15.3457
&lt;s&gt; parliament not supports amendment giving freedom tymoshenko &lt;/s&gt;    29.3873,5.82324,12.8096,-9,-9,-6,0,0,-1,-8,16.1914,17.9443
&lt;s&gt; parliament supports amendment providing freedom tymoshenko &lt;/s&gt;     20.7769,8.48828,21.1689,-8,-8,-4,0,-1,0,-7,50.2627,17.3027
</pre><p>These should agree with n-best lists generated directly by alilats2splats</p>
<ul>
<li>N.B. There can be significant numerical differences between computations under the tropical lexicographic semiring vs the tuplearc semiring: printstrings and alilats2splats might not give exactly the same results. In such cases, the alilats2splats result is probably the better choice.</li>
</ul>
<h1><a class="anchor" id="rescoring"></a>
Rescoring Procedures</h1>
<h2><a class="anchor" id="rescoring_lm"></a>
Efficient Language Model Rescoring</h2>
<p>As discussed above, HiFST uses a lexicographic semiring (<a class="el" href="md_Tutorial.html#basic_scores">Scores, Costs, and Semirings</a>, [<a class="el" href="md_Tutorial.html#Roark2011">Roark2011</a>]) of two tropical weights. In each arc of a lattice generated by HiFST, the first weight (G+M) contains the correct score (translation grammar score + language model score). The second weight G only contains the translation grammar score. An example is repeated here: </p>
<pre class="fragment"> &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstprint | head -n 10
 0 1    1                            1             -2.609375,-2.609375
 1 36   999999999                    999999999      29.5185547,29.5185547
 1 35   12198                        12198          19.6816635,7.27929688
 1 34   12198                        12198          17.9902573,5.58789062
 1 33   9227                         9227           16.845089,2.56347656
 1 32   9121                         9121           9.55193996,-1.04199219
 1 31   9121                         9121           7.86053371,-2.73339844
 1 30   9121                         9121           9.33318996,-1.26074219
 1 29   8559                         8559           14.9520674,4.25878906
 1 28   4608                         4608           17.6058693,4.88671875
</pre><p>The advantage of using the lexicographic semiring to represent (G+M,G) weights is that the language model score can be substracted very efficiently. HiFST binaries do this mapping internally with an <a href="http://www.openfst.org/twiki/bin/view/FST/ArcMapDoc">fstmap</a> operation. The result is a WFSA whose weights contain only the translation grammar scores (G,G). The lexmap tool can be used to do this mapping, as follows, yielding lexicographic weights (G,G): </p>
<pre class="fragment">  &gt; zcat output/exp.baseline.outputnoprune/LATS/1.fst.gz | lexmap.O2  | fstprint | head -n 10
  0  1 1    1                                               -2.609375,-2.609375
  1  36     999999999                                       999999999       29.5185547,29.5185547
  1  35     12198                                           12198           7.27929688,7.27929688
  1  34     12198                                           12198           5.58789062,5.58789062
  1  33     9227                                            9227            2.56347656,2.56347656
  1  32     9121                                            9121            -1.04199219,-1.04199219
  1  31     9121                                            9121            -2.73339844,-2.73339844
  1  30     9121                                            9121            -1.26074219,-1.26074219
  1  29     8559                                            8559            4.25878906,4.25878906
  1  28     4608                                            4608            4.88671875,4.88671875
</pre><p>Using this facility to remove language model scores, the HiFST applylm tool can be used to rescore lattices under a different language model than was used in first-pass translation. Operations are as follows:</p>
<ol type="1">
<li>A lattice with lexicographic (G+M,G) weights is loaded</li>
<li>Weights are converted to (G,G) via fstmap</li>
<li>The new language model(s) are applied via composition under the lexicographic semiring, with optional scale factors and word insertion penalties. The new WFSAs weights are of the form (G+M',G), where M' are the new language model weights.</li>
<li>The reweighted WFSA is written to disk, with either lexicographic or standard tropical weights</li>
</ol>
<p>The following example uses applylm to rescore lattices generated with almost no pruning (<code>output/exp.baseline.outputnoprune/LATS</code>). Rescoring uses the same 4-gram language model originally used to generate the lattice, but with a different scale factor (<code>lm.scale=0.9</code>). </p>
<pre class="fragment"> &gt; applylm.O2 --config=configs/CF.baseline.outputnoprune.lmrescore  &amp;&gt; log/log.lmrescore
</pre><p>For the first sentence, the original 1-best hypothesis was: </p>
<pre class="fragment">&gt; zcat output/exp.baseline.outputnoprune/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
&lt;s&gt; republican strategy of resistance to the renewal of obamas election &lt;/s&gt;        57.4707,-8.03809
</pre><p>Rescoring yields a slightly different 1-best: </p>
<pre class="fragment">&gt; zcat output/exp.baseline.lmrescore/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
&lt;s&gt; the republican strategy of resistance to the renewal of obamas election &lt;/s&gt;    50.9163,-8.66992
</pre><p>Note the "load.deletelmcost" option in the configuration file, which instructs the tool to subtract old lm scores first.</p>
<p>If the scaling is not changed, both lattices should be identical (<code>applylm.O2 --config=configs/CF.baseline.outputnoprune.lmrescore --lm.featureweights=1</code>).</p>
<h2><a class="anchor" id="lmbr"></a>
Lattice Minimum Bayes Risk Decoding</h2>
<p>For a detailed discussion of LMBR, see Chapters 7 and 8 in [<a class="el" href="md_Tutorial.html#BlackwoodPhD">BlackwoodPhD</a>].</p>
<p>LMBR is a decoding procedure, based on the following:</p>
<ul>
<li>Evidence space: a lattice (WFSA) containing weighted translations produced by the SMT system.<ul>
<li>N-gram posterior distributions, with pathwise posteriors, are extracted from this WFSA.</li>
</ul>
</li>
<li>The hypotheses space: an unweighted lattice (FSA) containing hypotheses to be rescored.</li>
</ul>
<p>The following steps are carried out in LMBR decoding:</p>
<ol type="1">
<li>The evidence space is normalised after applying a grammar scale factor (<code>--alpha=</code>). Scaling is done by the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://www.openfst.org/twiki/bin/view/FST/PushDoc">Push</a> towards final states, and setting the final state probability to 1.0.</li>
<li>N-grams are extracted from the hypothesis space.</li>
<li>N-gram path-posterior probabilities are computed over the evidence space using a modified Forward procedure (see <a class="el" href="md_Tutorial.html#Blackwood2010">Blackwood2010</a>)</li>
<li>Cyclic WFSAs are built to represent posterior probability distribution of each n-gram order and compose with the original hypotheses space. A word insertion penalty (<code>--wps=</code>) is also included in the costs of the cyclic WFSAs.</li>
<li>Risk is computed through a sequence of compositions.</li>
<li>The result for LMBR decoding is a WFSA; each weighted path represents a hypothesis and its risk.</li>
</ol>
<p>The weighted hypothesis space can be save as a WFSA, or the minimum risk hypothesis can be generated via the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://www.openfst.org/twiki/bin/view/FST/ShortestPathDoc">ShortestPath</a> operation.</p>
<p>The following example applies LMBR decoding to the baseline lattices </p>
<pre class="fragment">&gt; lmbr.O2 --config=configs/CF.baseline.lmbr &amp;&gt; log/log.baseline.lmbr
</pre><p>The LMBR output hyppthesis file keeps the scale factor, word penalty, and sentence id at the start of the file; the hypothesis follows the colon </p>
<pre class="fragment">&gt; cat output/exp.baseline.lmbr/HYPS/0.40_0.02.hyp
0.4 0.02 1:1 3 9121 384 6 2756 7 3 4144 6 159312 42 1341 2
0.4 0.02 2:1 3 1119 6 3 9121 1711 54 79 6 3 85 7 525 3 13907 17 3 628 5 2
</pre><p>LMBR can be optimised by tuning the grammar scale factor and word insertion penalty. Once lattices are loaded into memory and n-grams are extracted (steps 1 - 5), rescoring is fast enough that it is practical and efficient to perform a grid search over a range of parameter values (see config file).</p>
<p>Hypotheses are written to different files, with names based on parameter values (e.g. as <code>--writeonebest=output/exp.baseline.lmbr/HYPS/%alpha%_%wps%%.hyp</code> ). The best set of values can be selected based on BLEU score, after mapping each integer mapped output back to words, detokenizing, and scoring on against references.</p>
<p>LMBR relies on a unigram precision (p) and precision ratio (r) that are computed over a development set, e.g. with verbose logs of a BLEU scorer such as NIST mteval1. The script <code>$HiFSTROOT/scripts/lmbr/compute-testset-precisions.pl</code> is included for this purpose.</p>
<p>If the option <code>--preprune=</code> is specified, the evidence space is pruned prior to computing posterior probabilities (i.e. pruning is done at threshold 7 in this example). If this option is not defined, the full evidence space will be passed through.</p>
<h1><a class="anchor" id="pda"></a>
Push-Down Automata</h1>
<p>This HiFST package can also perform decoding using Push-Down Automata (PDA) as described in [<a class="el" href="md_Tutorial.html#Iglesias2011">Iglesias2011</a>, <a class="el" href="md_Tutorial.html#Allauzen2014">Allauzen2014</a>]. We call this the HiPDT decoder. A brief overview of it and an example on how to use it is given next.</p>
<p>In this framework, the RTN at the top-most cell of the CYK grid is converted into a PDA (via the Replace operation in the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> Extensions) and efficiently composed with a language model to produce another PDA. To obtain the final output FSA, the PDA is expanded to an FSA either entirely (if memory is sufficient and the language model is small enough) or via pruned expansion (for larger language models). This is useful in exploring large and complex translation grammars where HiFST requires a lot of local pruning. However, it requires the language model to be 'small' (for example, entropy-pruned as in [<a class="el" href="md_Tutorial.html#Iglesias2011">Iglesias2011</a>, <a class="el" href="md_Tutorial.html#Allauzen2014">Allauzen2014</a>]). Therefore, we then typically rescore the pruned output FSA with a stronger language model.</p>
<p>This whole setup can be accomplished in one single command as follows: </p>
<pre class="fragment">&gt; hifst.O2 --config=configs/CF.hiero.pdt &amp;&gt; log/log.hiero.pdt
</pre><p>Please see the config file for the parameters needed, along with explanatory comments.</p>
<p>In this particular example, the output 1-best for the first two sentences is identical to the baseline Hiero case (with translation via RTN replacement followed by composition). However, the output lattices differ as they contain different hypotheses (due to the different pruning strategy).</p>
<h2><a class="anchor" id="pda_rtns"></a>
Recursive Transition Networks</h2>
<p>As already discussed with respect to <a class="el" href="md_Tutorial.html#lpruning">Local pruning / pruning in search</a>, HiFST generates an initial representation of the space of translation in the form of an RTN, which is then transformed either to WFSAs or to PDAs using the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation prior to application of the language model.</p>
<p>HiFST can save the RTNs to disk, and the language model application and shortest path operations can be carried out using the <a class="el" href="md_Tutorial.html#OpenFst">OpenFst</a> command line tools.</p>
<p>For example, consider generation of the baseline lattices with the Shallow-1 translation grammar and the 4-gram language model. The command can be re-run, but with added instructions to save the RTNs to disk: </p>
<pre class="fragment">&gt; hifst.O2 --config=configs/CF.baseline --hifst.writertn=output/exp.baseline/rtn/?/%%rtn_label%%.fst --grammar.storentorder=output/exp.baseline/rtn/ntmap  --hifst.rtnopt=yes &amp;&gt; log/log.baseline.rtn
</pre><p>The RTNs are written to the directory <code>output/exp.baseline/rtn/*</code> as </p>
<pre class="fragment">&gt; ls output/exp.baseline/rtn/1/
1001000007.fst  1003001002.fst  1003003000.fst  1003004001.fst  1003006000.fst  1004002000.fst  1004005000.fst
1003001000.fst  1003002000.fst  1003003001.fst  1003005000.fst  1003007000.fst  1004003000.fst  1004006000.fst
1003001001.fst  1003002001.fst  1003004000.fst  1003005001.fst  1004001000.fst  1004004000.fst
</pre><p>and the non-terminal mapping is written to output/exp.baseline/rtn/ntmap : </p>
<pre class="fragment">&gt; cat output/exp.baseline/rtn/ntmap
S   1
D   2
X   3
V   4
</pre><p>Each RTN name is of the form 1ABC.fst , where A, B, and C are 3-digit strings.</p>
<ul>
<li>A is the numerical code for a non-terminal in the grammar; in this case, 001 corresponds to S.</li>
<li>B indicates a position in the source sentence: 0 &lt;= B &lt; I, where I is the source sentence length</li>
<li>C indicates the offset to the end of a span, 0 &lt;= C and B+C &lt; I</li>
<li>The automata 1ABC.fst corresponds to T_(A,B,B+C) in the formulation of <a class="el" href="md_Tutorial.html#lpruning">Local pruning / pruning in search</a></li>
</ul>
<p>In this example, the automata 1003001002.fst contains all derivations headed by X, the third non-terminal, and spanning source positions 1 to 3 (=1+2).</p>
<p>The root automata is 1001000007.fst, since A=001 (the S non-terminal), B=0 (the first source position), and C=7 (the sentence has 8 words). This automata is a representation of all possible translations of the source sentence under this grammar, as can be seen by printing its paths (here the first 2): </p>
<pre class="fragment">&gt; cat output/exp.baseline/rtn/1/1001000007.fst | printstrings.O2 --semiring=lexstdarc -u --nbest=2 2&gt;/dev/null
1 1004001000 11 384 1004003000 1004004000 1004005000 1004006000 2 
1 1004001000 11 384 1003003001 1004005000 1004006000 2 
</pre><p>As can be seen, the symbols are a mix of target language symbols (1,2,11,384,...) and pointers to other automata (1004001000, 1004003000, ...). Conversion of the RTN is done by recursive substitution of these symbols by the FSTs to which they point, starting from the root automata.</p>
<h2><a class="anchor" id="pda_replace"></a>
Replacement: Translation by Converting RTNs to WFSAs</h2>
<pre class="fragment"> &gt; fstreplace | head -n 3
 Recursively replaces FST arcs with other FST(s).

 Usage: fstreplace root.fst rootlabel [rule1.fst label1 ...] [out.fst]
</pre><p>Note that the root FST and root label are always of the form 1001000C, where C = I - 1 , where I is the source sentence length.</p>
<p>HiFST uses the same names for rules and labels, so we get get the filenames and labels in the right form by, e.g. </p>
<pre class="fragment"> &gt; ls output/exp.baseline/rtn/1/1*.fst | sed 's,\(.*\)/\(.*\).fst,\1/\2.fst \2,' | head -n 5
 output/exp.baseline/rtn/1/1001000007.fst 1001000007
 output/exp.baseline/rtn/1/1003001000.fst 1003001000
 output/exp.baseline/rtn/1/1003001001.fst 1003001001
 output/exp.baseline/rtn/1/1003001002.fst 1003001002
 output/exp.baseline/rtn/1/1003002000.fst 1003002000
</pre><p>The following will expand the RTN into an FSA: </p>
<pre class="fragment"> &gt; fstreplace `ls output/exp.baseline/rtn/1/1*.fst | sed 's,\(.*\)/\(.*\).fst,\1/\2.fst \2,'` &gt; output/exp.baseline/rtn/1/T.fst
</pre><p>The WFSA T is the replacement of the RTN that was generated in translation.</p>
<h3><a class="anchor" id="rtn_lm_app"></a>
Composition and Shortest Path</h3>
<p>The applylm tool can be used to apply the baseline 4-gram language model to T via composition. This generates a new WFSA containing both translation and language model scores:</p>
<ul>
<li>Input<ul>
<li>M/lm.4g.mmap : n-gram LM</li>
<li>output/exp.baseline/rtn/1/T.fst : WFSA containing translation scores</li>
</ul>
</li>
<li>Output<ul>
<li>output/exp.baseline/rtn/1/TG.fst.gz : WFSA containing translation grammar and LM scores</li>
</ul>
</li>
</ul>
<p>The output is written in the form of a transducer, with the RTN labels as the input symbols and the target language words on the output symbols: </p>
<pre class="fragment"> &gt; applylm.O2 --lm.load=M/lm.4g.mmap --semiring=lexstdarc --lattice.load=output/exp.baseline/rtn/1/T.fst --lattice.store=output/exp.baseline/rtn/1/TG.fst.gz

 &gt; zcat output/exp.baseline/rtn/1/TG.fst.gz | fstshortestpath | fstrmepsilon | fsttopsort | fstprint
 0    1     1               1       -2.609375,-2.609375
 1    2     9121            9121    9.33318996,-1.26074219
 2    3     1004002000      0
 3    4     384             384     7.13530731,-2.609375
 4    5     1004003000      0       1.28222656,1.28222656
 5    6     6               6       -0.390115976,-3.328125
 6    7     2756            2756    9.45967484,0.288085938
 7    8     7               7       1.79730964,0
 8    9     1004004000      0
 9    10    3               3       -0.395056069,-1.23925781
 10   11    4144            4144    9.78138161,0
 11   12    6               6       0.201819927,0
 12   13    1003005001      0       3.29199219,3.29199219
 13   14    1458528         1458528 10.2062063,-1.0703125
 14   15    1004005000      0
 15   16    1341            1341    6.04568958,1.55957031
 16   17    2               2       2.33047056,-2.34277344
 17
</pre><p>To see the translation alone, we project to the output symbols: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/rtn/1/TG.fst.gz | fstproject --project_output | printstrings.O2 --semiring=lexstdarc -w -m wmaps/wmt13.en.wmap 2&gt;/dev/null
&lt;s&gt; republican strategy of resistance to the renewal of obamas election &lt;/s&gt;                        57.4707,-8.03809
</pre><p>which should agree with the previously generated contents of output/exp.baseline/LATS/1.fst.gz produced by the baseline system: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc -w -m wmaps/wmt13.en.wmap 2&gt;/dev/null
&lt;s&gt; republican strategy of resistance to the renewal of obamas election &lt;/s&gt;             57.4707,-8.03809
</pre><h2><a class="anchor" id="pda_expand"></a>
Expansion: Translation by Composition of PDAs and WFSAs followed by Pruned Expansion</h2>
<p>When using PDTs, the decoding process differs from the above in that the RTN is not expanded to an FSA prior to composition with the LM. Instead, HiPDT replaces the RTN by a PDA, which is efficiently composed with the LM to produce another PDA. Finally, this resulting PDA is converted to the final translation FSA via pruned expansion.</p>
<p>As explained in the example of <a class="el" href="md_Tutorial.html#pda">Push-Down Automata</a>, this process is done by the decoder in one go. For explanatory reasons, here we reproduce it externally for one sentence via the command line.</p>
<p>(Note: as of March 2014 we have not managed to make the OpenFST PDT command line operations work well with our lexicographic library, so the standard tropical semirring must be used in this example. This makes it slightly more complex than it should be, but we believe it is useful to understand HiPDT anyway)</p>
<p>First, we dump the RTN for the full hiero grammar as follows: </p>
<pre class="fragment">&gt; hifst.O2 --config=configs/CF.hiero --hifst.writertn=output/exp.hiero/rtn/?/%%rtn_label%%.fst --grammar.storentorder=output/exp.hiero/rtn/ntmap --hifst.rtnopt=yes &amp;&gt; log/log.hiero.rtn
</pre><p>Then, we ensure that the RTN files are in the tropical semiring: </p>
<pre class="fragment">&gt; mkdir -p output/exp.hiero/rtn-tp/1 output/exp.hiero/rtn-tp/2
&gt; pushd output/exp.hiero/rtn/ ; for f in ?/100*.fst; do cat $f | lexmap.O2 --action=lex2std &gt; ../rtn-tp/$f; done; popd
</pre><p>The PDT is then created as follows: </p>
<pre class="fragment">&gt; for f in 1 2; do pdtreplace --pdt_parentheses=output/exp.hiero/rtn-tp/$f/parens.txt `ls output/exp.hiero/rtn-tp/$f/1*.fst | sed 's,\(.*\)/\(.*\).fst,\1/\2.fst \2,'` &gt; output/exp.hiero/rtn-tp/$f/T.pdt; done
</pre><p>where the `pdt_parentheses' option indicates that the open/close parentheses symbols are to be stored into a file. This is used later in order to expand the PDA to an FSA.</p>
<p>Then the PDA is composed with the weak language model. This is done via the standard composition algorithm, but making sure that open/close parentheses symbols are treated as epsilons by the composition algorithm. To accomplish this, their respective output symbols need to be relabel to 0 before applying the LM: </p>
<pre class="fragment">&gt; for f in 1 2; do cat output/exp.hiero/rtn-tp/1/parens.txt | tr '\t' '\n' | sed 's/$/\t0/' &gt; output/exp.hiero/rtn-tp/$f/parens-to-epsilon.txt ; 
&gt; cat output/exp.hiero/rtn-tp/$f/T.pdt | fstrelabel -relabel_opairs=output/exp.hiero/rtn-tp/$f/parens-to-epsilon.txt &gt; output/exp.hiero/rtn-tp/$f/Tb.pdt 
&gt; applylm.O2 --lm.load=M/lm.4g.eprnd.mmap --lattice.load=output/exp.hiero/rtn-tp/$f/Tb.pdt --lattice.store=output/exp.hiero/rtn-tp/$f/TG.pdt ; 
&gt; done
</pre><p>Then the resulting PDT is expanded into an FSA while applying a pruning weight of 9: </p>
<pre class="fragment">&gt; for f in 1 2; do fstproject output/exp.hiero/rtn-tp/$f/TG.pdt | pdtexpand --pdt_parentheses=output/exp.hiero/rtn-tp/$f/parens.txt --weight=9 &gt; output/exp.hiero/rtn-tp/$f/TG.fst ; done
</pre><p>Finally, the weak LM is removed and the full LM is applied: </p>
<pre class="fragment">&gt; for f in 1 2; do applylm.O2 --lm.load=M/lm.4g.eprnd.mmap --lm.featureweights=-1 --lattice.load=output/exp.hiero/rtn-tp/$f/TG.fst --lattice.store=output/exp.hiero/rtn-tp/$f/TG-nolm.fst ; applylm.O2 --lm.load=M/lm.4g.mmap --lattice.load=output/exp.hiero/rtn-tp/$f/TG-nolm.fst --lattice.store=output/exp.hiero/rtn-tp/$f/TG-final.fst ; done
</pre><p>The final FSA that results from this process (<code>output/exp.hiero/rtn-tp/1/TG-final.fst</code>) should be equivalent to the one obtained by HiPDT (<code>output/exp.hiero.pdt/LATS/1.fst.gz</code>) except for numerical differences. Their 1-best hypothesis can be obtained as follows: </p>
<pre class="fragment">&gt; zcat output/exp.hiero.pdt/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc -w -m wmaps/wmt13.en.wmap 2&gt;/dev/null
&lt;s&gt; the republican strategy of resistance to the renewal of obama 's election &lt;/s&gt;  55.2515,-11.6445

&gt; cat output/exp.hiero/rtn-tp/1/TG-final.fst | printstrings.O2 -w -m wmaps/wmt13.en.wmap 2&gt;/dev/null
    &lt;s&gt; the republican strategy of resistance to the renewal of obama 's election &lt;/s&gt;      55.2515</pre><h1><a class="anchor" id="true_casing"></a>
Fst-based True casing</h1>
<p>HiFST includes a tool typically used for true casing the output. It relies on two models:</p>
<ul>
<li>A true-case integer-mapped language model in ARPA or KenLM format.</li>
<li>A flower transducer that transduces uncased words to every true case alternative. This model is loaded from a file with the following format per line, one for each uncased word:<ul>
<li>uncased-case-word true-case-word1 prob1 true-case-word2 prob2 ...</li>
<li>This format is compatible with the unigram model for <a class="el" href="md_Tutorial.html#SRILM">SRILM</a> <a href="http://www.speech.sri.com/projects/srilm/manpages/disambig.1.html">disambig</a> tool (see <code>--map</code> option).</li>
</ul>
</li>
</ul>
<p>Words must be integer-mapped. A file with this model is available: </p>
<pre class="fragment">&gt; head  G/tc.unimap
1 1 1.0
2 2 1.0
3 5943350 0.00002 3 0.86370 5943349 0.13628
4 4 1.00000
5 5 1.00000
6 5942623 0.00452 5942624 0.00002 6 0.99546
7 5943397 0.00000 5943398 0.01875 7 0.98121 5943399 0.00004
8 5941239 0.00003 8 0.99494 5941238 0.00502
9 5942238 0.06269 9 0.93729 5942239 0.00002
10 5943348 0.00001 10 0.99498 5943347 0.00501
</pre><p>For example, under this model word 4 (comma ",") transduces to itself with probability 1. The uncased word 3 ("the") has three upper-case alternatives: "the", "THE", and "The", with the following probabilities </p>
<pre class="fragment"> P(the | the) = 0.86
 P(THE | the) = 0.00002
 P(The | the) = 0.13628
</pre><p>To generate these probabilities, you just need counts of truecased words. You can extract these unigrams with <a class="el" href="md_Tutorial.html#SRILM">SRILM</a> <a href="http://www.speech.sri.com/projects/srilm/manpages/ngram-count.1.html">ngram-count</a> tool, and calculate the probability of each particular true-cased form given the aggregated number of lower-cased instances.</p>
<p>These models are provided to the recaser module via the following configuration options </p>
<pre class="fragment">&gt; cat configs/CF.recaser
[recaser]
lm.load=M/lm.tc.gz
unimap.load=G/tc.unimap
</pre><p>The true casing procedure is very similar to that of <a class="el" href="md_Tutorial.html#SRILM">SRILM</a> <a href="http://www.speech.sri.com/projects/srilm/manpages/disambig.1.html">disambig</a> tool. In our case this is accomplished with two subsequent compositions, followed by exact pruning. An acceptable performance vs speed/memory trade-off can be achieved e.g. with offline entropy pruning of the language model.</p>
<p>A range of input lattices can be true-cased in the following way with our fst-based disambig tool: </p>
<pre class="fragment">&gt; disambig.O2 configs/CF.recaser --recaser.input=output/exp.hiero.pdt/LATS/?.fst.gz --recaser.output=output/exp.recasing/LATS/?.fst.gz --range=1:2 -s lexstdarc
</pre><p>The result can be printed as so: </p>
<pre class="fragment">&gt; printstrings.O2 --range=1:2 --input=output/exp.recasing/LATS/?.fst.gz --semiring=lexstdarc --label-map=wmaps/wmt13.en.wmap 2&gt;/dev/null
&lt;s&gt; The Republican strategy of resistance to the renewal of Obama 's election &lt;/s&gt;
&lt;s&gt; The leaders of the Republican justified their policies need to deal with the spin on the elections . &lt;/s&gt;
</pre><p>Note that both models need to be integer-mapped, hence the external target wordmap (&ndash;label-map) must also map true case words.</p>
<p>HiFST can include truecasing as subsequent step following decoding, prior to writing the output hypotheses. For instance: </p>
<pre class="fragment">&gt; hifst.O2 --config=configs/CF.hiero.pdt.recaser --target.store=output/exp.hiero.pdt/recased/hyps &amp;&gt; log/log.hiero.pdt.recase

&gt; farcompilestrings --entry_type=line output/exp.hiero.pdt/recased/hyps | farprintstrings --symbols=wmaps/wmt13.en.wmap
&lt;s&gt; The Republican strategy of resistance to the renewal of Obama 's election &lt;/s&gt;
&lt;s&gt; The leaders of the Republican justified their policies need to deal with the spin on the elections . &lt;/s&gt;
</pre><p>However, the output lattices are left in uncased form: </p>
<pre class="fragment">&gt; zcat output/exp.hiero.pdt/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc --label-map=wmaps/wmt13.en.wmap -w 2&gt;/dev/null
&lt;s&gt; the republican strategy of resistance to the renewal of obama 's election &lt;/s&gt;               55.2515,-11.6445</pre><h1><a class="anchor" id="Appendices"></a>
Appendices</h1>
<h2><a class="anchor" id="gext"></a>
Using Grammars From Open Source SMT Systems</h2>
<p>The current release of HiFST does not include our rule extraction system. For this tutorial, we demonstrate how to convert grammars obtained from other systems into the HiFST format. All files following the above format can be used in HiFST. Here we briefly introduce ways for adapting SCFGs generated by several existing SMT systems to the HiFST format.</p>
<h3><a class="anchor" id="gext_moses"></a>
Moses</h3>
<p><a href="http://www.statmt.org/moses/">http://www.statmt.org/moses/</a></p>
<p>The Moses package provides utilities for both phrase extraction and hierarchical rule extraction. Suppose that we have a pair of sentences and the word-level alignment between them: </p>
<pre class="fragment"> SOURCE    = 61 10717 3267 486 3 6501 3 53 1230 44 4 902 7631 2213 16 2265 319 5
 TARGET    = 71 29 5558 5494 831 3 1331 2242 7 1250 5 1003 13 439 8 5663 601 4 712 4 7 86 6
 ALIGNMENT = 0-0 0-1 1-2 2-3 8-3 1-4 10-5 11-6 12-7 13-7 14-8 16-9 15-11 16-13 1-15 3-16 4-17 5-18 6-19 7-21 17-22
</pre><p>We can obtain a phrase translation table using Moses, like this, </p>
<pre class="fragment">  ...                                                                                                          
  44 4 902 7631 2213 ||| 3 1331 2242 ||| 0.5 0.25 1 1 2.718 ||| ||| 2 1                                        
  44 4 902 ||| 3 1331 ||| 0.5 1 1 1 2.718 ||| ||| 2 1                                                          
  44 4 ||| 3 ||| 0.5 1 1 1 2.718 ||| ||| 2 1                                                                   
  ...                                                                                                          
</pre><p>It is trivial to transform the above phrase pairs into the HiFST rules: we only need to introduce a non-terminal symbol into the LHSs of the rules. Here we take X as the only one non-terminal symbol. Then we concatenate the sequence of words with an underscore <code>_</code> on both language sides, e.g., <code>44 4 902 7631 2213</code> =&gt; <code>44_4_902_7631_2213</code>. Then the weights can be used by performing a -log() computation to fit for the tropical semiring used in HiFST.</p>
<p>Note that HiFST prefers the use of target-word count as a feature in the SCFG to eliminate the bias towards short sentences caused by the use of n-gram language model. So here we add the negative target-word count into the weight vector as a new feature, e.g, for the first phrase pair, the first weight should be -3 (i.e., minus target-word number). By using the above methods, we get the rules in the HiFST format, as follows </p>
<pre class="fragment">  ...                                                                                                          
  X 44_4_902_7631_2213 3_1331_2242 -3 0.693 1.386 0 0 -1                                                       
  X 44_4_902 3_1331 -2 -0.693 0 0 0 -1                                                                         
  X 44_4 3 -1 -0.693 0 0 0 -1                                                                                  
  ...                                                                                                          
</pre><p>In addition to the phrasal translations, Moses provides a tool for hierarchical phrasal rule extraction. For the above example, the Moses hierarchical rule extractor can generate a rule file: </p>
<pre class="fragment">  ...                                                                                                          
  44 [X][X] 16 [X] ||| [X][X] 7 [X] ||| 0.084 0.5 1 1 2.718 ||| 1-0 ||| 1.074 0.090                            
  44 [X][X] 16 [X][X] [X] ||| [X][X] 7 [X][X] 8 [X] ||| 0.125 0.5 0.275 0.25 2.718 ||| 1-0 3-2 ||| 0.307 0.139 
  44 [X][X] 16 [X][X] [X] ||| [X][X] 7 [X][X] [X] ||| 0.128 0.5 0.724 1 2.718 ||| 1-0 3-2 ||| 0.786 0.139      
  ...                                                                                                          
</pre><p>On each language side of a rule, <code>[X]</code> represents the non-terminal symbol X of the left hand side (Note: both languages share the same LHS). <code>[X][X]</code> represents a non-terminal with a non-terminal symbol <code>X</code>. If more than one non-terminals are involved, we need the alignment information to determine the relative order of non-terminals. This can be found in the 4-th field of each rule. E.g., for the second rule in the above example, we can access the alignment information "1-0 3-2" and know that it is a monotonic translation rule. Hence all these hierarchical phrasal rules in Moses can be transformed into the rules used in HiFST, like so </p>
<pre class="fragment"> ...                                                                                                          
 X 44_X_16 X_7 -1 2.476 0.693 0 0 -1                                                                          
 X 44_X1_16_X2 X1_7_X2_8 -2 2.079 0.693 1.290 1.386 -1                                                        
 X 44_X1_16_X2 X1_7_X2 -1 2.055 0.693 0.322 0 -1                                                              
 ...                                                                                                          
</pre><p>As HiFST uses a single grammar file as input, we can concatenate the phrase translation file and the hierarchical rule file to form the final SCFG file.</p>
<h3><a class="anchor" id="gext_joshua"></a>
Joshua and cdec</h3>
<p><a href="http://joshua-decoder.org/">http://joshua-decoder.org/</a> and <a href="http://cdec-decoder.org/index.php?title=Main_Page">http://cdec-decoder.org/index.php?title=Main_Page</a></p>
<p>Joshua and cdec support a different definition of SCFG files, where each rule follows the format of </p>
<pre class="fragment">[LHS] ||| RHS_SOURCE ||| RHS_TARGET ||| FEATURES(WEIGHTS) 
</pre><p>For the above example, one can obtain the following rule file in the Joshua format. </p>
<pre class="fragment">...                                                                                                           
[X] ||| 44 4 902 7631 2213 ||| 3 1331 2242 ||| -0.693 -1.386 0 0 1                                           
[X] ||| 44 4 902 ||| 3 1331 ||| 0.693 0 0 0 1                                                                
[X] ||| 44 4 ||| 3 ||| 0.693 0 0 0 1                                                                         
...                                                                                                          
[X] ||| 44 [X,1] 16 ||| [X,1] 7 ||| -2.476 -0.693 0 0 1                                                      
[X] ||| 44 [X,1] 16 [X,2] ||| [X,1] 7 [X,2] 8 ||| -2.079 -0.693 -1.290 -1.386 1                              
[X] ||| 44 [X,1] 16 [X,2] ||| [X,1] 7 [X,2] ||| -2.055 -0.693 -0.322 0  1                                    
...                                                                                                          
</pre><p>Here <code>[X]</code> represents the non-terminal symbol on the left-hand side of a rule. <code>[X,n]</code> represents a non-terminal on the right hand side, where n is an index to determine the non-terminal order. All features(weights) are in log-scale. So we use minus operation to fit them into the weight definition preferred by HiFST. Also, the (minus) target-word number is added as an additional feature. For this Joshua version of the SCFG file, we can transform it into the HiFST-style file: </p>
<pre class="fragment">...                                                                                                          
X 44_4_902_7631_2213 3_1331_2242 -3 0.693 1.386 0 0 -1                                                       
X 44_4_902 3_1331 -2 -0.693 0 0 0 -1                                                                         
X 44_4 3 -1 -0.693 0 0 0 -1                                                                                  
...                                                                                                          
X 44_X_16 X_7 -1 2.476 0.693 0 0 -1                                                                          
X 44_X1_16_X2 X1_7_X2_8 -2 2.079 0.693 1.290 1.386 -1                                                        
X 44_X1_16_X2 X1_7_X2 -1 2.055 0.693 0.322 0 -1                                                              
...                                                                                                          
</pre><h3><a class="anchor" id="gext_niutrans"></a>
NiuTrans</h3>
<p><a href="http://www.nlplab.com/NiuPlan/NiuTrans.html">http://www.nlplab.com/NiuPlan/NiuTrans.html</a></p>
<p>NiuTrans uses another format for SCFG files, like </p>
<pre class="fragment">RHS_SOURCE ||| RHS_TARGT ||| LHS ||| FEATURE (WEIGHTS) 
</pre><p>Based on the NiuTrans format, the above sample grammar can be written, as follows: </p>
<pre class="fragment">...                                                                                                          
44 4 902 7631 2213 ||| 3 1331 2242 ||| X ||| -0.693 -1.386 0 0 1                                             
44 4 902 ||| 3 1331 ||| X ||| 0.693 0 0 0 1                                                                  
44 4 ||| 3 ||| X ||| 0.693 0 0 0 1                                                                           
...                                                                                                          
44 #X 16 ||| #1 7 ||| X ||| -2.476 -0.693 0 0 1                                                              
44 #X 16 #X ||| #1 7 #2 8 ||| X ||| -2.079 -0.693 -1.290 -1.386 1                                            
44 #X 16 #X ||| #1 7 #2 ||| X ||| -2.055 -0.693 -0.322 0  1                                                  
...                                                                                                          
</pre><p>On the source-language side, each non-terminal is marked with '#', followed by its symbol. E.g., #X is a non-terminal with the symbol X. On the target-language side, each non-terminal is also marked with '#' but uses an integer to indicate the index of the corresponding non-terminal in the source-language side. E.g., in the last rule of the above example, #2 is a non-terminal on the target-language side. It is aligned with the second non-terminal on the source-language side and shares the symbol X with its counterpart.</p>
<p>It is not difficult to transform a NiuTrans-style grammar file into a HiFST-style grammar file. For this example, we have (the same result again) </p>
<pre class="fragment">...                                                                                                          
X 44_4_902_7631_2213 3_1331_2242 -3 0.693 1.386 0 0 -1                                                       
X 44_4_902 3_1331 -2 -0.693 0 0 0 -1                                                                         
X 44_4 3 -1 -0.693 0 0 0 -1                                                                                  
...                                                                                                          
X 44_X_16 X_7 -1 2.476 0.693 0 0 -1                                                                          
X 44_X1_16_X2 X1_7_X2_8 -2 2.079 0.693 1.290 1.386 -1                                                        
X 44_X1_16_X2 X1_7_X2 -1 2.055 0.693 0.322 0 -1                                                              
...                                                                                                          
</pre><h2><a class="anchor" id="grammarpruning"></a>
Translation Grammar Pruning</h2>
<p>Once the grammar is obtained, one may consider to prune it for a more compact model as well as a more efficient decoding process. Note that grammar pruning is widely used in SMT systems. This is especially important for some large-scale translation tasks, e.g., training a Chinese-English SMT system on millions of sentence pairs. Without appropriate pruning, the translation speed might be unacceptable in some cases. Here we present a few tips on grammar pruning for better use of HiFST.</p>
<p>[1]. For the source-language of each phrase/rule, we can keep the top-n target-sides in terms of (source-to-target) translation probability. This method is adopted in most open source systems by default. Generally setting n = 10~30 is enough for most tasks.</p>
<p>[2]. We can prune away low frequency phrases/rules. For example, there is generally much noise in hierarchical phrase translation rules that occur once in the training data. So it is relatively "safe" to discard this type of rule.</p>
<p>[3]. Also, we can only keep hierarchical phrase rules that are extracted on a good-quality portion of the parallel corpus, that is, we still obtain phrasal translations from the whole corpus but extract hierarchical phrase rules from a smaller set of sentences.</p>
<p>[4]. Another method is to filter rules according to rule patterns. For example, both rule patterns "X -&gt; X w, X w" and "X -&gt; w X, w X" explain monotonic translation phenomena (where X is a variable and w is a sequence of words). In this case, we can throw away one of this patterns because they are actually doing the same thing.</p>
<p>[5]. In addition to rule filtering, we can also write the grammar to another form for more efficient decoding. For example, we can transform an SCFG to a sallow grammar which avoids unlimited nested structures and makes the decoding much faster. Please see next section for more discussion on shallow grammars.</p>
<p>The reader can refer to <a class="el" href="md_Tutorial.html#Iglesias2009">Iglesias2009</a> for more methods for grammar pruning. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sun May 25 2014 23:40:51 for Cambridge SMT System by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.3
</small></address>
</body>
</html>
