<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.4"/>
<title>Cambridge SMT System: Pruning</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.4 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('md__home_giglesias_HIFST_8github_demo-files_Docs_8dox_Tutorial_802_8pruning.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Pruning </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="lpruning"></a>
Local pruning / pruning in search</h1>
<p>Local pruning controls processing speed and memory use during translation. Only enough details are reviewed here to describe how HiFST performs pruning in search; for a detailed discussion of local pruning and pruning in search, see Section 2.2.2 of [<a class="el" href="index.html#deGispert2010">deGispert2010</a>].</p>
<p>Given a translation grammar and a source language sentence, HiFST first constructs a Recursive Transition Network (RTN) representing the translation hypotheses [<a class="el" href="index.html#Iglesias2009">Iglesias2009</a>, <a class="el" href="index.html#Iglesias2011">Iglesias2011</a>]. This is done as part of a modified CYK algorithm used to parse the source sentence under the translation grammar. The RTN is then <em>expanded</em> to an equivalent WFSA via the <a class="el" href="index.html#OpenFst">OpenFst</a> <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation. This WFSA contains the translation hypotheses along with their scores under the translation grammar. We refer to this as the `top-level' WFSA, because it is associated with the top-most cell in the CYK grid. This top-level WFSA can be pruned after composition with the language model, as described in the discussion of <a class="el" href="basic.html#basic_toplevelpruning">Admissible pruning / top-level pruning</a>. We refer to this as <em>exact search</em> or <em>exact translation</em>. In exact translation, no translation hypotheses are discarded prior to applying the complete translation and language model scores.</p>
<p>Exact translation can be done under some combinations of translation grammars, language models, and language pairs. In particular, the <a class="el" href="start.html#tgrammars_shallow">Shallow-N Translation Grammars</a> were designed for exact search. However attempting exact translation under many translation grammars would cause either the <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation or the subsequent language model composition to become computationally intractable. We therefore have developed a pruning strategy that prunes the RTN during its construction.</p>
<p>The RTN created by HiFST can be described as follows:</p>
<ul>
<li><img class="formulaInl" alt="$X$" src="form_0.png"/> is the set of non-terminals in the translation grammar, with <img class="formulaInl" alt="$S$" src="form_1.png"/> as the root</li>
<li><img class="formulaInl" alt="$\Sigma$" src="form_2.png"/> is the target language vocabulary, i.e. the terminals in the target language</li>
<li><img class="formulaInl" alt="$I$" src="form_3.png"/> is the length of the source sentence <img class="formulaInl" alt="$s$" src="form_4.png"/>, i.e. <img class="formulaInl" alt="$s = s_0...s_{I - 1}$" src="form_5.png"/></li>
<li>A new set of non-terminals is defined as <img class="formulaInl" alt="$N = \{ (x,i,j) : x \in X , 0 <= i <= j < I \}$" src="form_6.png"/><ul>
<li>Note that <img class="formulaInl" alt="$(S,0,I-1) \in N$" src="form_7.png"/></li>
</ul>
</li>
<li><img class="formulaInl" alt="$(T_u)_{u \in N}$" src="form_8.png"/>, is a family of WFSAs with input alphabet <img class="formulaInl" alt="$\Sigma \cup N$" src="form_9.png"/><ul>
<li>Each <img class="formulaInl" alt="$T_u$" src="form_10.png"/> with <img class="formulaInl" alt="$u = (x, i, j)$" src="form_11.png"/>, is a WFSA that describes all applications of translation rules with left-hand side non-terminal <img class="formulaInl" alt="$x$" src="form_12.png"/> that span the substring <img class="formulaInl" alt="$s_i ... s_j$" src="form_13.png"/></li>
<li><img class="formulaInl" alt="$T_u$" src="form_10.png"/> is associated with the CYK grid cell associated with source space <img class="formulaInl" alt="$[i,j]$" src="form_14.png"/> and headed by non-terminal <img class="formulaInl" alt="$x$" src="form_12.png"/></li>
</ul>
</li>
<li>The top-level RTN is defined as <img class="formulaInl" alt="$R_{(S,0,I-1)} = (N, \Sigma, (T_u)_{u \in N}, (S,0,I-1))$" src="form_15.png"/>.<ul>
<li>The root symbol of this RTN is <img class="formulaInl" alt="$(S,0,I-1)$" src="form_16.png"/>.</li>
<li>The WFSA <img class="formulaInl" alt="$T_{(S,0,I-1)}$" src="form_17.png"/> represents all applications of translation rules that span the entire sentence and are rooted with non-terminal <img class="formulaInl" alt="$S$" src="form_1.png"/>.</li>
</ul>
</li>
</ul>
<p>Exact translation is achieved if every <img class="formulaInl" alt="$T_u$" src="form_10.png"/> is complete (i.e. if no pruning is done) prior to the <a class="el" href="index.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation on the RTN <img class="formulaInl" alt="$R_{(S,0,I-1)}$" src="form_18.png"/>. This produces a WFSA that contains all translations that can be produced under the translation grammar.</p>
<p>The RTN pruning strategy relies on noting that each of the WFSAs <img class="formulaInl" alt="$T_{u'}$" src="form_19.png"/>, <img class="formulaInl" alt="$u' = (x', i', j')$" src="form_20.png"/>, also defines an RTN <img class="formulaInl" alt="$R_{u'}$" src="form_21.png"/>, as follows:</p>
<ul>
<li>Define a subset of non-terminals <img class="formulaInl" alt="$N' = \{ (x,i,j) : x \in X , i' <= i <=j < j' \}$" src="form_22.png"/> , i.e. <img class="formulaInl" alt="$N' \subset N$" src="form_23.png"/></li>
<li><img class="formulaInl" alt="$R_{u'} = (N', (T_u)_{u \in N'}, (x', i', j') )$" src="form_24.png"/><ul>
<li>The root symbol of this RTN is <img class="formulaInl" alt="$(x', i', j')$" src="form_25.png"/></li>
</ul>
</li>
</ul>
<p>The <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation can be applied to the RTNs <img class="formulaInl" alt="$R_{u'}$" src="form_21.png"/> to produce an WFSA containing all translations of the source string <img class="formulaInl" alt="$S_{i'} ... S_{j'}$" src="form_26.png"/> using derivations rooted in the non-terminal <img class="formulaInl" alt="$x'$" src="form_27.png"/>. This WFSA can be pruned and used in place of the original <img class="formulaInl" alt="$T_{u'}$" src="form_19.png"/>.</p>
<p>Because of the possibility of search errors we refer to this as 'local pruning' or inadmissible pruning. There is the possibility that pruning any of the <img class="formulaInl" alt="$T_u$" src="form_10.png"/> may possibly cause some good translations to be discarded. For this reason it is important to tune the pruning strategy for the translation grammar and language model. Once pruning has been set, the benefits are</p>
<ul>
<li>faster creation of the top-level WFSA via the <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation</li>
<li>faster composition of the translation WFSA with the language model</li>
<li>less memory used in RTN construction and language model composition</li>
</ul>
<p>Local pruning should be done under the combined translation and the language model scores, rather than under the translation grammar scores alone alone. However, the LM used in local pruning can be relatively weak. For example, if the main language model used in translation is a 4-gram, perhaps a 3-gram or even a bigram language model could be used in local pruning. Using a smaller language model will make pruning faster, as will an efficient scheme to remove the scores of the language models used in pruning. The lexicographic semiring, see <a class="el" href="basic.html#basic_scores">Scores, Costs, and Semirings</a>, makes this last operation easy.</p>
<h1><a class="anchor" id="local_prune"></a>
Local Pruning Algorithm</h1>
<p>HiFST monitors the size of the <img class="formulaInl" alt="$T_u$" src="form_10.png"/> during translation. Any of these automata that exceed specified thresholds are converted to WFSAs and pruned. Subsequent expansion of the RTN <img class="formulaInl" alt="$R_{(S,0,I-1)}$" src="form_18.png"/> is then done with respect to the pruned versions of <img class="formulaInl" alt="$T_u$" src="form_10.png"/>.</p>
<p>Local pruning is controlled via the following HiFST parameters: </p>
<pre class="fragment">hifst.localprune.enable=yes # must be set to activate local pruning
hifst.localprune.conditions=NT_1,span_1,size_1,threshold_1,...,NT_N,span_N,size_N,threshold_N
hifst.localprune.lm.load=lm_1,...lm_K
hifst.localprune.lm.featureweights=scale_1,...,scale_K
hifst.localprune.lm.wps=wp_1,...,wp_K
</pre><p>In the above, an arbitrary number N of tuples (<code>NT_n</code>, <code>span_n</code>, <code>size_n</code>, <code>threshold_n</code>) can be provided; similarly, an arbitrary number K of language model parameters (<code>lm_k</code>, <code>scale_k</code>, <code>wp_k</code>) can also be used in pruning.</p>
<p>Pruning is applied during construction of the RTN, as follows:</p>
<ul>
<li>If any <img class="formulaInl" alt="$T_u$" src="form_10.png"/> satisfies the following conditions for any parameter set (<code>NT_n</code>, <code>span_n</code>, <code>size_n</code>, <code>threshold_n</code>), n=1,...,N<ul>
<li>NT_n = X</li>
<li>span_n &lt;= j-i</li>
<li>size_n &lt;= number of states of <img class="formulaInl" alt="$T_u$" src="form_10.png"/>, computed via <a class="el" href="index.html#OpenFst">OpenFst</a> <code>NumStates()</code></li>
</ul>
</li>
<li>then <img class="formulaInl" alt="$T_u$" src="form_10.png"/> is pruned as follows:<ul>
<li>OpenFst <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> converts <img class="formulaInl" alt="$R_u$" src="form_28.png"/> to a WFSA</li>
<li><a href="http://www.openfst.org/twiki/bin/view/FST/RmEpsilonDoc">RmEpsilon</a>,<a href="http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc">Deteminize</a>, and <a href="http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc">Minimize</a> generate a compacted WFSA</li>
<li><a href="http://www.openfst.org/twiki/bin/view/FST/ComposeDoc">Composition</a> with K language model(s) WFSAs<ul>
<li>The parameters (<code>lm_k</code>, <code>scale_k</code>, <code>wp_k</code>) specify the language models, language model scale factors, and word penalties to be applied</li>
</ul>
</li>
<li>OpenFst <a href="http://www.openfst.org/twiki/bin/view/FST/PruneDoc">Prune</a> is applied with threshold <code>threshold_n</code></li>
<li>Language model scores are removed by copying component weights in the lexicographic semiring, see <a class="el" href="basic.html#basic_scores">Scores, Costs, and Semirings</a></li>
<li><a href="http://www.openfst.org/twiki/bin/view/FST/RmEpsilonDoc">RmEpsilon</a>,<a href="http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc">Deteminize</a>, and <a href="http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc">Minimize</a>, yielding a pruned WFSA <img class="formulaInl" alt="$T_u$" src="form_10.png"/> with only translation scores and target language symbols</li>
</ul>
</li>
<li>The pruned version of <img class="formulaInl" alt="$T_u$" src="form_10.png"/> is then used in place of the original version in the RTN</li>
</ul>
<h1><a class="anchor" id="lpruning_effects"></a>
Effect on Speed, Memory, Scores</h1>
<p>Pruning in search is particularly important when running HiFST with grammars that are more powerful than the shallow grammar used in earlier examples.</p>
<p>For example, HiFST can be run with a full Hiero grammar, while monitoring memory consumption via the UNIX top command: </p>
<pre class="fragment"> &gt; (time hifst.O2 --config=configs/CF.hiero) &amp;&gt; log/log.hiero
</pre><p>The memory use is approximately 2GB and translation takes approximately 1m45s. (The resource consumption may vary depending on your hardware, we provide these numbers to illustrate the effect of local pruning.)</p>
<p>If translation is performed with the same grammar and language model, but with local pruning, </p>
<pre class="fragment"> &gt; (time hifst.O2 --config=configs/CF.hiero.localprune) &amp;&gt; log/log.hiero.localprune
</pre><p>then the memory consumption is reduced to under 300MB and the processing time to approximately 25s. Inspecting the log file indicates that local pruning was applied to 18 sublattices for the second sentence: </p>
<pre class="fragment"> &gt; tail -n 16 log/log.hiero.localprune | head -n 12
 Fri May  9 15:20:38 2014: run.INF:=====Translate sentence 1:1 20870 2447 5443 50916 78159 3621 2
 Fri May  9 15:20:38 2014: run.INF:Loading hierarchical grammar: G/rules.hiero.gz
 Fri May  9 15:20:38 2014: run.INF:loading LM=M/lm.4g.mmap
 Fri May  9 15:20:38 2014: run.INF:loading LM=M/lm.3g.mmap
 Fri May  9 15:20:38 2014: run.INF:Stats for Sentence 1: local pruning, number of times=0
 Fri May  9 15:20:38 2014: run.INF:End Sentence ******************************************************
 Fri May  9 15:20:38 2014: run.INF:Translation 1best is: 1 3 9121 384 6 2756 7 3 4144 6 159312 42 1341 2
 Fri May  9 15:20:38 2014: run.INF:=====Translate sentence 2:1 1716 20196 95123 154 1049 6778 996 9 239837 7 1799 4 2
 Fri May  9 15:20:47 2014: run.INF:Stats for Sentence 2: local pruning, number of times=18
 Fri May  9 15:20:55 2014: run.INF:End Sentence ******************************************************
 Fri May  9 15:20:56 2014: run.INF:Translation 1best is: 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2
 Fri May  9 15:20:56 2014: main.INF:hifst.O2 ends!
</pre><p>In this case, local pruning has no effect on the translations produced: </p>
<pre class="fragment"> &gt; head output/exp.hiero.localprune/hyps output/exp.hiero/hyps
 ==&gt; output/exp.hiero.localprune/hyps &lt;==
 1 3 9121 384 6 2756 7 3 4144 6 159312 42 1341 2
 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2

 ==&gt; output/exp.hiero/hyps &lt;==
 1 3 9121 384 6 2756 7 3 4144 6 159312 42 1341 2
 1 3 1119 6 3 9121 1711 63 355 85 7 369 24 3 13907 17 3 628 5 2
</pre><p>The effect of pruning can be more dramatic on longer, more difficult to translate sentences. For example, the third sentence in this set is difficult to translate under the full Hiero grammar without pruning, although it can be translated using local pruning as </p>
<pre class="fragment"> &gt; (time hifst.O2 --config=configs/CF.hiero.localprune --range=3:3) &amp;&gt; log/log.hiero.localprune2
</pre><p>Even with local pruning, the processing time for this one sentence is over 4 minutes.</p>
<p>By comparison, translation is much faster with much more aggressive local pruning, which we introduce via command line options to override the settings in the configuration file: </p>
<pre class="fragment"> &gt; (time hifst.O2 --config=configs/CF.hiero.localprune --range=3:3 --hifst.lattice.store=output/exp.hiero.localprunemore/LATS/?.fst.gz --target.store=output/exp.hiero.localprunemore/hyps --hifst.localprune.conditions=X,3,10,1,V,3,10,1) &amp;&gt; log/log.hiero.localprune3
</pre><p>Translation finishes in less than 6 seconds, but this more aggressive local pruning changes the translation hypothesis: </p>
<pre class="fragment"> &gt; zcat output/exp.hiero.localprune/LATS/3.fst.gz | printstrings.O2 -w --semiring=lexstdarc -m wmaps/wmt13.en.wmap 2&gt;/dev/null
 &lt;s&gt; however , in the heart of the take the last myth , arguing that the rare cases of fraud in elections in the united states , the deaths of a lightning strike . &lt;/s&gt;  128.842,-0.150391

 &gt; zcat output/exp.hiero.localprunemore/LATS/3.fst.gz | printstrings.O2 -w --semiring=lexstdarc -m wmaps/wmt13.en.wmap 2&gt;/dev/null
 &lt;s&gt; however , in the heart of the take the last myth , arguing that a rare cases of fraud in elections in the united states , the deaths of a lightning strike . &lt;/s&gt;  130.054,-0.943359
</pre><p>The best hypothesis generated with less local pruning in <code>exp.hiero.localprune/</code> has a combined translation and language model score of 128.842 . This hypothesis does not survive more local pruning in <code>exp.hiero.localprunemore/</code> , where the best hypothesis has a higher combined score of 130.054 .</p>
<h1><a class="anchor" id="chopping"></a>
Source Sentence Chopping</h1>
<p>Long source sentences make the translation process slow and expensive in memory consumption; see [<a class="el" href="index.html#Allauzen2014">Allauzen2014</a>] for a discussion of how source sentence length affects computational complexity and memory use by HiFST and HiPDT. There are various strategies for controlling translation complexity; pruning has been discussed (<a class="el" href="md__home_giglesias_HIFST_8github_demo-files_Docs_8dox_Tutorial_802_8pruning.html#lpruning">Local pruning / pruning in search</a>), and it is also possible to set the maximum span and gap spans allowed in translation so as to control computational complexity. However translation quality can be affected if pruning is too heavy or if span constraints are set too aggressively.</p>
<p>An alternative approach is to 'chop' long sentences into shorter segements which can then be translated separately. If the sentence chopping is done carefully, the impact on the translation quality can be minimized. The benefits to chopping are faster translation that consumes less memory. The potential drawbacks are twofold: chopping can prevent the search procedure from finding good hypothesis under the grammar, and care must be taken to correctly apply the target language model at the sentence level.</p>
<h2><a class="anchor" id="chopping_gb"></a>
Grammar-based Sentence Chopping</h2>
<p>Chopping can be done by inserting the special 'chop' symbol '0' in the source sentence, and then translating with a modified grammar. The chopping grammar is constructed so that translation rules are not applied across the chopping points, thus limiting the space of translation that are generated. Conceptually, translation proceeds as:</p>
<ol type="1">
<li>the translation grammar is applied separately to source sentence segments demarcated by chop symbols</li>
<li>local pruning can be applied to the translations of these segments</li>
<li>the resulting WFSAs containing translations of the segments are concatenated under the chopping grammar, possibly with local pruning</li>
<li>the language model is applied</li>
<li>top-level, admissible pruning is done under the translation and languaage model scores</li>
</ol>
<p>In this way the FSTs produced by translating the segments are concatenated prior to application of the target language model; in this way the language model context is not broken by the source sentence chopping.</p>
<p>As an example, a grammar modified for chopping contains the following rules (without weights): </p>
<pre class="fragment"> R 1 1
 R R_D_X R_D_X
 R R_X R_X

 T 0 0
 T T_D_X T_D_X
 T T_X T_X

 S S_U S_U
 S Q Q
 Q R R
 U T T
</pre><p>The rules in the first block above are similar to those used in the usual Hiero grammar, with the original '<code>S</code>' changed to '<code>R</code>'. These rules are responsible for concatenating the partial translations of the source sentence, starting from the sentence-start symbol '1', up to but not including the first instance of the chopping symbol '0'.</p>
<p>Each subsequent sequence of source words starting with symbol '0', is handled in a similar way by the second block of rules above. Note that the only rule that can be applied to the input symbol '0' is '<code>T 0 0</code>', making the translation of each chopped segment independent. This makes use of the OpenFST convention of mapping 0 to epsilon: the 0's in the input are parsed as regular symbols by HiFST, while 0's on the output side are mapped to epsilons and ignored in composition with the language model.</p>
<p>The third block of rules above will join together the results obtained for each chopped segment. As with the glue rule '<code>S</code>' in the usual Hiero grammar, it is necessary to allow this new set of rules to be applied to any span. This is done by setting </p>
<pre class="fragment">  cykparser.ntexceptionsmaxspan=S,Q,R,T,U
</pre><p>The additional mapping provided by the last two rules controls the pruning applied to the top CYK cell relative to each chopped segment: </p>
<pre class="fragment">  hifst.localprune.conditions=Q,1,100,12,U,1,100,12,X,5,10000,9,V,3,20000,9
</pre><p>In the above example tighter parameters are chosen for '<code>Q</code>' and '<code>U</code>' to force pruning. In this way the final lattice obtained by concatenation is prevented from growing too large. However a wider beam (12) with respect to the other cell types is used, to avoid discarding too many potentially useful hypotheses.</p>
<p>It is possible to specify explicitly that FSTs generated for rules with LHS '<code>X</code>' or '<code>V</code>' can be kept as pointers rather then expanded in the FST (RTN) that is built for a higher CYK cell. This is achieved setting </p>
<pre class="fragment">  hifst.replacefstbyarc=X,V
  hifst.replacefstbyarc.exceptions=S,R,T
</pre><p>The second line above prevents substitution for rules with LHS '<code>S</code>', '<code>R</code>' and '<code>T</code>'. It is better to have a fully expanded FST for these rules for more effective optimisation (Determinization and Minimisation).</p>
<h2><a class="anchor" id="chopping_eg"></a>
Converting Grammars and Input Text for Chopping</h2>
<p>The usual Hiero grammar can be converted for chopping, as follows; note that no-cost, 0 valued, weights are added to rules :</p>
<p>First, create the chopping and glue rules: </p>
<pre class="fragment"> &gt; (echo "T T_D_X T_D_X 0" ; echo "T T_X T_X 0" ; echo "T 0 0 0") &gt; G/rules.hiero.chop
 &gt; (echo "S S_U S_U 0" ; echo "U T T 0" ; echo "Q R R 0" ; echo "S Q Q 0") &gt;&gt; G/rules.hiero.chop
 &gt; cat G/rules.hiero.chop
 T T_D_X T_D_X 0
 T T_X T_X 0
 T 0 0 0
 S S_U S_U 0
 U T T 0
 Q R R 0
 S Q Q 0
</pre><p>Next, append all rules, mapping glue rules with LHS S to LHS R: </p>
<pre class="fragment"> &gt; zcat G/rules.hiero.gz | sed 's,S,R,g' &gt;&gt; G/rules.hiero.chop
 &gt; gzip G/rules.hiero.chop
</pre><p>The source text (<code>RU/RU.set1.chop.idx</code>) will be chopped simply inserting the chopping marker '0' after each comma (integer mapped to 3 in the Russian wordmap); this is a simplistic approach that is easily implemented for this demonstration: </p>
<pre class="fragment"> &gt; sed 's, 3 , 3 0 ,g' RU/RU.set1.idx &gt; RU/RU.set1.chopping.idx

 &gt; diff RU/RU.set1.idx RU/RU.set1.chopping.idx | head -n4
 3c3
 &lt; 1 109 5 458 756435 1225 1358 60145 3 12725 3 11 3678 66369 7 1799 5 1317 2946 45 32023 3 75 3678 1102 24 10272 28960 4 2
 ---
 &gt; 1 109 5 458 756435 1225 1358 60145 3 0 12725 3 0 11 3678 66369 7 1799 5 1317 2946 45 32023 3 0 75 3678 1102 24 10272 28960 4 2
</pre><p>The following command will translate lines 3,12,19 in the Russian integer-mapped file RU/RU.set1.chop.idx : </p>
<pre class="fragment"> # Run HiFST, with chopping.  Input is the chopped source RU/RU.set1.chopping.idx.
 # hypotheses are written to output/exp.chopping/chop/hyps and lattices to output/exp.chopping/chop/LATS/
 &gt; (time hifst.O2 --config=configs/CF.hiero.chopping) &amp;&gt; log/log.chopping.chop
</pre><p>Now we decode again keeping the same configuration (same grammar and language model), but with the non-chopped version of the input: </p>
<pre class="fragment"> # Run HiFST, without chopping.  Input is the original, unchopped source RU/RU.set1.idx
 # hypotheses are written to output/exp.chopping/nochop/hyps and lattices to output/exp.chopping/nochop/LATS/
 &gt; (time hifst.O2 --config=configs/CF.hiero.chopping --source.load=RU/RU.set1.idx --target.store=output/exp.chopping/nochop/hyps --hifst.lattice.store=output/exp.chopping/nochop/LATS/?.fst.gz) &amp;&gt; log/log.chopping.nochop
</pre><p>Comparing the time and memory consumption of the two experiments shows that source-sentence chopping is significantly faster and uses far less memory; in particular, local pruning is required less often under the chopping grammar: </p>
<pre class="fragment">                                         Number of local prunings
 Input      Tot time      Max memory    Sent 3    Sent 12   Sent 19
 --------   --------      ----------    ------    -------   -------
 Unchopped  22m 37s         5.4Gb        92        106       168
 Chopped     3m 57s         0.8Gb        34         46        20
</pre><p>However, chopping restricts the space of translations. Looking at the scores of the best translation hypotheses, chopping the source sentence prevents the decoder from finding the best scoring hypothesis under the grammar; for the third sentence, the hypothesis produced without chopping has a lower (i.e. better) combined cost (128.842) than the hypothesis produced with chopping (130.309):</p>
<pre class="fragment"> # find the score of the best hypothesis for the 3rd sentence, without chopping
 &gt; zcat output/exp.chopping/nochop/LATS/3.fst.gz | printstrings.O2 --semiring=lexstdarc
 1 106 4 9 3 1552 6 3 96 3 200 8072 4 5452 10 3 4143 535 6 1206 9 628 9 3 232 56 4 3 2723 6 11 21441 2645 5 2       128.842,-0.150391
 # find the score of the best hypothesis for the 3rd sentence, with chopping
 &gt; zcat output/exp.chopping/chop/LATS/3.fst.gz | printstrings.O2 --semiring=lexstdarc
 1 106 4 9 3 1552 6 3 96 3 200 8072 4 5452 10 3 4143 535 6 1206 9 628 9 3 232 56 4 3 2723 6 11 21441 2645 5 2       130.309,1.31641
</pre><h2><a class="anchor" id="chopping_sseg"></a>
Chopping by Explicit Source Sentence Segmentation</h2>
<p>It is also possible to segment and translate each segment completely independently, as shown in this example for sentence 3. Here, the original Russian sentence is chopped into three shorter sentences which are to be translated independently, as follows: </p>
<pre class="fragment"> # split sentence 3 into 4 separate sentences
 &gt; awk 'NR==3' RU/RU.set1.idx | sed 's, 3 , 3 2\n1 ,g'  &gt; RU/RU.sent3.chopped
 &gt; cat RU/RU.sent3.chopped
 1 109 5 458 756435 1225 1358 60145 3 2
 1 12725 3 2
 1 11 3678 66369 7 1799 5 1317 2946 45 32023 3 2
 1 75 3678 1102 24 10272 28960 4 2

 # run HiFST over all segments
 &gt; hifst.O2 --config=configs/CF.hiero.chopping --source.load=RU/RU.sent3.chopped --target.store=output/exp.chopping/sent3/hyps --hifst.lattice.store=output/exp.chopping/sent3/LATS/seg.?.fst --range=1:4

 # concatenate output lattices
 &gt; fstconcat output/exp.chopping/sent3/LATS/seg.1.fst output/exp.chopping/sent3/LATS/seg.2.fst | fstconcat - output/exp.chopping/sent3/LATS/seg.3.fst | fstconcat - output/exp.chopping/sent3/LATS/seg.4.fst &gt; output/exp.chopping/sent3/LATS/sent.fst

 # print output string
 &gt; cat output/exp.chopping/sent3/LATS/sent.fst | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
 &lt;s&gt; however , in the middle of the last myth , believe &lt;/s&gt; &lt;s&gt; by saying , &lt;/s&gt; &lt;s&gt; the cases of fraud in elections in the united states , a rare &lt;/s&gt; &lt;s&gt; the deaths of a lightning strike . &lt;/s&gt;  141.166,-12.5742
</pre><p>In the above example, the language model is applied separately to the translations of each segment leading to the substrings "&lt;/s&gt; &lt;s&gt;" in the hypothesis. A transducer can be built to remove these from the output lattice, as follows </p>
<pre class="fragment"> &gt; mkdir tmp
 &gt; echo -e "0\t1\t1\t1" &gt; tmp/strip_1_2.txt
 &gt; echo -e "1\t2\t2\t0" &gt;&gt; tmp/strip_1_2.txt
 &gt; echo -e "2\t1\t1\t0" &gt;&gt; tmp/strip_1_2.txt
 &gt; echo -e "2\t2\t0\t0" &gt;&gt; tmp/strip_1_2.txt
 &gt; echo -e "1\t3\t2\t2" &gt;&gt; tmp/strip_1_2.txt
 &gt; echo -e "3" &gt;&gt; tmp/strip_1_2.txt
 &gt; awk '$2 != 1 &amp;&amp; $2 != 2 {printf "1\t1\t%d\t%d\n", $2,$2}' wmaps/wmt13.en.wmap &gt;&gt; tmp/strip_1_2.txt
 &gt; fstcompile  --arc_type=tropical_LT_tropical tmp/strip_1_2.txt | fstarcsort &gt; tmp/strip_1_2.fst

 # apply the strip_1_2.fst transducer to the fst for sentence 3
 &gt; fstcompose output/exp.chopping/sent3/LATS/sent.fst tmp/strip_1_2.fst | fstproject --project_output | fstrmepsilon &gt; output/exp.chopping/sent3/LATS/sent_no12.fst

 # look at output
 &gt; cat output/exp.chopping/sent3/LATS/sent_no12.fst | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
 &lt;s&gt; however , in the middle of the last myth , believe by saying , the cases of fraud in elections in the united states , a rare the deaths of a lightning strike . &lt;/s&gt;  141.166,-12.5742
</pre><p>Note however that the language model score for this hypothesis are not correct, since the language model histories cannot span translations across the chopped segments.</p>
<p>To fix this, the applylm tool (see <a class="el" href="rescoring.html#rescoring_lm">Efficient Language Model Rescoring</a>) can be used to remove and reapply the language model so that it spans the source segment translations. The following example simply removes the language model scores from output/exp.chopping/sent3/LATS/sent_no12.fst and then reapplies them via composition </p>
<pre class="fragment"> &gt; applylm.O2 --lm.load=M/lm.4g.mmap --lm.featureweights=1 --lm.wps=0.0 --semiring=lexstdarc --lattice.load=output/exp.chopping/sent3/LATS/sent_no12.fst --lattice.store=output/exp.chopping/sent?/LATS/sent_no12_rescore.fst --lattice.load.deletelmcost --range=3:3
</pre><p>The rescored output is written to <code>output/exp.chopping/sent3/LATS/sent_no12_rescore.fst</code> with correctly applied language model scores. The total translation cost is much lower (better) than when segment hypotheses are simply combined (i.e. 115.855 vs. 141.166): </p>
<pre class="fragment"> &gt; cat output/exp.chopping/sent3/LATS/sent_no12_rescore.fst | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
 &lt;s&gt; however , in the heart of the take the last myth , arguing that the rare cases of fraud in elections in the united states , the deaths of a lightning strike . &lt;/s&gt;       115.855,-13.1377</pre> </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Thu Aug 28 2014 20:06:34 for Cambridge SMT System by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.4 </li>
  </ul>
</div>
</body>
</html>
