<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Cambridge SMT System: Rescoring with Bilingual Neural Network Models</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('bilm.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Rescoring with Bilingual Neural Network Models </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><b>Important Note:</b> In order to run this tutorial you will first need to clone <a href="https://github.com/ucam-smt">our local copy of nplm</a>, and recompile ucam-smt package following the instructions in Makefile.inc.</p>
<p>In the following we describe how to rescore lattices with bilingual models as described by [<a class="el" href="intro.html#Devlin2014">Devlin2014</a>].</p>
<p>Bilingual Neural Network models are simple feedforward models with word embeddings on the first layer. They are trained with bilingual samples extracted from the aligned parallel data. The model M/nplm.s3t4 provided with this tutorial is trained with [<a class="el" href="intro.html#NPLM">NPLM</a>] on samples that have 3 source words and 4 target words: </p><pre class="fragment"> s1 s2 s3 t1 t2 t3 t4
</pre><p>t1 t2 t3 is the target history; t4 is the prediction, which is <em>affiliated</em> to s2, the center of the window of 3 source words. The affiliation heuristic smooths the word alignments so that there is exactly one link per target word and a sample can be extracted for every target word. More details in [<a class="el" href="intro.html#Devlin2014">Devlin2014</a>].</p>
<p><b>Important Note:</b> In order to generate models that are consistent with ucam-smt tools, the training samples must be numberized. As numbers are mapped to different words in source and target, but the word embeddings are shared, we offset the source integers with an arbitrarily big number, so that NPLM word embeddings discriminate source words from target words. The running example uses an offset of 10000000. This must be consistent with the bilingual model application step.</p>
<p>During decoding or rescoring, we also need to determine the affiliations for the words in each rule. These can be computed on-the-fly if internal word alignments are available. We compute the affiliations offline and define a grammar with affiliations for each rule. </p><pre class="fragment"> &gt; zcat G/grammar.affiliation.gz | grep _ | grep 1_0 | head -2
 V 3_939 1108_4 4.114964 3.335020 -2 -1 0 0 0 0 -1 5.198832 4.680031     1_0
 V 3_1600 5_569_14 4.179298 0.148420 -3 -1 0 0 0 0 -1 5.587708 6.762751  0_1_0
</pre><p>In rule V 3_939 1108_4, 1_0 means that word 1108 is aligned to 939 and 4 is aligned to 3. In rule V 3_1600 5_569_14, 0_1_0 means that 5 and 14 are aligned to 3; 569 is aligned to 1600.</p>
<p>The following script contains a simple recipe that shows how to apply a bilingual model to a translation lattice: </p><pre class="fragment"> &gt; scripts/runBiLMRescoring.bash
</pre><p>The script runs various steps necessary to achieve this goal, including the usual pipeline comprising translation, alignment and feature vector lattice generation. These are described respectively in:</p><ul>
<li><a class="el" href="basictrans.html#basic_trans">Basic Translation Operations</a></li>
<li><a class="el" href="basictrans.html#mert_nblist_derivations">Step 2. Guided Translation / Forced Alignment</a></li>
<li><a class="el" href="basictrans.html#mert_alilats">Step 3. Hypotheses with Unweighted Feature Vectors</a></li>
</ul>
<p>In the following we highlight the novel steps, specific to the bilingual model rescoring procedure.</p>
<p><b>Running HiFST in affiliation mode</b></p>
<p>Running HiFST in affiliation mode is very similar to running in alignment mode, with the difference that instead of rules we now have one source link for each target word. With this additional information provided in the grammar, HiFST runs in affiliation mode as so: </p><pre class="fragment"> &gt; hifst.${TGTBINMK}.bin config/CF.hifst,config/CF.hifst-a,config/CF.lm --hifst.alilatsmode.type=affiliation --range=1:$M --featureweights=$FW --nthreads=15 --hifst.lattice.store=output/bilmexp/AFILATS/?.fst.gz
</pre><p>Affiliation Lattices have affiliation sequences on the input and words on the output. A translation hypothesis has one or more affiliation sequences. For example: </p><pre class="fragment"> &gt; printstrings.O2 --input=output/bilmexp/AFILATS.STD/1.fst.gz  --print-input-output-labels -n 10 | grep "1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2"
 1 5 6 4 2 7 7 9 9 9 12 11 10 14         1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2
 1 5 6 4 2 7 7 9 9 9 13 11 10 14         1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2
</pre><p>For instance, word 9967 is linked to the 10th source word in both afiliation sequences. But word 717 is linked to the 12th word in one case, and to the 13th word in the other.</p>
<p><b>Affiliation Lattice Disambiguation</b></p>
<p>We disambiguate these affiliation lattices with the <code>disambignffst</code> tool. See <a class="el" href="nfdisambiguation.html">Non-functional WFST Disambiguation</a> for more details on how this tool works.</p>
<p><b>Composition of an Affiliation lattice with a Bilingual Neural Network Model.</b></p>
<p>For the the bilingual model application we use the <code>applylm</code> tool described in <a class="el" href="basictrans.html#rescoring_lm">Language Model Rescoring</a>. </p><pre class="fragment"> &gt; applylm.${TGTBINMK}.bin --range=1:$M --nthreads=1 --lm.load=models/nplm.s3t4  --lm.featureweights=1  --lm.wps=0 --lattice.load=output/bilmexp/AFIDETLATS.0W/?.fst.gz --usebilm=yes --usebilm.sourcesize=3 --usebilm.sourcesentencefile=AR/mt02_05_tune.ara.special.idx --lattice.store=output/bilmexp/BLMONLY/?.fst.gz
</pre><p>The tool takes as input unweighted unambiguous lattices available in output/bilmexp/AFIDETLATS.0W/. The output is a lattice with the bilingual model scores. The program option <code>--usebilm=yes</code> enables bilingual composition. Note that an NPLM model is not aware of distinctions on the input between source and target; this has to be provided manually by the user with <code>--usebilm.sourcesize=3</code>. Finally, <code>--usebilm.sourcesentencefile=</code> points to a special version of the source file. Compare with the one used for HiFST: </p><pre class="fragment">&gt; head -1 AR/mt02_05_tune.ara.special.10first.idx
1 10000180 10000003 10000447 10000003 10001305 10006008 10000009 10001796 10006264 10022050 10000003 10000250 2
&gt; head -1 AR/mt02_05_tune.ara.10first.idx
1 180 3 447 3 1305 6008 9 1796 6264 22050 3 250 2
</pre><p>The difference is the aforementioned offset to ensure that NPLM distinguishes source words from target words.</p>
<p><b>Adding the bilingual feature to the feature vector lattices</b></p>
<p>The script has a little function <code>addFeatureToVECFEA</code> that pipes OpenFst/ucam-smt tools to add the new feature into the feature vector lattices. Recall that a feature vector lattice contains hypotheses with their individual feature contributions. </p><pre class="fragment"> &gt; FW=1.000000,0.820073,1.048347,0.798443,0.349793,0.286489,15.352371,-5.753633,-3.766533,0.052922,0.624889,-0.015877
 &gt; printstrings.O2 --input=output/bilmexp/VECFEA/1.fst.gz  --semiring=tuplearc --tuplearc.weights=$FW -w --sparseformat
 1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2       0,9,1,55.5727,2,10.626,3,8.83496,4,-14,5,-8,6,-5,10,-8,11,29.29,12,31.3643
</pre><p>The highest feature index is 12. The new feature vector lattices contain the extra bilingual feature at position 13. </p><pre class="fragment"> &gt; printstrings.O2 --input=output/bilmexp/VECFEA+BLM/1.fst.gz  --semiring=tuplearc --tuplearc.weights=$FW,0 -w --sparseformat
 1 3 511 342 1480 866 11 3 3286 5 717 35351 9967 2       0,10,1,55.5727,2,10.626,3,8.83496,4,-14,5,-8,6,-5,10,-8,11,29.29,12,31.3643,13,33.4465
</pre><p>Note that the feature weights provided to the tool need an extra 0. Otherwise, the following message would appear: </p><pre class="fragment"> feature vector has a larger dimensionality than the parameters. Params: 12 Features: 13
</pre><p>At this point, you only need to run <code>lmert</code> tool to get a new set of weights, e.g. using as starting parameters $FW,0. See <a class="el" href="basictrans.html#lmert">Lattice MERT</a> for more details. Use <code>printstrings</code> tool to get the best hypotheses under the new weights. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Wed May 25 2016 10:13:37 for Cambridge SMT System by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
