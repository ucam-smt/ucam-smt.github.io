<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>Cambridge SMT System: Translation and FST Operations</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('basictrans.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Translation and FST Operations </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><b>Notes:</b></p>
<ul>
<li>Make sure that environment variables are set as described in <a class="el" href="build.html#tutorial_install">Tutorial Installation</a> and <a class="el" href="build.html#hifst_paths">HiFST Paths and Environment Variables</a>.</li>
<li>Make sure that the language models are downloaded and uncompressed into the <code>$DEMO/M/</code> directory.</li>
<li>Make sure that the wordmaps are uncompressed in the <code>$DEMO/wmaps/</code> directory</li>
<li>Make sure you're in the <code>$DEMO</code> directory.</li>
</ul>
<blockquote class="doxtable">
<p>&gt; cd $DEMO</p>
<p></p>
</blockquote>
<h1><a class="anchor" id="basic_trans"></a>
Basic Translation Operations</h1>
<p>The first demonstration exercise is to generate translations of integer-mapped Russian text using the translation grammar and English n-gram language model provided. HiFST is configured to generate one-best translation hypotheses as well as translation lattices.</p>
<p>The baseline configuration file is <code>configs/CF.baseline</code>, which also contains comments giving brief explanations of HiFST options. The following command will translate the first 2 lines in the Russian integer-mapped file <code>RU/RU.set1.idx</code>: </p>
<pre class="fragment"># Run HiFST
&gt; mkdir log
&gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline &amp;&gt; log/log.baseline
</pre><p>The log file output can be viewed as: </p>
<pre class="fragment">&gt; tail -n 11 log/log.baseline
Fri Apr  3 12:48:16 2015: run.INF:loading LM=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap
Fri Apr  3 12:48:16 2015: run.INF:Stats for Sentence 1: local pruning, number of times=0
Fri Apr  3 12:48:16 2015: run.INF:End Sentence ******************************************************
Fri Apr  3 12:48:16 2015: run.INF:Writing lattice 1 to ... output/exp.baseline/LATS/1.fst.gz
Fri Apr  3 12:48:16 2015: run.INF:Translation 1best is: 1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 
Fri Apr  3 12:48:16 2015: run.INF:=====Translate sentence 2:1 16055 3 102 5182 66 18 23602 12611 5 6522 2377 3431 3 98 52858 61 46 2140 4422 15871 25 67408 17658 26 1731 19663 4 2
Fri Apr  3 12:48:16 2015: run.INF:Stats for Sentence 2: local pruning, number of times=0
Fri Apr  3 12:48:22 2015: run.INF:End Sentence ******************************************************
Fri Apr  3 12:48:22 2015: run.INF:Writing lattice 2 to ... output/exp.baseline/LATS/2.fst.gz
Fri Apr  3 12:48:22 2015: run.INF:Translation 1best is: 1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2 
Fri Apr  3 12:48:22 2015: ~MainClass.INF:hifst.O2.bin ends!
</pre><p>The best scoring translation hypotheses are given in integer-mapped form, e.g. for the first Russian sentence, the best-scoring translation hypothesis is </p>
<pre class="fragment">run.INF:Translation 1best is: 1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2
</pre><h1><a class="anchor" id="printing_hyps"></a>
Printing the 1-Best Hypotheses</h1>
<p>The configuration file <code>configs/CF.baseline</code> instructs HiFST to write its 1-best translations to the output file <code>output/exp.baseline/hyps</code> (see the <code>target.store=output/exp.baseline/hyps</code> specification in the config file). The contents of this file should agree with the Translation 1best entries in the log file (compare these results to the entries in the log file, above): </p>
<pre class="fragment"> &gt; cat output/exp.baseline/hyps
 1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 
 1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2
</pre><p>The FST Archive (FAR) command line tools (in the <a class="el" href="intro.html#OpenFst">OpenFst</a> FAR <a href="http://openfst.org/twiki/bin/view/FST/FstExtensions">extensions</a>) can be used to load the English wordmap (<code>wmaps/wmt13.en.wmap</code>) and print the hypotheses in readable form: </p>
<pre class="fragment">&gt; farcompilestrings --entry_type=line output/exp.baseline/hyps | farprintstrings --symbols=wmaps/wmt13.en.wmap
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;
&lt;s&gt; amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt;
</pre><p>Note that loading the English wordmap can be time consuming due to its size. Ideally, in processing multiple translation hypotheses, the wordmap should be loaded only once, rather than once for each sentence.</p>
<h1><a class="anchor" id="basic_latshyps"></a>
Extracting the Best Translation from a Lattice</h1>
<p>The configuration file also directs HiFST to write translation lattices to <code>output/exp.baseline/LATS/?.fst.gz</code> (see the <code>hifst.lattice.store=output/exp.baseline/LATS/?.fst.gz</code> specification in the config file). Note the use of the placeholder '<code>?</code>' in the argument <code>.../LATS/?.fst.gz</code> . The placeholder is replaced by the line number of sentence being translated, e.g. so that <code>.../LATS/2.fst.gz</code> is a weighted finite state transducer (WFST) containing translations of the second line in the source text file. Note also the use of the '<code>.gz</code>' extension: when this is provided, lattices are written as gzipped files.</p>
<p><a class="el" href="intro.html#OpenFst">OpenFst</a> operations can be used to compute the <b>shortest path</b> through each of these output lattices, and the results should agree with the top-scoring hypotheses in the file <code>output/exp.baseline/hyps</code> and the log file: </p>
<pre class="fragment">&gt; echo `zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint | awk '{print $3}'`
1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2
&gt; echo `zcat output/exp.baseline/LATS/2.fst.gz | fstshortestpath | fsttopsort | fstprint | awk '{print $3}'`
1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2
</pre><p>The English wordmap can be supplied to <code>fstprint</code> to convert from integer mapped strings to English: </p>
<pre class="fragment">&gt; echo `zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint --isymbols=wmaps/wmt13.en.wmap | awk '{print $3}'`
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;

&gt; echo `zcat output/exp.baseline/LATS/2.fst.gz | fstshortestpath | fsttopsort | fstprint --isymbols=wmaps/wmt13.en.wmap | awk '{print $3}'`
&lt;s&gt; amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt;
</pre><p>For convenience, the HiFST <code>printstring</code> utility programme gathers all these operations into a single binary: </p>
<pre class="fragment">&gt; printstrings.${TGTBINMK}.bin --range=1:2 --label-map=wmaps/wmt13.en.wmap --input=output/exp.baseline/LATS/?.fst.gz --semiring=lexstdarc         
...
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt; 
...
</pre><p><b>Note</b> the use of the <code>range=1:2</code> command line option, which specifies which fsts are to be processed, and also that wordmap file is loaded only once, which can speed operations. <b>Note</b> also that the output fsts are in the <code>lexstdarc</code> semiring, described later.</p>
<h2><a class="anchor" id="fst_shortestpath_discuss"></a>
OpenFst ShortestPath Operations</h2>
<p>The above <a class="el" href="intro.html#OpenFst">OpenFst</a> operations do the following:</p>
<ol type="1">
<li><code>zcat</code> pipes the HiFST output lattice to the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/ShortestPathDoc">Shortest Path</a> tool which produces an FST containing only the shortest path in the lattice</li>
<li>the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/TopSortDoc">Topological Sort</a> operation renumbers the state IDs so that all arcs are links from lower to higher state IDs</li>
<li>the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/FstQuickTour#Printing_Drawing_and_Summarizing">fstprint</a> operation reads the English wordmap, for the arc input symbols, and traverses the input fst, writing each arc as it is encountered; TopSort ensures these arcs are written in the correct order</li>
<li>the awk operation prints only the words on the arcs</li>
<li>wrapping everything inside echo generates a single string</li>
</ol>
<p>To see what is produced at the various steps in the pipeline:</p>
<p>Input lattice: </p>
<pre class="fragment"> &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstinfo | head -6
 fst type                                          vector
 arc type                                          tropical_LT_tropical
 input symbol table                                none
 output symbol table                               none
 # of states                                       656
 # of arcs                                         1513
</pre><p>Shortest Path: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fstprint
16     15      1 1 -2.68554688,-2.68554688
0
1      0       2       2        2.83719015,-2.34277344
2      1       23899   23899    10.2336502,0
3      2       6       6        1.83065951,0
4      3       425     425      4.19441986,0
5      4       3       3        -3.28652811,-5.1171875
6      5       48      48       3.01827216,0
7      6       1145    1145     4.64596272,0
8      7       25      25       1.83065951,0
9      8       4       4        0.548432946,-1.28222656
10     9       245     245      5.13327551,-0.48828125
11     10      3       3        0.961314559,0
12     11      103     103      4.64596272,0
13     12      20      20       0.961314559,0
14     13      135     135      1.43660688,-4.65039062
15     14      50      50       6.39422989,-2.88476562
</pre><p><b>Note</b> that the paired weights are described in <a class="el" href="basictrans.html#basic_scores">Lexicographic Semirings: Translation Grammar and Language Model Scores</a>.</p>
<p>Topologically Sorted Shortest Path: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint
1      1       1       1     -2.68554688,-2.68554688
1      2       50      50    6.39422989,-2.88476562
2      3       135     135   1.43660688,-4.65039062
3      4       20      20    0.961314559,0
4      5       103     103   4.64596272,0
5      6       3       3     0.961314559,0
6      7       245     245   5.13327551,-0.48828125
7      8       4       4     0.548432946,-1.28222656
8      9       25      25    1.83065951,0
9      10      1145    1145  4.64596272,0
10     11      48      48    3.01827216,0
11     12      3       3     -3.28652811,-5.1171875
12     13      425     425   4.19441986,0
13     14      6       6     1.83065951,0
14     15      23899   23899 10.2336502,0
15     16      2       2     2.83719015,-2.34277344
16
</pre><p>Toplogically Sorted Shortest Path, with English words replacing the arc input symbols </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint --isymbols=wmaps/wmt13.en.wmap --acceptor
0      1    &lt;s&gt;         -2.68554688,-2.68554688
1      2    parliament  6.39422989,-2.88476562
2      3    does        1.43660688,-4.65039062
3      4    not         0.961314559,0
4      5    support     4.64596272,0
5      6    the         0.961314559,0
6      7    amendment   5.13327551,-0.48828125
7      8    ,           0.548432946,-1.28222656
8      9    which       1.83065951,0
9      10   gives       4.64596272,0
10     11   you         3.01827216,0
11     12   the         -3.28652811,-5.1171875
12     13   freedom     4.19441986,0
13     14   of          1.83065951,0
14     15   tymoshenko  10.2336502,0
15     16   &lt;/s&gt;        2.83719015,-2.34277344
16
</pre><h1><a class="anchor" id="basic_nbest"></a>
Extracting N-Best Translations from Lattices</h1>
<p>The <code>printstrings</code> can also can print the top-N hypotheses, using the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/ShortestPathDoc">Shortest Path</a> operation, with its n-shortest path option: </p>
<pre class="fragment">&gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --nbest=10 --unique --input=output/exp.baseline/LATS/1.fst.gz 
...
1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 
1 50 135 20 103 34 245 4 25 1145 48 3 425 6 23899 2 
1 3 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 
1 50 311 20 103 3 245 4 25 1145 48 3 425 6 23899 2 
1 50 135 20 103 3 245 4 25 1145 48 3 425 7 23899 2 
1 50 135 20 103 3 245 4 25 1145 48 425 6 23899 2 
1 50 135 20 103 3 245 4 25 1145 425 6 23899 2 
1 50 135 20 103 3 245 4 25 1145 408 23899 2 
1 50 135 20 103 3 245 4 25 1145 3 425 6 23899 2 
1 50 135 20 103 3 245 4 25 1145 48 408 23899 2 
...
</pre><p>With the English wordmap, <code>printstrings</code> will map the integer representation to English text: </p>
<pre class="fragment">&gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --nbest=10 --unique --input=output/exp.baseline/LATS/1.fst.gz --label-map=wmaps/wmt13.en.wmap
...
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support an amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament did not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom to tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives free tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you free tymoshenko &lt;/s&gt; 
...
</pre><p>It often happens that a translation hypothesis can be produced by multiple derivations (i.e. rule sequences), so the top scoring hypotheses need not be unique. For example, omitting the <code>--unique</code> shows repetitions among the top hypotheses: </p>
<pre class="fragment">&gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --nbest=10 --input=output/exp.baseline/LATS/1.fst.gz --label-map=wmaps/wmt13.en.wmap
...
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 
...
</pre><h1><a class="anchor" id="vector_feature_grammars"></a>
Weight Vectors and Feature Vectors</h1>
<p>HiFST computes a translation score <code>S(e)</code> for each hypothesis <code>e</code> by applying a weight vector <code>P</code> to a feature vector <code>F(e)</code>. The relationship of feature vectors and scores at the hypothesis level is as follows:</p>
<ul>
<li>Suppose there are m language models, with weights s_1 ...,s_m .<ul>
<li>These weights are specified by the HiFST parameters <code>lm.featureweights=s_1,s_2,..,s_m</code></li>
</ul>
</li>
<li>Suppose there are n-dimensional feature vectors for each rule in the translation grammar,<ul>
<li>The weights to be applied are specified by the HiFST parameters <code>grammar.featureweights=w_1,..,w_n</code></li>
</ul>
</li>
<li>A feature weight vector is formed as P = [s_1 ... s_m w_1 ... w_n]</li>
<li>A translation hypothesis e has a feature vector F(e) = [lm_1(e) ... lm_m(e) f_1(e) ... f_n(e)]<ul>
<li>lm_i(e): the i-th language model score for e</li>
<li>f_j(e): j-th grammar feature (see Section 3.2.1, [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>])</li>
</ul>
</li>
<li>The score of translation hypothesis e can be found as S(e) = F(e) . P (dot product)</li>
</ul>
<h1><a class="anchor" id="weight_feature_vector_examples"></a>
Applying Weight Vectors in Translation</h1>
<p>As an example, the translation grammar <code>G/rules.shallow.vecfea.gz</code> has unweighted 11-dimensional (<code>n=11</code>) feature vectors associated with each rule: </p>
<pre class="fragment">&gt; gzip -d -c G/rules.shallow.vecfea.gz | head -3
V 3 4 0.223527 0.116794 -1 -1 0 0 0 0 -1 1.268789 0.687159
V 3 4_3 3.333756 0.338107 -2 -1 0 0 0 0 -1 1.662178 3.363062
V 3 8 3.74095 3.279819 -1 -1 0 0 0 0 -1 3.741382 2.271445
</pre><p>We have run lattice <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a> (<a class="el" href="basictrans.html#lmert">Lattice MERT</a>) to generate a parameter weight vector for this grammar with these features and the language model <code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap</code> </p>
<pre class="fragment">&gt; P=1.0,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
</pre><p>Note that <code>P</code> is 12-dimensional: the weighting of the language model score is <code>1.0</code>, and the weights applied to the grammar feature vectors are </p>
<pre class="fragment">GW=0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
</pre><p>There are (at least) three different ways to apply the feature weights to the unweighted feature vectors in the grammar:</p>
<p><b>Feature weights can be applied to the grammar, prior to translation</b>: </p>
<pre class="fragment">&gt; mkdir -p tmp/
&gt; gzip -dc G/rules.shallow.vecfea.gz | ./scripts/weightgrammar -w=$GW | gzip  &gt; tmp/rules.shallow.RS.gz
&gt; hifst.${TGTBINMK}.bin --config=configs/CF.nogrammar --grammar.load=tmp/rules.shallow.RS.gz --target.store=tmp/hyps.1
</pre><p><b>Feature weights can be applied in translation</b>, using the options <code>--grammar.featureweights=$GW</code> and <code>--lm.featureweights=1.0</code>.</p>
<p>HiFST loads the grammar with unweighted feature vectors, and applies the feature weights on-the-fly: </p>
<pre class="fragment">&gt; hifst.${TGTBINMK}.bin --config=configs/CF.nogrammar --grammar.load=G/rules.shallow.vecfea.gz --grammar.featureweights=$GW --lm.featureweights=1.0 --target.store=tmp/hyps.2 
</pre><p><b>Feature weights can be applied in translation</b>, using the <code>--featureweight=$P</code> option.</p>
<p>HiFST loads the grammar with unweighted feature vectors, and applies the feature weights on-the-fly. HiFST automatically determines which elements of <code>P</code> should be applied to the language model scores, and which should be applied to the unweighted feature vectors in the translation grammar: </p>
<pre class="fragment">&gt; hifst.${TGTBINMK}.bin --featureweights=$P --config=configs/CF.nogrammar --target.store=tmp/hyps.3 --grammar.load=G/rules.shallow.vecfea.gz
</pre><p>These three alternative methods should yield identical results (to verify, compare <code>tmp/hyps.[123]</code>). Note that the last alternative is useful for iterative parameter estimation procedures, such as <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a>.</p>
<h1><a class="anchor" id="basic_scores"></a>
Lexicographic Semirings: Translation Grammar and Language Model Scores</h1>
<p>HiFST follows the formalism in which rule probabilities are represented as arc weights (see Section 2 of [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>]). A rule with probability <em>p</em> is represented as a negative log probability, i.e. </p>
<pre class="fragment"> X -&gt; &lt; A , B &gt; / - log(p)
</pre><p>with n-gram language model scores encoded similarly, as costs -log P(w|h) for word <em>w</em> with LM history <em>h</em>. Costs are accumulated at the path level, so that the shortest path through the output FSA accepts the highest scoring hypothesis under the translation grammar and the language model (with feature weights applied as described in <a class="el" href="basictrans.html#vector_feature_grammars">Weight Vectors and Feature Vectors</a>). With this mapping of scores to costs, the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/ShortestPathDoc">ShortestPath</a> can be used to extract the best scoring hypothesis under the tropical semiring.</p>
<p>HiFST uses a <b>lexicographic semiring of two tropical weights</b> [<a class="el" href="intro.html#Roark2011">Roark2011</a>]. For example, </p>
<pre class="fragment"> &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstinfo | head -2
 fst type                                          vector
 arc type                                          tropical_LT_tropical

 &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstprint | head -n 10
 0 1    1       1   -2.68554688,-2.68554688
 1 4    170     170 11.4819975,4.25097656
 1 3    50      50  6.39422989,-2.88476562
 1 2    3       3   1.87243128,-2.70800781
 1 197  50      50  3.70087051,-5.578125
 1 196  3       3   -0.820928097,-5.40136719
 1 456  50      50  7.04462051,-2.234375
 1 455  3       3   1.72790003,-2.85253906
 1 195  50      50  6.01825333,-3.26074219
 1 81   170     170 5.89898968,-1.33203125
</pre><p>The <code>arc type</code> of <code>tropical_LT_tropical</code> indicates that the lexicographic semiring is a pair of tropical weights. As produced by HiFST, the pairs of tropical weights are (<code>G+M</code>, <code>G</code>), where the first weight (<code>G+M</code>) contains the complete translation score (the translation grammar score <code>G</code> + the language model score <code>M</code>), and the second weight <code>G</code> only contains the translation grammar score. The advantage of using the lexicographic semiring in this way is that the language model scores can be removed and reapplied very efficiently (see <a class="el" href="basictrans.html#rescoring_lm">Language Model Rescoring</a>).</p>
<p>In the above example, scores are distributed over the arcs in the FST. The <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/PushDoc">Push</a> operation can be used to accumulate weights at the path level within the shortest path fst: </p>
<pre class="fragment"> &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstpush --push_weights --to_final | fstprint --isymbols=wmaps/wmt13.en.wmap --acceptor
 0      1    &lt;s&gt;
 1      2    parliament
 2      3    does
 3      4    not
 4      5    support
 5      6    the
 6      7    amendment
 7      8    ,
 8      9    which
 9      10   gives
 10     11   you
 11     12   the
 12     13   freedom
 13     14   of
 14     15   tymoshenko
 15     16   &lt;/s&gt;
 16     42.6998749,-19.4511719
</pre><p>In this example, <code>(G+M, G)</code> is <code>(42.6998749,-19.4511719)</code>. The first component, 42.6998749, contains the sum of the translation grammar scores and the language model - this is the score assigned to the hypotheses by the decoder. The second component, <code>-19.4511719</code>, is the translation grammar score alone, i.e. contains the score assigned to the hypotheses under the translation grammar without the language model (see Section 5.1 of [<a class="el" href="intro.html#Allauzen2014">Allauzen2014</a>]). The lexicographic semiring is such that these scores are computed correctly at the path level:</p>
<ul>
<li>The cost of the shortest path found by <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ShortestPathDoc">ShortestPath</a> is that of the best hypothesis under the sum of the translation grammar score and the language model score(s)</li>
<li>In the lexicographic semiring, when the path weight is pushed to the final state:<ul>
<li>the first weight component is the correct combined translation grammar and language model score</li>
<li>the second weight component is the best grammar score over all possible derivations that could have generated this hypothesis</li>
</ul>
</li>
</ul>
<p>The HiFST utility <code>printstrings</code> also works with the lexicographic semiring, and gives the same results as using the ShortestPath and Push operations: </p>
<pre class="fragment">&gt; printstrings.${TGTBINMK}.bin --input=output/exp.baseline/LATS/1.fst.gz --label-map=wmaps/wmt13.en.wmap --semiring=lexstdarc --weight
...
</pre><p> &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 42.6999,-19.4512 ...</p>
<p>Printing the top 5 hypotheses shows that hypotheses are scored and ranked under the combined grammar and language model score <code>G+M</code>: </p>
<pre class="fragment">&gt; printstrings.${TGTBINMK}.bin --input=output/exp.baseline/LATS/1.fst.gz --label-map=wmaps/wmt13.en.wmap --semiring=lexstdarc --weight --nbest=5 --unique
...
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;  42.7003,-19.4512
&lt;s&gt; parliament does not support an amendment , which gives you the freedom of tymoshenko &lt;/s&gt;   44.2802,-20.8945
&lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      45.4819,-19.9385
&lt;s&gt; parliament did not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;   45.7096,-15.5635
&lt;s&gt; parliament does not support the amendment , which gives you the freedom to tymoshenko &lt;/s&gt;  45.8136,-19.0361
</pre><h1><a class="anchor" id="basic_toplevelpruning"></a>
Admissible Pruning</h1>
<p>HiFST can prune translation lattices prior to saving them to disk; this is most often a practical necessity. Pruning is done using the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/PruneDoc">Prune</a> operation. Pruning in this case is <b>admissible</b>, since it is performed after grammar and language model scores have been completely applied. Low-scoring hypotheses are discarded, but no search errors are introduced by this pruning. This is also referred to as <b>top-level pruning</b>, as described in detail in Section 2.2.2, [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>].</p>
<p>Top-level pruning is controlled by the <code>--hifst.prune</code> option. In the previous examples, <code>--hifst.prune</code> was set to 9. If we use the default (3.40282347e+38), then the output lattice size becomes very large. For example, compare lattices in <code>exp1/</code> generated with <code>prune=9</code> vs. unpruned lattices in <code>exp2/</code>: </p>
<pre class="fragment">&gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline.outputnoprune &amp;&gt; log/log.baseline.outputnoprune
&gt; du -sh output/exp.baseline/LATS/1.fst.gz output/exp.baseline.outputnoprune/LATS/1.fst.gz
12K    output/exp.baseline/LATS/1.fst.gz
1.1M   output/exp.baseline.outputnoprune/LATS/1.fst.gz
&gt; du -sh output/exp.baseline/LATS/2.fst.gz output/exp.baseline.outputnoprune/LATS/2.fst.gz
148K   output/exp.baseline/LATS/2.fst.gz
16M    output/exp.baseline.outputnoprune/LATS/2.fst.gz
</pre><p>The <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/FstQuickTour#Printing_Drawing_and_Summarizing">fstinfo</a> command also indicates much larger outputs: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/2.fst.gz | fstinfo | grep \# | head -2
# of states                                       5833
# of arcs                                         22845

&gt; zcat output/exp.baseline.outputnoprune/LATS/2.fst.gz | fstinfo | grep \# | head -2
# of states                                       172843
# of arcs                                         2481006
</pre><p>The unpruned lattices are much bigger, and contain many translation hypotheses, although the top scoring hypotheses should be unchanged by this form of pruning, as is the case in this example: </p>
<pre class="fragment">&gt; head output/exp.baseline/hyps output/exp.baseline.outputnoprune/hyps
==&gt; output/exp.baseline/hyps &lt;==
1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 
1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2 

==&gt; output/exp.baseline.outputnoprune/hyps &lt;==
1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 
1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2 
</pre><h1><a class="anchor" id="lpruning"></a>
Inadmissible Pruning</h1>
<p>Inadmissible pruning, or <b>local pruning</b>, controls processing speed and memory use during translation. Only enough details are reviewed here to describe how HiFST performs pruning in search; for a detailed discussion of local pruning and pruning in search, see Section 2.2.2 of [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>].</p>
<p>Given a translation grammar and a source language sentence, HiFST first constructs a Recursive Transition Network (RTN) representing the translation hypotheses [<a class="el" href="intro.html#Iglesias2009a">Iglesias2009a</a>, <a class="el" href="intro.html#Iglesias2011">Iglesias2011</a>]. This is done as part of a modified CYK algorithm used to parse the source sentence under the translation grammar. The RTN is then <em>expanded</em> to an equivalent WFSA via the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation. This WFSA contains the translation hypotheses along with their scores under the translation grammar. We refer to this as the `top-level' WFSA, because it is associated with the top-most cell in the CYK grid. This top-level WFSA can be pruned after composition with the language model, as described in the discussion of <a class="el" href="basictrans.html#basic_toplevelpruning">Admissible Pruning</a>. We refer to this as <em>exact search</em> or <em>exact translation</em>. In exact translation, no translation hypotheses are discarded prior to applying the complete grammar and language model scores.</p>
<p>Exact translation can be done under some combinations of translation grammars, language models, and language pairs. In particular, the <a class="el" href="tutorial_.html#tgrammars_shallow">Shallow-N Translation Grammars</a> were designed for exact search. However attempting exact translation under many translation grammars would cause either the <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation or the subsequent language model composition to become computationally intractable. We therefore have developed a pruning strategy that prunes the RTN during its construction.</p>
<p>The RTN created by HiFST can be described as follows:</p>
<ul>
<li><img class="formulaInl" alt="$X$" src="form_0.png"/> is the set of non-terminals in the translation grammar, with <img class="formulaInl" alt="$S$" src="form_1.png"/> as the root</li>
<li><img class="formulaInl" alt="$\Sigma$" src="form_2.png"/> is the target language vocabulary, i.e. the terminals in the target language</li>
<li><img class="formulaInl" alt="$I$" src="form_3.png"/> is the length of the source sentence <img class="formulaInl" alt="$s$" src="form_4.png"/>, i.e. <img class="formulaInl" alt="$s = s_0...s_{I - 1}$" src="form_5.png"/></li>
<li>A new set of non-terminals is defined as <img class="formulaInl" alt="$N = \{ (x,i,j) : x \in X , 0 <= i <= j < I \}$" src="form_6.png"/><ul>
<li>Note that <img class="formulaInl" alt="$(S,0,I-1) \in N$" src="form_7.png"/></li>
</ul>
</li>
<li><img class="formulaInl" alt="$(T_u)_{u \in N}$" src="form_8.png"/>, is a family of WFSAs with input alphabet <img class="formulaInl" alt="$\Sigma \cup N$" src="form_9.png"/><ul>
<li>Each <img class="formulaInl" alt="$T_u$" src="form_10.png"/> with <img class="formulaInl" alt="$u = (x, i, j)$" src="form_11.png"/>, is a WFSA that describes all applications of translation rules with left-hand side non-terminal <img class="formulaInl" alt="$x$" src="form_12.png"/> that span the substring <img class="formulaInl" alt="$s_i ... s_j$" src="form_13.png"/></li>
<li><img class="formulaInl" alt="$T_u$" src="form_10.png"/> is associated with the CYK grid cell associated with source space <img class="formulaInl" alt="$[i,j]$" src="form_14.png"/> and headed by non-terminal <img class="formulaInl" alt="$x$" src="form_12.png"/></li>
</ul>
</li>
<li>The top-level RTN is defined as <img class="formulaInl" alt="$R_{(S,0,I-1)} = (N, \Sigma, (T_u)_{u \in N}, (S,0,I-1))$" src="form_15.png"/>.<ul>
<li>The root symbol of this RTN is <img class="formulaInl" alt="$(S,0,I-1)$" src="form_16.png"/>.</li>
<li>The WFSA <img class="formulaInl" alt="$T_{(S,0,I-1)}$" src="form_17.png"/> represents all applications of translation rules that span the entire sentence and are rooted with non-terminal <img class="formulaInl" alt="$S$" src="form_1.png"/>.</li>
</ul>
</li>
</ul>
<p>Exact translation is achieved if every <img class="formulaInl" alt="$T_u$" src="form_10.png"/> is complete (i.e. if no pruning is done) prior to the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://openfst.org/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation on the RTN <img class="formulaInl" alt="$R_{(S,0,I-1)}$" src="form_18.png"/>. This produces a WFSA that contains all translations that can be produced under the translation grammar.</p>
<p>The RTN pruning strategy relies on noting that each of the WFSAs <img class="formulaInl" alt="$T_{u'}$" src="form_19.png"/>, <img class="formulaInl" alt="$u' = (x', i', j')$" src="form_20.png"/>, also defines an RTN <img class="formulaInl" alt="$R_{u'}$" src="form_21.png"/>, as follows:</p>
<ul>
<li>Define a subset of non-terminals <img class="formulaInl" alt="$N' = \{ (x,i,j) : x \in X , i' <= i <=j < j' \}$" src="form_22.png"/> , i.e. <img class="formulaInl" alt="$N' \subset N$" src="form_23.png"/></li>
<li><img class="formulaInl" alt="$R_{u'} = (N', (T_u)_{u \in N'}, (x', i', j') )$" src="form_24.png"/><ul>
<li>The root symbol of this RTN is <img class="formulaInl" alt="$(x', i', j')$" src="form_25.png"/></li>
</ul>
</li>
</ul>
<p>The <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation can be applied to the RTNs <img class="formulaInl" alt="$R_{u'}$" src="form_21.png"/> to produce an WFSA containing all translations of the source string <img class="formulaInl" alt="$S_{i'} ... S_{j'}$" src="form_26.png"/> using derivations rooted in the non-terminal <img class="formulaInl" alt="$x'$" src="form_27.png"/>. This WFSA can be pruned and used in place of the original <img class="formulaInl" alt="$T_{u'}$" src="form_19.png"/>.</p>
<p>Because of the possibility of search errors we refer to this as 'local pruning' or inadmissible pruning. There is the possibility that pruning any of the <img class="formulaInl" alt="$T_u$" src="form_10.png"/> may possibly cause some good translations to be discarded. For this reason it is important to tune the pruning strategy for the translation grammar and language model. Once pruning has been set, the benefits are</p>
<ul>
<li>faster creation of the top-level WFSA via the <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> operation</li>
<li>faster composition of the translation WFSA with the language model</li>
<li>less memory used in RTN construction and language model composition</li>
</ul>
<p>Local pruning should be done under the combined grammar and the language model scores, rather than under the translation grammar scores alone alone. However, the LM used in local pruning can be relatively weak. For example, if the main language model used in translation is a 4-gram, perhaps a 3-gram or even a bigram language model could be used in local pruning. Using a smaller language model will make pruning faster, as will an efficient scheme to remove the scores of the language models used in pruning. The lexicographic semiring, see <a class="el" href="basictrans.html#basic_scores">Lexicographic Semirings: Translation Grammar and Language Model Scores</a>, makes this last operation easy.</p>
<h2><a class="anchor" id="local_prune"></a>
Local Pruning Algorithm</h2>
<p>HiFST monitors the size of the <img class="formulaInl" alt="$T_u$" src="form_10.png"/> during translation. Any of these automata that exceed specified thresholds are converted to WFSAs and pruned. Subsequent expansion of the RTN <img class="formulaInl" alt="$R_{(S,0,I-1)}$" src="form_18.png"/> is then done with respect to the pruned versions of <img class="formulaInl" alt="$T_u$" src="form_10.png"/>.</p>
<p>Local pruning is controlled via the following HiFST parameters: </p>
<pre class="fragment">hifst.localprune.enable=yes # must be set to activate local pruning
hifst.localprune.conditions=NT_1,span_1,size_1,threshold_1,...,NT_N,span_N,size_N,threshold_N
hifst.localprune.lm.load=lm_1,...lm_K
hifst.localprune.lm.featureweights=scale_1,...,scale_K
hifst.localprune.lm.wps=wp_1,...,wp_K
</pre><p>In the above, an arbitrary number N of tuples (<code>NT_n</code>, <code>span_n</code>, <code>size_n</code>, <code>threshold_n</code>) can be provided; similarly, an arbitrary number K of language model parameters (<code>lm_k</code>, <code>scale_k</code>, <code>wp_k</code>) can also be used in pruning.</p>
<p>Pruning is applied during construction of the RTN, as follows:</p>
<ul>
<li>If any <img class="formulaInl" alt="$T_u$" src="form_10.png"/> satisfies the following conditions for any parameter set (<code>NT_n</code>, <code>span_n</code>, <code>size_n</code>, <code>threshold_n</code>), n=1,...,N<ul>
<li>NT_n = X</li>
<li>span_n &lt;= j-i</li>
<li>size_n &lt;= number of states of <img class="formulaInl" alt="$T_u$" src="form_10.png"/>, computed via <a class="el" href="intro.html#OpenFst">OpenFst</a> <code>NumStates()</code></li>
</ul>
</li>
<li>then <img class="formulaInl" alt="$T_u$" src="form_10.png"/> is pruned as follows:<ul>
<li>OpenFst <a href="http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc">Replace</a> converts <img class="formulaInl" alt="$R_u$" src="form_28.png"/> to a WFSA</li>
<li><a href="http://www.openfst.org/twiki/bin/view/FST/RmEpsilonDoc">RmEpsilon</a>,<a href="http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc">Deteminize</a>, and <a href="http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc">Minimize</a> generate a compacted WFSA</li>
<li><a href="http://www.openfst.org/twiki/bin/view/FST/ComposeDoc">Composition</a> with K language model(s) WFSAs<ul>
<li>The parameters (<code>lm_k</code>, <code>scale_k</code>, <code>wp_k</code>) specify the language models, language model scale factors, and word penalties to be applied</li>
</ul>
</li>
<li>OpenFst <a href="http://www.openfst.org/twiki/bin/view/FST/PruneDoc">Prune</a> is applied with threshold <code>threshold_n</code></li>
<li>Language model scores are removed by copying component weights in the lexicographic semiring, see <a class="el" href="basictrans.html#basic_scores">Lexicographic Semirings: Translation Grammar and Language Model Scores</a></li>
<li><a href="http://www.openfst.org/twiki/bin/view/FST/RmEpsilonDoc">RmEpsilon</a>,<a href="http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc">Deteminize</a>, and <a href="http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc">Minimize</a>, yielding a pruned WFSA <img class="formulaInl" alt="$T_u$" src="form_10.png"/> with only translation scores and target language symbols</li>
</ul>
</li>
<li>The pruned version of <img class="formulaInl" alt="$T_u$" src="form_10.png"/> is then used in place of the original version in the RTN</li>
</ul>
<h2><a class="anchor" id="lpruning_effects"></a>
Effect on Speed, Memory, Scores</h2>
<p>Pruning in search is particularly important when running HiFST with grammars that are more powerful than the shallow grammar used in these examples. However, we can still see speed and memory use improvements through local pruning, particularly with long sentences.</p>
<p>First, pick some long source language sentences to translate: </p>
<pre class="fragment"> &gt; awk 'NF&gt;80' RU/RU.tune.idx  &gt; tmp/RU.long.idx # should be 3 sentences
</pre><p>Translate these long sentences under the baseline Shallow-1 grammar; note this will overwrite the output in <code>output/exp.baseline/</code> </p>
<pre class="fragment"> &gt; (time hifst.${TGTBINMK}.bin --config=configs/CF.baseline --range=1:3 --source.load=tmp/RU.long.idx ) &amp;&gt; log/log.long
</pre><p>The maximum memory use is approximately 2.5GB and translation takes approximately 1m30s. (The resource consumption may vary depending on your hardware, we provide these numbers to illustrate the effect of local pruning.)</p>
<p>If translation is performed with the same grammar and language model, but with aggressive local pruning, </p>
<pre class="fragment"> &gt; (time hifst.${TGTBINMK}.bin --config=configs/CF.baseline.localprune) &amp;&gt; log/log.baseline.localprune
</pre><p>then the memory consumption is reduced to under 2.2GB and the processing time to approximately 40s. Note that local pruning does not have as great an effect on translation under the Shallow-1 grammar as it would with e.g. a full Hiero grammar.</p>
<p>Inspecting the log file indicates that local pruning was applied extensively in translation:</p>
<pre class="fragment"> &gt; grep pruning log/log.baseline.localprune
 Mon Apr  6 19:06:46 2015: run.INF:Stats for Sentence 1: local pruning, number of times=266
 Mon Apr  6 19:07:01 2015: run.INF:Stats for Sentence 2: local pruning, number of times=279
 Mon Apr  6 19:07:16 2015: run.INF:Stats for Sentence 3: local pruning, number of times=154
</pre><p>Pruning is aggressive enough here that the translations are affected: </p>
<pre class="fragment"> &gt; printstrings.${TGTBINMK}.bin -w --semiring=lexstdarc -m wmaps/wmt13.en.wmap --input=output/exp.baseline/LATS/1.fst.gz
 &lt;s&gt; the decline in economic activity caused , until november , the permanent rise in unemployment in the state , according to the national institute of statistics and geography , in the first three quarters was recorded accelerated growth in unemployment , from january to march 55.053 resident sinaloa were unemployed , with the share of per cent of the economically active population , in the second quarter , the share rose to percent from july to september , she continued to rise to percent share , if calculated per number of people is more than unemployed people in sinaloa , the 18.969 people more than in the first half . &lt;/s&gt;        471.123,2.46484

 &gt; printstrings.${TGTBINMK}.bin -w --semiring=lexstdarc -m wmaps/wmt13.en.wmap --input=output/exp.baseline.localprune/LATS/1.fst.gz
 &lt;s&gt; the decline in economic activity caused , until november , the permanent rise in unemployment in the state , according to the national institute of statistics and geography , in the first three quarters was recorded accelerated growth in unemployment , from january to march 55.053 resident sinaloa were unemployed , with the share of per cent of the economically active population , in the second quarter share rose to percent from july to september , she continued to rise to percent stake , which is recalculated the number of people is more than unemployed people in sinaloa , the 18.969 people more than in the first half . &lt;/s&gt;      472.86,3.74023
</pre><p>The best hypothesis generated under the baseline system without local pruning has a combined grammar and language model score of <code>471.123</code>. This hypothesis does not survive more aggressive local pruning under these parameters , where the best hypothesis has a higher combined score of <code>472.86</code>.</p>
<h1><a class="anchor" id="rescoring_lm"></a>
Language Model Rescoring</h1>
<p>As discussed above, HiFST uses a lexicographic semiring (<a class="el" href="basictrans.html#basic_scores">Lexicographic Semirings: Translation Grammar and Language Model Scores</a>, [<a class="el" href="intro.html#Roark2011">Roark2011</a>]) of two tropical weights. In each arc of a lattice generated by HiFST, the first weight <code>(G+M)</code> contains the correct score (translation grammar score + language model score). The second weight <code>G</code> only contains the translation grammar score. An example is repeated here: </p>
<pre class="fragment"># re-run the translation baseline
&gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline &amp;&gt; log/log.baseline

&gt; zcat output/exp.baseline/LATS/1.fst.gz | fsttopsort | fstprint | head -n 10
0     1     1     1   -2.68554688,-2.68554688
1   319   170   170   11.4819975,4.25097656
1   118    50    50   6.39422989,-2.88476562
1   104     3     3   1.87243128,-2.70800781
1    91    50    50   3.70087051,-5.578125
1    87     3     3   -0.820928097,-5.40136719
1    65    50    50   7.04462051,-2.234375
1    62     3     3   1.72790003,-2.85253906
1    47    50    50   6.01825333,-3.26074219
1    43   170   170   5.89898968,-1.33203125
</pre><p>The advantage of using the lexicographic semiring to represent <code>(G+M,G)</code> weights is that the language model score can be removed very efficiently: the second field is simply copied over the first field. HiFST binaries do this mapping internally with an <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://www.openfst.org/twiki/bin/view/FST/ArcMapDoc">fstmap</a> operation. The result is a WFSA whose weights contain only the translation grammar scores. The lexmap tool can be used to do this mapping, as follows, yielding lexicographic weights <code>(G,G)</code>: </p>
<pre class="fragment">&gt; zcat output/exp.baseline/LATS/1.fst.gz | lexmap.${TGTBINMK}.bin | fsttopsort | fstprint | head -n 10
0     1     1     1   -2.68554688,-2.68554688
1   319   170   170   4.25097656,4.25097656
1   118    50    50   -2.88476562,-2.88476562
1   104     3     3   -2.70800781,-2.70800781
1    91    50    50   -5.578125,-5.578125
1    87     3     3   -5.40136719,-5.40136719
1    65    50    50   -2.234375,-2.234375
1    62     3     3   -2.85253906,-2.85253906
1    47    50    50   -3.26074219,-3.26074219
1    43   170   170   -1.33203125,-1.33203125
</pre><p>Using this facility to remove language model scores, the HiFST <code>applylm</code> tool can be used to rescore lattices under a different language model than was used in first-pass translation. Operations are as follows:</p>
<ol type="1">
<li>A lattice with lexicographic <code>(G+M,G)</code> weights is loaded</li>
<li>Weights are converted to <code>(G,G)</code> via <a href="http://www.openfst.org/twiki/bin/view/FST/ArcMapDoc">fstmap</a></li>
<li>The new language model(s) are applied via composition under the lexicographic semiring, with optional scale factors and word insertion penalties. The new WFSAs weights are of the form <code>(G+M2,G)</code>, where <code>M2</code> are the new language model weights.</li>
<li>The reweighted WFSA is written to disk, with either lexicographic or standard tropical weights</li>
</ol>
<p>The following example uses applylm to rescore lattices generated with almost no pruning (<code>output/exp.baseline.outputnoprune/LATS</code>). Rescoring uses the same 4-gram language model originally used to generate the lattice, but with a different scale factor (<code>lm.scale=0.9</code>). </p>
<pre class="fragment"> &gt; applylm.${TGTBINMK}.bin --config=configs/CF.baseline.outputnoprune.lmrescore  &amp;&gt; log/log.lmrescore
</pre><p>For the second sentence, the original 1-best hypothesis was: </p>
<pre class="fragment"> &gt; zcat output/exp.baseline.outputnoprune/LATS/2.fst.gz | printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
 &lt;s&gt; amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt;     101.582,-37.5605
</pre><p>Rescoring yields a slightly different 1-best: </p>
<pre class="fragment"> &gt; zcat output/exp.baseline.lmrescore/LATS/2.fst.gz | printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
 &lt;s&gt; amendment , which would have led to the release of which is in jail of former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt;  87.5249,-39.5508
</pre><p>Note the <code>load.deletelmcost</code> option in the configuration file, which instructs the tool to subtract old lm scores first. If the scaling is not changed, both lattices should be identical </p>
<pre class="fragment">(`applylm.${TGTBINMK}.bin --config=configs/CF.baseline.outputnoprune.lmrescore --lm.featureweights=1`).
</pre><h1><a class="anchor" id="multithread"></a>
Multithreading</h1>
<p><b>Note</b> that the timing results here are illustrative only.</p>
<p>HiFST uses <a href="http://www.boost.org/doc/libs/1_38_0/doc/html/thread.html">Boost.Thread</a> to enable multithreading. This is disabled by default, but can enabled using the flag <code>--nthreads=N</code> . If set, each source language sentence is translated simultaneously on its own thread (trimmed to the number of CPUs available). The translation grammar and language model are kept in shared memory.</p>
<p>To see the effects of multithreading on speed and memory use, the baseline configuration is run over the first twenty sentences without multithreading: </p>
<pre class="fragment"> &gt; time hifst.${TGTBINMK}.bin --config=configs/CF.baseline --range=1:20
</pre><p>Processing time is 105 seconds and maximum memory use is about 2GB. In the same decoder configuration but with 2 threads </p>
<pre class="fragment"> &gt; time hifst.${TGTBINMK}.bin --config=configs/CF.baseline --range=1:20 --nthreads=2
</pre><p>processing time is reduced to 60 seconds with maximum memory use of about 2.5GB.</p>
<p>In these examples, both the LM and translation grammar are relatively small, and so there is not a great deal of gain from keeping them in shared memory. But in larger tasks, multithreading can be a significant advantage.</p>
<h1><a class="anchor" id="lmbr"></a>
Lattice Minimum Bayes Risk Decoding</h1>
<p>For a detailed discussion of LMBR, see Chapters 7 and 8 in [<a class="el" href="intro.html#BlackwoodPhD">BlackwoodPhD</a>].</p>
<p>LMBR is a decoding procedure, based on the following:</p>
<ul>
<li>Evidence space: a lattice (WFSA) containing weighted translations produced by the SMT system.<ul>
<li>N-gram posterior distributions, with pathwise posteriors, are extracted from this WFSA.</li>
</ul>
</li>
<li>The hypotheses space: an unweighted lattice (FSA) containing hypotheses to be rescored.</li>
</ul>
<p>The following steps are carried out in LMBR decoding:</p>
<ol type="1">
<li>The evidence space is normalised after applying a grammar scale factor (<code>--alpha=</code>). Scaling is done by the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://www.openfst.org/twiki/bin/view/FST/PushDoc">Push</a> towards final states, and setting the final state probability to 1.0.</li>
<li>N-grams are extracted from the hypothesis space.</li>
<li>N-gram path-posterior probabilities are computed over the evidence space using a modified Forward procedure (see <a class="el" href="intro.html#Blackwood2010">Blackwood2010</a>)</li>
<li>Cyclic WFSAs are built to represent posterior probability distribution of each n-gram order and compose with the original hypotheses space. A word insertion penalty (<code>--wps=</code>) is also included in the costs of the cyclic WFSAs.</li>
<li>Risk is computed through a sequence of compositions.</li>
<li>The result for LMBR decoding is a WFSA; each weighted path represents a hypothesis and its risk.</li>
</ol>
<p>The weighted hypothesis space can be save as a WFSA, or the minimum risk hypothesis can be generated via the <a class="el" href="intro.html#OpenFst">OpenFst</a> <a href="http://www.openfst.org/twiki/bin/view/FST/ShortestPathDoc">ShortestPath</a> operation.</p>
<p>The following example applies LMBR decoding to the baseline lattices </p>
<pre class="fragment">&gt; lmbr.${TGTBINMK}.bin --config=configs/CF.baseline.lmbr &amp;&gt; log/log.baseline.lmbr
</pre><p>The LMBR output hyppthesis file keeps the scale factor, word penalty, and sentence id at the start of the file; the hypothesis follows the colon </p>
<pre class="fragment">&gt; cat output/exp.baseline.lmbr/HYPS/0.40_0.02.hyp
0.4 0.02 1:1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 
0.4 0.02 2:1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 6 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2 
</pre><p>LMBR can be optimised by tuning the grammar scale factor and word insertion penalty. Once lattices are loaded into memory and n-grams are extracted (steps 1 - 5), rescoring is fast enough that it is practical and efficient to perform a grid search over a range of parameter values (see config file).</p>
<p>Hypotheses are written to different files, with names based on parameter values (e.g. as <code>--writeonebest=output/exp.baseline.lmbr/HYPS/%alpha%_%wps%%.hyp</code> ). The best set of values can be selected based on BLEU score, after mapping each integer mapped output back to words, detokenizing, and scoring on against references.</p>
<p>LMBR relies on a unigram precision (p) and precision ratio (r) that are computed over a development set, e.g. with verbose logs of a BLEU scorer such as NIST mteval1. The script <code>$HiFSTROOT/scripts/lmbr/compute-testset-precisions.pl</code> is included for this purpose.</p>
<p>If the option <code>--preprune=</code> is specified, the evidence space is pruned prior to computing posterior probabilities (i.e. pruning is done at threshold 7 in this example). If this option is not defined, the full evidence space will be passed through.</p>
<h1><a class="anchor" id="mert"></a>
MERT (Features Only)</h1>
<p>This section describes how to generate N-Best lists of features for use with <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a> [<a href="http://aclweb.org/anthology/P/P03/P03-1021.pdf">Och 2003</a>].</p>
<p>The following sequence of operations will generate hypotheses and feature vectors that can be used by <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a>. We use N-Best lists of depth <code>N = 100</code>, set by the <code>prunereferenceshortestpath=</code> option in <code>configs/CF.mert.alilats.nbest</code>. For this tutorial, the configuration files specify multithreading, and N-Best lists will be generated only for the first 2 sentences in RU/RU.tune.idx. </p>
<pre class="fragment">&gt; M=2
# Step 1.  Generate hyps
&gt; hifst.${TGTBINMK}.bin --config=configs/CF.mert.hyps --range=1:$M &amp;&gt; log/log.mert.hyps
# Step 2. Generate alignment lats
&gt; hifst.${TGTBINMK}.bin --config=configs/CF.mert.alilats.nbest --range=1:$M &amp;&gt; log/log.mert.alilats.nbest
# Step 3. Generate feature vectors
&gt; alilats2splats.${TGTBINMK}.bin --config=configs/CF.mert.vecfea.nbest --range=1:$M &amp;&gt; log/log.mert.nbest
</pre><p>The process takes the following as its input:</p>
<ul>
<li><code>RU/RU.tune.idx</code> &ndash; tuning set source language sentences</li>
<li><code>G/rules.shallow.vecfea.gz</code> &ndash; translation grammar, with unweighted feature vectors</li>
<li><code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code> &ndash; target language model</li>
<li>Initial feature weights for the language model and translation grammar (see <code>configs/CF.mert.hyps</code> and <a class="el" href="basictrans.html#weight_feature_vector_examples">Applying Weight Vectors in Translation</a>)<ul>
<li>P=1.0,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136</li>
</ul>
</li>
</ul>
<p>The output is written to two linked sets of files, for <code>m=1,...,$M</code></p>
<ul>
<li><code>output/exp.mert/nbest/VECFEA/$m.nbest.gz</code> &ndash; word hypotheses</li>
<li><code>output/exp.mert/nbest/VECFEA/$m.vecfea.gz</code> &ndash; unweighted feature vectors</li>
</ul>
<p>The output files contain 1) the top <code>N</code> hypotheses for each source sentence and 2) the corresponding unweighted feature vector obtained from the best derivation of each of those hypotheses: </p>
<pre class="fragment">&gt; zcat output/exp.mert/nbest/VECFEA/1.nbest.gz | head -2
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;  43.0904
&lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      43.1757

&gt; zcat output/exp.mert/nbest/VECFEA/1.vecfea.gz | head -2
62.5442 10.8672 8.3936  -16.0000        -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293
63.1159 12.8613 8.7959  -17.0000        -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482
</pre><p>As a sanity check, computing the inner product between the vector <code>$P</code> and the first unweighted feature vector yields the following score </p>
<pre class="fragment">43.09045369 = $P . [62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000 -1.0000 0.0000 -7.0000 16.3076 40.5293]
</pre><p>which agrees with the score assigned to the top hypotheses by the decoder: </p>
<pre class="fragment">&gt; printstrings.${TGTBINMK}.bin --input=output/exp.mert/LATS/1.fst.gz --weight --semiring=lexstdarc --label-map=wmaps/wmt13.en.wmap
...
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 43.093,-19.4512
...
</pre><p><b>Note</b> that there can be minor numerical differences in the scores of the best derivations and the score found in the initial decoding, as the above example shows.</p>
<p>The three steps to generating the n-best hypotheses and unweighted feature vectors are described next.</p>
<h2><a class="anchor" id="mert_hyps"></a>
Step 1. Hypotheses for MERT</h2>
<ul>
<li>Input:<ul>
<li><code>RU/RU.tune.idx</code> &ndash; tuning set source language sentences</li>
<li><code>G/rules.shallow.vecfea.gz</code> &ndash; translation grammar</li>
<li><code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code> &ndash; target language model</li>
<li>language model and translation grammar feature weights (see <code>configs/CF.mert.hyps</code>)</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.mert/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized</li>
</ul>
</li>
</ul>
<p>This step runs HiFST in the usual way to generate a set of translation hypotheses which will be used in <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a>. </p>
<pre class="fragment">&gt; hifst.${TGTBINMK}.bin --config=configs/CF.mert.hyps --range=1:$M &amp;&gt; log/log.mert.hyps
</pre><p>In this configuration, the grammar feature weights and the language model feature weights are applied on-the-fly to the grammar and language model as they are loaded. This allows feature vector weights to be changed at each iteration of <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a>. This behaviour is specified through the following options in the <code>CF.mert.hyps</code> file, where we use the parameters from the baseline system: </p>
<pre class="fragment">[lm]
load=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap
featureweights=1.0
# Note that for only one language model, this parameter will always be set to 1.
# If there are multiple language models, the language model weights will be updated
# after each iteration of MERT

[grammar]
load=G/rules.shallow.vecfea.gz
featureweights=0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
# Note that this parameter vector should be updated after each iteration of MERT
# Updated versions can be provided via command line arguments
</pre><p>The translation grammar has its rules with unweighted feature vectors: </p>
<pre class="fragment">&gt; zcat G/rules.shallow.vecfea.gz | head -n 3
V 3 4 0.223527 0.116794 -1 -1 0 0 0 0 -1 1.268789 0.687159
V 3 4_3 3.333756 0.338107 -2 -1 0 0 0 0 -1 1.662178 3.363062
V 3 8 3.74095 3.279819 -1 -1 0 0 0 0 -1 3.741382 2.271445
</pre><p>The output lattices in <code>output/exp.mert/LATS</code> are acceptors containing word hypotheses, with weights in the form of the lexicographic semiring as described earlier. </p>
<pre class="fragment">&gt; zcat output/exp.mert/LATS/1.fst.gz | fstinfo | head -n 2
fst type                                          vector
arc type                                          tropical_LT_tropical

&gt; zcat output/exp.mert/LATS/1.fst.gz | printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      43.093,-19.4512
</pre><h2><a class="anchor" id="mert_nblist_derivations"></a>
Step 2. Guided Translation / Forced Alignment</h2>
<ul>
<li>Input:<ul>
<li><code>RU/RU.tune.idx</code> &ndash; tuning set source language sentences</li>
<li><code>G/rules.shallow.gz</code> &ndash; translation grammar</li>
<li><code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code> &ndash; target language model</li>
<li><code>output/exp.mert/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized (from <a class="el" href="basictrans.html#mert_hyps">Step 1. Hypotheses for MERT</a>)</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.mert/nbest/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations (e.g. Fig. 7, [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>])</li>
</ul>
</li>
</ul>
<p>Alignment is the task of finding the derivations (sequences of rules) that can produce a given translation. HiFST performs alignment via constrained translation (see Section 2.3, [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>] for a detailed description). This command runs HiFST in alignment mode: </p>
<pre class="fragment">&gt; hifst.${TGTBINMK}.bin --config=configs/CF.mert.alilats.nbest --range=1:$M &amp;&gt; log/log.mert.alilats.nbest
</pre><p>In alignment mode, HiFST constructs <em>substring acceptors</em> (see Fig. 8, [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>]). These are constructed for each sentence as follows:</p>
<ul>
<li>the lattice from Step 1 is loaded by HiFST</li>
<li>an N-Best list in the form of a WFSA is extracted (using <code>fstshortestpath</code>) under the grammar and language model score from Step 1</li>
<li>weights are removed from N-Best WFSA</li>
<li>the WFSA is transformed to a substring acceptor</li>
</ul>
<p>The translation grammar is applied in the usual way, but the translations are intersected with the substring acceptors so that only translations in the N-Best lists are retained. This generates every possible derivation of each N-Best list entry.</p>
<p>This behaviour is specified by the following config file parameters: </p>
<pre class="fragment">[referencefilter]
load=output/exp.mert/LATS/?.fst.gz
# perform alignment against these reference lattices containing initial hypotheses
prunereferenceshortestpath=100
# on loading the reference lattices, transform them to n-best lists prior to alignment.
# uses fstshortestpath
</pre><p>Note that application of the substring acceptors is very efficient; and this alignment step should be much faster than the translation operation of Step 1. The alignment lattices (referred to as ALILATS) map rule sequences (derivations) to translation hypotheses. Weights remain in lexicographic semiring form. </p>
<pre class="fragment">&gt; zcat output/exp.mert/nbest/ALILATS/1.fst.gz | fstinfo | head -n 2
fst type                                          vector
arc type                                          tropical_LT_tropical
</pre><p>Individual rules are identified by their line number in the translation grammar file. A rule map can be created as </p>
<pre class="fragment">&gt; zcat G/rules.shallow.gz | awk 'BEGIN{print "0\t0"}{printf "%s-&gt;&lt;%s,%s&gt;\t%d\n", $1, $2, $3, NR}'  &gt; tmp/rules.shallow.map
&gt; head -5 tmp/rules.shallow.map
0       0
V-&gt;&lt;3,4&gt;        1
V-&gt;&lt;3,4_3&gt;      2
V-&gt;&lt;3,8&gt;        3
V-&gt;&lt;3,10&gt;       4
</pre><p>The ALILATS transducers are not determinised: they contain every possible derivation for each N-Best list entry. The following example prints some of the alternative derivations of the top-scoring hypothesis: </p>
<pre class="fragment"># create a simple, unweighted acceptor for the top-scoring hypothesis
&gt; zcat output/exp.mert/LATS/1.fst.gz | fstshortestpath | fstmap --map_type=rmweight &gt; tmp/1.fst 

# print the two best derivations for the top-scoring hypothesis
&gt; zcat output/exp.mert/nbest/ALILATS/1.fst.gz | fstcompose - tmp/1.fst | fstproject | printstrings.${TGTBINMK}.bin --nbest=2 --semiring=lexstdarc -m tmp/rules.shallow.map
S-&gt;&lt;1,1&gt; V-&gt;&lt;3526,50&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;28847,245&gt; X-&gt;&lt;10_1278_V,135_20_103_3_V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;3_64570,4_25_1145_48&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;1857,3_425_6&gt; X-&gt;&lt;V_7786,V_23899&gt; S-&gt;&lt;S_X,S_X&gt; X-&gt;&lt;2,&lt;/s&gt;&gt; S-&gt;&lt;S_X,S_X&gt;
S-&gt;&lt;1,1&gt; V-&gt;&lt;3526,50&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;28847,245&gt; X-&gt;&lt;10_1278_V,135_20_103_3_V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;3_64570,4_25_1145_48&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;1857,3_425_6&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;7786,23899&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; X-&gt;&lt;2,&lt;/s&gt;&gt; S-&gt;&lt;S_X,S_X&gt;
</pre><p>The order of the rules in these rule sequences correspond to HiFST's bottom-up (left-to-right) CYK grid structure. Rule IDs are added as input symbols to the component WFSTs in the RTN following the translation rule (with its non-terminals). This leads to the bottom-up ordering after Replacement.</p>
<h2><a class="anchor" id="mert_alilats"></a>
Step 3. Hypotheses with Unweighted Feature Vectors</h2>
<ul>
<li>Input:<ul>
<li><code>G/rules.shallow.vecfea.gz</code> &ndash; translation grammar, rules with (unweighted) feature vectors</li>
<li><code>output/exp.mert/nbest/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations, for n-best entries (from <a class="el" href="basictrans.html#mert_nblist_derivations">Step 2. Guided Translation / Forced Alignment</a>)</li>
<li>language model and translation grammar feature weights (see <code>configs/CF.mert.vecfea.nbest</code>)</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.mert/lats/VECFEA/?.nbest.gz</code> &ndash; N-best hypotheses</li>
<li><code>output/exp.mert/lats/VECFEA/?.vecfea.gz</code> &ndash; N-best unweighted features</li>
</ul>
</li>
</ul>
<p>The alilats2splats tool transforms ALILATS alignment lattices (transducers) to sparse vector weight lattices; see Section 2.3.1, [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>] for a detailed explanation. </p>
<pre class="fragment">&gt; alilats2splats.${TGTBINMK}.bin --config=configs/CF.mert.vecfea.nbest --range=1:$M &amp;&gt; log/log.mert.nbest
</pre><p>The output is written to two sets of files:</p>
<p>N-best lists: </p>
<pre class="fragment"> &gt; zcat -f output/exp.mert/nbest/VECFEA/1.nbest.gz | head -n 2
 &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;     43.0904
 &lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 43.1757
</pre><p>Vecfea files: </p>
<pre class="fragment"> &gt; zcat -f output/exp.mert/nbest/VECFEA/1.vecfea | head -n 2 
 62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293
 63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482
</pre><ul>
<li>Line <code>n</code> in the nbest list is the `n-th' translation hypotheses, as ranked under the combined grammar and language model scores.</li>
<li>Line <code>n</code> in the vecfea file is a vector obtained by summing the unweighted feature vectors of each rule in the best derivation of the <code>n-th</code> hypothesis</li>
</ul>
<p>The alilats2splats tool works as follows:</p>
<ul>
<li>The translation grammar with (unweighted) feature vectors is loaded</li>
<li>a Rule Flower acceptor, R, is created. This is an acceptor for rule sequences that applies vector weights (specifically, the feature vector for each rule). See Fig. 9 of [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>] for an example and an explanation.</li>
<li>For each source sentence, the ALILATS derivation-to-translation transducer from Step 2 is loaded, and its weights are removed. Call this T_u</li>
<li>The unweighted derivations-to-translation trandsducer T_u is composed with the Rule Flower acceptor R under the tropical sparse tuple weight semiring with the same feature vectors as are used to generate the translation.</li>
<li>The feature vector for the best scoring derivation for every translation is found as Determinise(Project_output(R o T_u) )</li>
<li>Language model scores M_1, ..., M_m are applied (again in the tropical sparse tuple weight semiring, so that each score ends up in a separate element in the vector) as Determinise(Project_output(R o T_u) ) o M_1 o ... o M_m</li>
</ul>
<p>Writing of N-Best lists and features is controlled by the <code>sparseweightvectorlattice</code> options <code>storenbestfile</code> and <code>storefeaturefile</code>: </p>
<pre class="fragment">[sparseweightvectorlattice]
loadalilats=output/exp.mert/nbest/ALILATS/?.fst.gz
storenbestfile=output/exp.mert/nbest/VECFEA/?.nbest
storefeaturefile=output/exp.mert/nbest/VECFEA/?.vecfea
wordmap=wmaps/wmt13.en.wmap
</pre><p>With the wordmap specified, the output of alilats2splats is in readable form in the target language. Note that the sentence boundary symbols and the combined grammar and language model score appear in the nbest file. The N-best lists have the format</p>
<ul>
<li>wordindex1 wordindex2 ... translation_score</li>
</ul>
<p>The relationship of feature vectors and scores at the hypothesis level is as follows:</p>
<ul>
<li>Suppose there are m language models, with weights s_1 ...,s_m .<ul>
<li>These weights are specified by the HiFST parameters <code>lm.featureweights=s_1,s_2,..,s_m</code></li>
</ul>
</li>
<li>Suppose there are n dimensional feature vectors for each rule,<ul>
<li>The weights to be applied are specified by the HiFST parameters <code>grammar.featureweights=w_1,..,w_n</code></li>
</ul>
</li>
<li>A feature weight vector is formed as P = [s_1 ... s_m w_1 ... w_n]</li>
<li>A translation hypothesis e has a feature vector F(e) = [lm_1(e) ... lm_m(e) f_1(e) ... f_n(e)]<ul>
<li>lm_i(e): the i-th language model score for e</li>
<li>f_j(e): j-th grammar feature (see Section 3.2.1, [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>])</li>
</ul>
</li>
<li>The score of translation hypothesis e can be found as S(e) = F(e) . P (dot product)</li>
</ul>
<p>Each line k in the feature file has the format</p>
<ul>
<li>lm_1(e_k) ... lm_m(e_k) f_1(e_k) ... f_n(e_k)</li>
</ul>
<p>which are the unweighted feature values for the k-th hypothesis, e.g. </p>
<pre class="fragment"> &gt; zcat -f output/exp.mert/nbest/VECFEA/1.vecfea.gz | head -n 2
 62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293
 63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482
</pre><p>and the translation_score in line k is F(e_k) . P </p>
<pre class="fragment">&gt; zcat -f output/exp.mert/nbest/VECFEA/1.nbest.gz | head -n 2
&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      43.0904
&lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;  43.1757
</pre><h1><a class="anchor" id="lmert"></a>
Lattice MERT</h1>
<p>This section describes how to:</p>
<ul>
<li>generate lattices for use with LMERT [<a class="el" href="intro.html#Macherey2008">Macherey2008</a>]</li>
<li>run the HiFST implementations of LMERT for iterative parameter estimation [<a class="el" href="intro.html#Waite2012">Waite2012</a>]</li>
</ul>
<p>This HiFST release includes an implementation of LMERT [<a class="el" href="intro.html#Waite2012">Waite2012</a>]. The script <code>HiFST_lmert</code> runs several iterations of lattice generation and parameter estimation using the <code>lmert</code> tool. </p>
<pre class="fragment">&gt; export M=10 # run lmert over 10 sentences; for exposition only.   
&gt; scripts/HiFST_lmert
</pre><p>This script runs 3 iterations of LMERT, with each iteration consisting of the four steps that follow in the sections below. In our experience, on a X86_64 Linux computer with 4 2.8GHz CPUs and 24GB RAM, each iteration takes ca. 3 hours over the full 1502 sentence tuning set. Output from each iteration is written to files <code>log/log.lmert.[1,2,3,4]</code>. For example, </p>
<pre class="fragment">&gt; tail -n 8 log/log.lmert.?
==&gt; log/log.lmert.1 &lt;==
Fri Apr  3 21:04:07 2015: RandomLineSearch.INF:Bleu gain less than threshold. Exiting.
Fri Apr  3 21:04:07 2015: RandomLineSearch.INF:Initial Bleu: 0.308047 (0.990986)
Fri Apr  3 21:04:07 2015: RandomLineSearch.INF:Final Bleu:   0.326275 (0.996078)
Fri Apr  3 21:04:07 2015: RandomLineSearch.INF:Final Lambda: 1 0.804341 0.857127 2.94424 -1.08039 1.31302 41.8783 -6.55013 -5.76312 -0.843428 0.294197 0.300588 
Fri Apr  3 21:04:07 2015: main.INF:lmert.O2.bin finished!
==Params
Fri Apr  3 21:04:10 BST 2015
1,0.804341,0.857127,2.94424,-1.08039,1.31302,41.8783,-6.55013,-5.76312,-0.843428,0.294197,0.300588
==&gt; log/log.lmert.2 &lt;==
Sat Apr  4 00:12:01 2015: RandomLineSearch.INF:Bleu gain less than threshold. Exiting.
Sat Apr  4 00:12:01 2015: RandomLineSearch.INF:Initial Bleu: 0.323506 (0.998028)
Sat Apr  4 00:12:01 2015: RandomLineSearch.INF:Final Bleu:   0.326441 (0.999893)
Sat Apr  4 00:12:01 2015: RandomLineSearch.INF:Final Lambda: 1 1.05897 0.893133 2.81302 -0.752161 1.2221 35.8963 -6.62349 -4.37219 -0.583849 0.242874 0.194865 
Sat Apr  4 00:12:01 2015: main.INF:lmert.O2.bin finished!
==Params
Sat Apr  4 00:12:04 BST 2015
1,1.05897,0.893133,2.81302,-0.752161,1.2221,35.8963,-6.62349,-4.37219,-0.583849,0.242874,0.194865

==&gt; log/log.lmert.3 &lt;==
Sat Apr  4 04:55:40 2015: RandomLineSearch.INF:Bleu gain less than threshold. Exiting.
Sat Apr  4 04:55:40 2015: RandomLineSearch.INF:Initial Bleu: 0.326105 (0.999734)
Sat Apr  4 04:55:40 2015: RandomLineSearch.INF:Final Bleu:   0.327024 (0.999893)
Sat Apr  4 04:55:40 2015: RandomLineSearch.INF:Final Lambda: 1 1.05159 0.88871 2.80248 -0.750582 1.22436 30.0357 -5.14523 -2.57261 -0.585201 0.249596 0.196453 
Sat Apr  4 04:55:40 2015: main.INF:lmert.O2.bin finished!
==Params
Sat Apr  4 04:55:43 BST 2015
1,1.05159,0.88871,2.80248,-0.750582,1.22436,30.0357,-5.14523,-2.57261,-0.585201,0.249596,0.196453
</pre><p>This indicates:</p>
<ul>
<li>The initial set of parameters yields a tuning set BLEU score of 0.308047, with brevity penalty 0.990986</li>
<li>The first iteration of LMERT improves the tuning set BLEU score to 0.32652 over the tuning set lattices generated with the initial parameters.</li>
<li>Retranslation with the parameters found at iteration 1 yields a tuning set BLEU score of 0.32488, with brevity penalty 0.999654</li>
<li>The second iteration of LMERT improves the tuning set BLEU score to 0.326884 over the tuning set lattices generated with the parameters from iteration 1</li>
</ul>
<p>Notes:</p>
<ul>
<li>The parameters used to initialise this demo are taken from the baseline system, and so few iterations are needed for LMERT to converge. Even when started from a flat start, LMERT tends to converge in fewer iterations than N-Best <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a>.</li>
<li>The n-gram language model is set by default to <code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap</code>. This is a quantized version of <code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code>. Slightly higher tuning set BLEU scores can be gotten with the unquantized LM, although memory use in tuning will be higher.</li>
<li>The script <code>HiFST_lmert</code> can be modified to perform LMERT over only the first (e.g.) 100 tuning set sentences. This can be done for debugging / demonstration, in that processing will be much faster, although the estimated parameters will not be as robust.</li>
</ul>
<p>The operations done by the <code>HiFST_lmert</code> script are described next.</p>
<h2><a class="anchor" id="lmert_hyps"></a>
Step 1. Hypotheses for LMERT</h2>
<p>Note that this step is also done in <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a> (<a class="el" href="basictrans.html#mert">MERT (Features Only)</a>), although the settings here are slightly different.</p>
<p>The following command is run at iteration <code>$it</code> , and will generate lattices for <code>$M</code> files.</p>
<ul>
<li>Input:<ul>
<li><code>$it</code> &ndash; lmert iteration (1, 2, ...)</li>
<li><code>$M</code> &ndash; number of sentences to process</li>
<li><code>RU/RU.tune.idx</code> &ndash; tuning set source language sentences</li>
<li><code>G/rules.shallow.vecfea.gz</code> &ndash; translation grammar</li>
<li><code>M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</code> &ndash; target language model</li>
<li>language model and translation grammar feature weights, provided via command line option</li>
</ul>
</li>
<li>Output<ul>
<li><code>output/exp.lmert/$it/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized</li>
<li><code>output/exp.lmert/$it/hyps</code> &ndash; translation hyps (discarded)</li>
</ul>
</li>
</ul>
<p>The feature weights are gathered into a single vector, and set via the <code>--featureweights</code> command line option</p>
<ul>
<li>The first parameter is the grammar scale factor (in the <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a> demo, this is set via <code>lm.featureweights</code>)</li>
<li>The remaining parameters are weights for the grammar features (in the <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a> demo, these are set via <code>grammar.featureweights</code>)</li>
</ul>
<p>The initial parameters are </p>
<pre class="fragment">it=1
export M=10
FW=1.0,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136
</pre><p>HiFST is run in translation mode as </p>
<pre class="fragment">&gt; hifst.${TGTBINMK}.bin --config=configs/CF.lmert.hyps --range=1:$M --featureweights=$FW --target.store=output/exp.lmert/$it/hyps --hifst.lattice.store=output/exp.lmert/$it/LATS/?.fst.gz
</pre><h2><a class="anchor" id="lmert_veclats"></a>
Step 2. Guided Translation / Forced Alignment</h2>
<ul>
<li>Input:<ul>
<li><code>$it</code> &ndash; lmert iteration (1, 2, ...)</li>
<li><code>$M</code> &ndash; number of sentences to process</li>
<li><code>output/exp.lmert/$it/LATS/?.fst.gz</code> &ndash; word lattices (WFSAs), determinized and minimized (from <a class="el" href="basictrans.html#lmert_hyps">Step 1. Hypotheses for LMERT</a>)</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.lmert/$it/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations (e.g. Fig. 7, [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>])</li>
<li><code>output/exp.lmert/$it/hyps</code> &ndash; translation hyps (discarded)</li>
</ul>
</li>
</ul>
<p>HiFST is run in alignment mode. Lattices (from <code>output/exp.lmert/$it/LATS/?.fst.gz</code>) read and transformed into substring acceptors used to constrain the space of alignments </p>
<pre class="fragment">&gt; hifst.${TGTBINMK}.bin --config=configs/CF.lmert.alilats --range=1:$M --referencefilter.load=output/exp.lmert/$it/LATS/?.fst.gz --target.store=output/exp.lmert/$it/hyps --hifst.lattice.store=output/exp.lmert/$it/ALILATS/?.fst.gz
</pre><p>Note the following parameters in the configuration file: </p>
<pre class="fragment">[referencefilter]
prunereferenceweight=4
# pruning threshold to be applied to input lattice prior to alignment
prunereferenceshortestpath=10000
# extract n-best list from input lattice to use as hypotheses
</pre><p>Reference lattices are read from <code>output/exp.lmert/$it/LATS/?.fst.gz</code>. For each lattice:</p>
<ul>
<li>an N-Best list of depth 10000 is extracted using <code>fstshortestpath</code></li>
<li>the lattices are pruned with threshold <code>prunereferenceweight=4</code></li>
<li>the pruned lattice is unioned with the N-Best list</li>
<li>the resulting WFSA is transformed (after removing weights, minimization and determinization) into a substring acceptor to be used in alignment</li>
</ul>
<p>A simpler approach could be simply to prune the reference lattices. However in practice it can be difficult to find a global pruning threshold that always yields a reference lattice that is big enough, but not too big. Including the n-best list ensures that there will always be a rich set of candidate hypotheses.</p>
<h2><a class="anchor" id="lmert_alilats"></a>
Step 3. WFSAs with Unweighted Feature Vectors</h2>
<ul>
<li>Input:<ul>
<li><code>$it</code> &ndash; lmert iteration (1, 2, ...)</li>
<li><code>$M</code> &ndash; number of sentences to process</li>
<li>language model and translation grammar feature weights, provided via command line options</li>
<li><code>output/exp.lmert/$it/ALILATS/?.fst.gz</code> &ndash; transducers mapping derivations to translations</li>
</ul>
</li>
<li>Output:<ul>
<li>output/exp.lmert/$it/VECFEA/?.fst.gz &ndash; translation lattices (WFSAs) with unweighted feature vectors</li>
</ul>
</li>
</ul>
<p><code>alilats2splats</code> transforms ALILATS alignment lattices to sparse vector weight lattices; see Section 2.3.1, [<a class="el" href="intro.html#deGispert2010">deGispert2010</a>] for a detailed explanation. A single output WFSA with sparse vector weights is written for each translation. Note that this is different from the <a class="el" href="structMERT.html" title="hifst-specific classes and methods included in this namespace. ">MERT</a> case, where two N-best lists of hypotheses and features are written for each translation.</p>
<p>The HiFST <code>alilats2splats</code> command is </p>
<pre class="fragment">&gt; alilats2splats.${TGTBINMK}.bin --config=configs/CF.lmert.vecfea --range=1:$M --featureweights=$FW --sparseweightvectorlattice.loadalilats=output/exp.lmert/$it/ALILATS/?.fst.gz --sparseweightvectorlattice.store=output/exp.lmert/$it/VECFEA/?.fst.gz
</pre><h2><a class="anchor" id="lmert_lmert"></a>
Step 4. LMERT</h2>
<ul>
<li>Input:<ul>
<li><code>$it</code> &ndash; lmert iteration (1, 2, ...)</li>
<li><code>$M</code> &ndash; number of sentences to process</li>
<li><code>output/exp.lmert/$it/VECFEA/?.fst.gz</code> &ndash; translation lattices (WFSAs) with unweighted feature vectors (from <a class="el" href="basictrans.html#lmert_alilats">Step 3. WFSAs with Unweighted Feature Vectors</a>)</li>
<li><code>EN/EN.tune.idx</code> &ndash; target language references in integer format</li>
</ul>
</li>
<li>Output:<ul>
<li><code>output/exp.lmert/params.$it</code> &ndash; reestimated feature vector under LMERT with BLEU</li>
</ul>
</li>
</ul>
<p><code>lmert</code> runs as follows </p>
<pre class="fragment">&gt; lmert.${TGTBINMK}.bin --config=configs/CF.lmert.lmert --range=1:$M \
--input=output/exp.lmert/$it/VECFEA/?.fst.gz  --initial_params=$FW \
--write_params=output/exp.lmert/params.$it 
</pre><h2><a class="anchor" id="lmert_references"></a>
BLEU, References, and De/Tokenization</h2>
<p><code>lmert</code> computes the BLEU score with respect to one or more reference translations. References can be provided either as integer-mapped sequences, or as plain text.</p>
<p>The example earlier in this section uses integer-mapped reference translations: </p>
<pre class="fragment">&gt; head -2 EN/EN.tune.idx 
50 135 20 103 245 9445 23899
3 245 10 35 578 7 9445 3 5073 972 1052 564 51 13011 317 312 734 6 3 122 14 16306 6 4448 14 119 3570 5
&gt; lmert.${TGTBINMK}.bin --int_refs=EN/EN.tune.idx --range=1:$M \
--input=output/exp.lmert/1/VECFEA/?.fst.gz --initial_params=$FW --random_seed=17 \
--write_params=tmp/params.1
...
Sat Apr  4 11:35:48 2015: RandomLineSearch.INF:Initial Bleu: 0.244382 (1)
Sat Apr  4 11:35:48 2015: RandomLineSearch.INF:Final Bleu:   0.379273 (1)
Sat Apr  4 11:35:48 2015: RandomLineSearch.INF:Final Lambda: 1 1.23568 0.883693 3.50594 -0.200368 -1.51527 41.5974 -4.49443 -5.2153 4.82334 -0.343277 0.45311 
...    
</pre><p>An alternative is to use plain-text reference files with the word map; with the same random seed, the results should agree with using integer mapped references: </p>
<pre class="fragment">&gt; head -2 EN/EN.tune     
parliament does not support amendment freeing tymoshenko
the amendment that would lead to freeing the imprisoned former prime minister was revoked during second reading of the proposal for mitigation of sentences for economic offences .
&gt; lmert.${TGTBINMK}.bin --word_refs=EN/EN.tune --word_map=wmaps/wmt13.en.wmap --range=1:$M \
--input=output/exp.lmert/1/VECFEA/?.fst.gz --initial_params=$FW --random_seed=17 \
--write_params=tmp/params.1
...
Sat Apr  4 11:39:25 2015: RandomLineSearch.INF:Initial Bleu: 0.244382 (1)
Sat Apr  4 11:39:25 2015: RandomLineSearch.INF:Final Bleu:   0.379273 (1)
Sat Apr  4 11:39:25 2015: RandomLineSearch.INF:Final Lambda: 1 1.23568 0.883693 3.50594 -0.200368 -1.51527 41.5974 -4.49443 -5.2153 4.82334 -0.343277 0.45311 
...
</pre><p>It is also possible to including de/tokenization of hypotheses prior to BLEU computation. For example, references are processed such that apostrophes are treated as separate tokens: </p>
<pre class="fragment">&gt; awk 'NR==3' EN/EN.tune
&gt; the verdict is not yet final ; the court will hear tymoshenko ' s appeal in december .
</pre><p>As an alternative, a set of references can be created which attach apostrophes to words, and the script used to process the references can be provided to <code>lmert</code> as an <code>external tokenizer</code>: </p>
<pre class="fragment">&gt; echo "s/[ ]*'[ ]*/'/" &gt; tmp/sed.apos 
&gt; sed -f tmp/sed.apos EN/EN.tune &gt; tmp/EN.tune.apos
&gt; awk 'NR==3' tmp/EN.tune.apos
the verdict is not yet final ; the court will hear tymoshenko's appeal in december .
&gt; lmert.${TGTBINMK}.bin --word_refs=EN/EN.tune --word_map=wmaps/wmt13.en.wmap --range=1:$M \
--input=output/exp.lmert/1/VECFEA/?.fst.gz --initial_params=$FW --random_seed=17 \
--write_params=tmp/params.1 --external_tokenizer="tee tmp/before | sed -u -f tmp/sed.apos | tee tmp/after"
...
Sat Apr  4 11:41:38 2015: RandomLineSearch.INF:Initial Bleu: 0.24523 (1)
Sat Apr  4 11:41:38 2015: RandomLineSearch.INF:Final Bleu:   0.380598 (1)
Sat Apr  4 11:41:38 2015: RandomLineSearch.INF:Final Lambda: 1 1.23568 0.883693 3.50594 -0.200368 -1.51527 41.5974 -4.49443 -5.2153 4.82334 -0.343277 0.45311 
...
</pre><p>The <code>tee</code> command makes it possible to compare hypotheses before and after processing: </p>
<pre class="fragment"> &gt; diff tmp/before tmp/after | head -4
 9c9
 &lt; instead of the dictator 's society is composed of rival clans , will be merged the koran .
 ---
 &gt; instead of the dictator's society is composed of rival clans , will be merged the koran .
</pre><p>Lmert optimises the BLEU score over the latter sets of hypotheses. <b>Note</b> that it is possible to use the <code>external_tokenizer</code> with integer references, but the external tokenizer will have to be able to read integer sequences at its input and write integer sequences at its output, i.e. it will have to apply a word map internally.</p>
<h1><a class="anchor" id="lmert_veclats_tst"></a>
Tropical Sparse Tuple Semiring</h1>
<p>The lattices generated by <code>alilats2splats</code> in <code>output/exp.lmert/1/lats/VECFEA</code> are <code>tropicalsparsetuple</code> vector weight lattices. </p>
<pre class="fragment">&gt; zcat output/exp.lmert/1/VECFEA/1.fst.gz | fstinfo | head -n 2
fst type                                          vector
arc type                                          tropicalsparsetuple
</pre><p>The scores in these lattices are unweighted by the feature vector weights, i.e. they are the raw feature scores against which L/MERT finds the optimal parameter vector values. Distances under these unweighted vectors do not agree with the initial translation hypotheses, e.g. the shortest-path does not agree with the best translation: </p>
<pre class="fragment">&gt; unset TUPLEARC_WEIGHT_VECTOR
&gt; zcat output/exp.lmert/1/VECFEA/1.fst.gz | fstshortestpath | fsttopsort | fstpush --to_final --push_weights | fstprint -isymbols=wmaps/wmt13.en.wmap
Warning: cannot find parameter vector. Defaulting to flat parameters
Warning: cannot find parameter vector. Defaulting to flat parameters
0       1       &lt;s&gt;     1
1       2       parliament      50
2       3       not     20
3       4       supports        1463
4       5       amendment       245
5       6       ,       4
6       7       gives   1145
7       8       freedom 425
8       9       tymoshenko      23899
9       10      &lt;/s&gt;    2
10      0,10,1,63.460289,2,7.08984375,3,10.9941406,4,-13,5,-9,6,-6,9,-1,10,-8,11,13.1025391,12,29.7294922,
</pre><p>The sparse vector weight format is </p>
<pre class="fragment">0,N,idx_1,fea_1,...,idx_N,fea_N
</pre><p>where N is the number of non-zero elements in that weight vector.</p>
<p>To compute semiring costs correctly, the <code>TUPLEARC_WEIGHT_VECTOR</code> environment variable should be set to contain the correct feature vector weight; this should be the same feature vector weight applied in translation in steps 1 and 2: </p>
<pre class="fragment">TUPLEARC_WEIGHT_VECTOR=[s_1 ... s_m w_1 ... w_n]
</pre><p>which in this particular example is </p>
<pre class="fragment">&gt; export TUPLEARC_WEIGHT_VECTOR="1,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136"
</pre><p>The shortest path found through the vector lattice is then the same hypothesis produced under the initial parameter settings: </p>
<pre class="fragment">&gt; zcat output/exp.lmert/1/VECFEA/1.fst.gz | fstshortestpath | fsttopsort | fstpush --to_final --push_weights | fstprint -isymbols=wmaps/wmt13.en.wmap
0       1       &lt;s&gt;     1
1       2       parliament      50
2       3       supports        1463
3       4       amendment       245
4       5       giving  803
5       6       freedom 425
6       7       tymoshenko      23899
7       8       &lt;/s&gt;    2
8       0,10,1,62.1510468,2,10.8671875,3,8.39355469,4,-16,5,-8,6,-5,8,-1,10,-7,11,16.3076172,12,40.5292969,
</pre><p>Note that <code>printstrings</code> can be used to extract n-best lists from the vector lattices, if the TUPLEARC_WEIGHT_VECTOR is correctly set: </p>
<pre class="fragment">&gt; zcat output/exp.lmert/1/VECFEA/1.fst.gz | printstrings.${TGTBINMK}.bin --semiring=tuplearc --nbest=10 --unique -w -m wmaps/wmt13.en.wmap --tuplearc.weights=$TUPLEARC_WEIGHT_VECTOR 2&gt;/dev/null
&lt;s&gt; parliament supports amendment giving freedom tymoshenko &lt;/s&gt;        20.7778,7.80957,17.8672,-8,-8,-5,0,0,-1,-7,20.0176,18.2979
&lt;s&gt; parliament supports amendment gives freedom tymoshenko &lt;/s&gt;         20.7773,8.48828,20.9248,-8,-8,-4,0,-1,0,-7,14.1162,14.1016
&lt;s&gt; parliament supports amendment giving freedom timoshenko &lt;/s&gt;        20.7778,9.70703,17.7393,-8,-8,-5,0,0,-1,-7,22.166,18.2529
&lt;s&gt; parliament supports correction giving freedom tymoshenko &lt;/s&gt;       20.7768,9.15527,18.6689,-8,-8,-4,0,0,-1,-7,22.4062,20.7334
&lt;s&gt; parliament supports amendment giving liberty tymoshenko &lt;/s&gt;        20.7768,10.2051,18.3838,-8,-8,-5,0,0,-1,-7,22.6582,19.4707
&lt;s&gt; parliament supports amendment gives freedom timoshenko &lt;/s&gt;         20.7773,10.1602,21.0596,-8,-8,-4,0,-1,0,-7,16.2646,14.0566
&lt;s&gt; parliament supports amendment enables freedom tymoshenko &lt;/s&gt;       20.7768,8.48828,20.0742,-8,-8,-4,0,-1,0,-7,50.2627,17.0137
&lt;s&gt; parliament supports amendment enable freedom tymoshenko &lt;/s&gt;        20.7768,8.48828,20.6904,-8,-8,-4,0,-1,0,-7,50.2627,15.3457
&lt;s&gt; parliament not supports amendment giving freedom tymoshenko &lt;/s&gt;    29.3873,5.82324,12.8096,-9,-9,-6,0,0,-1,-8,16.1914,17.9443
&lt;s&gt; parliament supports amendment providing freedom tymoshenko &lt;/s&gt;     20.7769,8.48828,21.1689,-8,-8,-4,0,-1,0,-7,50.2627,17.3027
</pre><p>These should agree with n-best lists generated directly by <code>alilats2splats</code> (see <a class="el" href="basictrans.html#mert_alilats">Step 3. Hypotheses with Unweighted Feature Vectors</a>).</p>
<p><b>Note</b> that there can be significant numerical differences between computations under the tropical lexicographic semiring vs the tuplearc semiring: printstrings and alilats2splats might not give exactly the same results. In such cases, the alilats2splats result is probably the better choice.</p>
<h1><a class="anchor" id="chopping"></a>
Source Sentence Chopping</h1>
<p>Long source sentences make the translation process slow and expensive in memory consumption; see [<a class="el" href="intro.html#Allauzen2014">Allauzen2014</a>] for a discussion of how source sentence length affects computational complexity and memory use by HiFST and HiPDT. There are various strategies for controlling translation complexity; pruning has been discussed (<a class="el" href="basictrans.html#lpruning">Inadmissible Pruning</a>), and it is also possible to set the maximum span and gap spans allowed in translation so as to control computational complexity. However translation quality can be affected if pruning is too heavy or if span constraints are set too aggressively.</p>
<p>An alternative approach is to 'chop' long sentences into shorter segements which can then be translated separately. If the sentence chopping is done carefully, the impact on the translation quality can be minimized. The benefits to chopping are faster translation that consumes less memory. The potential drawbacks are twofold: chopping can prevent the search procedure from finding good hypothesis under the grammar, and care must be taken to correctly apply the target language model at the sentence level.</p>
<p>We describe two approaches to source sentence chopping:</p>
<ol type="1">
<li><b>Explicit Segmentation</b>: Long source sentences are chopped into smaller segments which are each translated separately. The results are then spliced together, in various ways.</li>
<li><b>Grammar-based Sentence Chopping</b>: Chopping can be done by inserting the special chop symbol <code>0</code> in the source sentence, and then translating with a modified grammar. The chopping grammar is constructed so that translation rules are not applied across the chopping points, thus limiting the space of translation that are generated.</li>
</ol>
<p>To make the tutorial easy to follow, we will simply chop the Russian source sentences at every comma (<code>,</code>) which has index symbol <code>3</code>: </p>
<pre class="fragment">&gt; awk 'NR==4' wmaps/wmt13.ru.wmap 
,       3
</pre><h2><a class="anchor" id="chopping_sseg"></a>
Chopping by Explicit Source Sentence Segmentation</h2>
<p>The original Russian sentence is chopped into shorter sentences which are to be translated independently, as follows: </p>
<pre class="fragment"> # sentence 328 is 51 words long
 &gt; awk 'NR==328' RU/RU.tune.idx
 1 17914 3004 169868 123860 45 1246 53414 16617 15 6 215 26993 7704 5 142 680 13640 2481 1195794 16 7390 24705 14 3 1676 24844 33 3 24 1759 16617 5 18091 42 4340 140478 31410 5 13214 144114 6 8 12859 30201 2623 8 5 913863 4 2

 # chop sentence 328 into separate segments; 
 # split at the 3 symbol, and introduce sentence start and end symbols, 
 # so that language model and translation grammar are applied correctly
 &gt; awk 'NR==328' RU/RU.tune.idx | sed 's, 3 , 3 2\n1 ,g'  &gt; tmp/RU.328.segs
 &gt; cat tmp/RU.328.segs
 1 17914 3004 169868 123860 45 1246 53414 16617 15 6 215 26993 7704 5 142 680 13640 2481 1195794 16 7390 24705 14 3 2
 1 1676 24844 33 3 2
 1 24 1759 16617 5 18091 42 4340 140478 31410 5 13214 144114 6 8 12859 30201 2623 8 5 913863 4 2

 # run HiFST over all segments
 # lattices for each segment are written to tmp/seg.328.?.fst
 &gt; hifst.${TGTBINMK}.bin --source.load=tmp/RU.328.segs --target.store=tmp/hyps.328.segs --hifst.lattice.store=tmp/seg.328.?.fst --hifst.prune=9 --hifst.replacefstbyarc.nonterminals=X,V --lm.load=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap --grammar.load=G/rules.shallow.gz

 # concatenate lattices for each segment
 &gt; fstconcat tmp/seg.328.1.fst tmp/seg.328.2.fst | fstconcat - tmp/seg.328.3.fst &gt; tmp/seg.328.123.fst

 # print output strings
 &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -u -n 3 --input=tmp/seg.328.123.fst -w
 ...
 &lt;s&gt; architectural historian mÃ¤der toured more than 200 swimming pools , and now gathered experience in his recently published book badefreuden ( joy of swimming ) , &lt;/s&gt; &lt;s&gt; where included , &lt;/s&gt; &lt;s&gt; from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings " . &lt;/s&gt;     313.88,1.68848
 &lt;s&gt; architectural historian mÃ¤der toured more than 200 swimming pools , and now gathered experience in his recent book badefreuden ( joy of swimming ) , &lt;/s&gt; &lt;s&gt; where included , &lt;/s&gt; &lt;s&gt; from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings " . &lt;/s&gt;     314.045,5.41211
 &lt;s&gt; architectural historian mÃ¤der toured more than 200 swimming pools , and now gathered experience in his recently published book badefreuden ( joy of swimming ) , &lt;/s&gt; &lt;s&gt; where entered the &lt;/s&gt; &lt;s&gt; from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings " . &lt;/s&gt;    314.115,-0.470703
 ...
</pre><p>Simply concatenating the output lattices in this way leads to the substrings <code>&lt;/s&gt; &lt;s&gt;</code> in every hypothesis in the concatenated lattice. A transducer can be built to remove these from the output lattice. The transducer must</p>
<ul>
<li>keep the initial <code>1</code> (<code>&lt;s&gt;</code>)</li>
<li>keep the final <code>2</code> (<code>&lt;/s&gt;</code>)</li>
<li>delete every <code>1 2</code> (<code>&lt;/s&gt; &lt;s&gt;</code>) sequence</li>
<li>map every word to itself</li>
</ul>
<p>The following 3-state transducer will do this </p>
<pre class="fragment"> &gt; echo -e "0\t1\t1\t1" &gt; tmp/strip_1_2.txt              # keep initial `1`
 &gt; echo -e "1\t2\t2\t0\n2\t1\t1\t0" &gt;&gt; tmp/strip_1_2.txt # map `1 2` to `0 0`
 &gt; echo -e "1\t3\t2\t2" &gt;&gt; tmp/strip_1_2.txt             # keep final `2`
 &gt; echo -e "3" &gt;&gt; tmp/strip_1_2.txt
 &gt; awk '$2 != 1 &amp;&amp; $2 != 2 {printf "1\t1\t%d\t%d\n", $2,$2}' wmaps/wmt13.en.wmap &gt;&gt; tmp/strip_1_2.txt
 &gt; echo -e "1\t1\t999999998\t999999998" &gt;&gt;  tmp/strip_1_2.txt  # OOV symbol
 &gt; fstcompile  --arc_type=tropical_LT_tropical tmp/strip_1_2.txt | fstarcsort &gt; tmp/strip_1_2.fst

 # apply the strip_1_2.fst transducer to the fst of the concatenated translation lattices
 &gt; fstcompose tmp/seg.328.123.fst tmp/strip_1_2.fst | fstproject --project_output | fstrmepsilon &gt; tmp/seg.328.no12.fst 

 # look at output
 &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w --input=tmp/seg.328.no12.fst
 &lt;s&gt; architectural historian mÃ¤der toured more than 200 swimming pools , and now gathered experience in his recently published book badefreuden ( joy of swimming ) , where included , from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings " . &lt;/s&gt;   313.879,1.68848
</pre><p>The top hypothesis, and its score, are unchanged by removing the <code>&lt;/s&gt; &lt;s&gt;</code> substrings. <b>Note</b> however that the language model score for this hypothesis are not correct, since the language model histories are not applied correctly at the segment boundaries.</p>
<p>To fix this, the applylm tool (see <a class="el" href="basictrans.html#rescoring_lm">Language Model Rescoring</a>) can be used to remove and reapply the language model so that it spans the source segment translations. The following example simply removes the language model scores from output/exp.chopping.explicit/LATS/sent_no12.fst and then reapplies them via composition </p>
<pre class="fragment"> &gt; applylm.${TGTBINMK}.bin --lm.load=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap --lm.featureweights=1 --lm.wps=0.0 --semiring=lexstdarc --lattice.load=tmp/seg.328.no12.fst --lattice.store=tmp/seg.328.no12.relm.fst --lattice.load.deletelmcost 
</pre><p>The rescored output is written to <code>tmp/seg.328.no12.relm.fst</code> with correctly applied language model scores, i.e. the LM history is no longer broken by any <code>&lt;/s&gt; &lt;s&gt;</code> in the lattice. The total translation cost is much lower (better) than when segment hypotheses are simply combined (i.e. 291.958 vs. 313.879): </p>
<pre class="fragment">&gt; printstrings.${TGTBINMK}.bin --input=tmp/seg.328.no12.relm.fst --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 
&lt;s&gt; architectural historian mÃ¤der toured more than 200 swimming pools , and now gathered experience in his recently published book badefreuden ( joy of swimming ) , which included everything from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings " . &lt;/s&gt;  291.958,-0.329102
</pre><h2><a class="anchor" id="chopping_gb"></a>
Grammar-based Sentence Chopping</h2>
<p>Chopping can also be done by inserting a special 'chop' symbol <code>0</code> in the source sentence, and then translating with a modified grammar. The chopping grammar is constructed so that translation rules are not applied across the chopping points, thus limiting the space of translation that are generated. Conceptually, translation proceeds as:</p>
<ol type="1">
<li>the translation grammar is applied separately to source sentence segments demarcated by chop symbols</li>
<li>local pruning can be applied to the translations of these segments</li>
<li>the resulting WFSAs containing translations of the segments are concatenated under the chopping grammar, possibly with local pruning</li>
<li>the language model is applied to the concatenated WFSA</li>
<li>top-level, admissible pruning is done under the combined grammar and language model scores</li>
</ol>
<p>In this way the FSTs produced by translating the segments are concatenated prior to application of the target language model, and the language model context is not broken by the source sentence chopping. As an example, a grammar modified for chopping contains the following rules (without weights): </p>
<pre class="fragment"> R 1 1
 R R_D_X R_D_X
 R R_X R_X

 T 0 0
 T T_D_X T_D_X
 T T_X T_X

 S S_U S_U
 S Q Q
 Q R R
 U T T
</pre><p>The rules in the first block above are similar to those used in the usual Hiero grammar, with the original '<code>S</code>' changed to '<code>R</code>'. These rules are responsible for concatenating the partial translations of the source sentence, starting from the sentence-start symbol '1', up to but not including the first instance of the chopping symbol '0'.</p>
<p>Each subsequent sequence of source words starting with symbol '0', is handled in a similar way by the second block of rules above. Note that the only rule that can be applied to the input symbol '0' is '<code>T 0 0</code>', making the translation of each chopped segment independent. This makes use of the OpenFST convention of mapping 0 to epsilon: the 0's in the input are parsed as regular symbols by HiFST, while 0's on the output side are mapped to epsilons and ignored in composition with the language model.</p>
<p>The third block of rules above will join together the results obtained for each chopped segment. As with the glue rule '<code>S</code>' in the usual Hiero grammar, it is necessary to allow this new set of rules to be applied to any span. This is done by setting </p>
<pre class="fragment">  cykparser.ntexceptionsmaxspan=S,Q,R,T,U
</pre><p>The additional mapping provided by the last two rules controls the pruning applied to the top CYK cell relative to each chopped segment: </p>
<pre class="fragment">  hifst.localprune.conditions=Q,1,100,12,U,1,100,12,X,5,10000,9,V,3,20000,9
</pre><p>In the above example tighter parameters are chosen for '<code>Q</code>' and '<code>U</code>' to force pruning. In this way the final lattice obtained by concatenation is prevented from growing too large. However a wider beam (12) with respect to the other cell types is used, to avoid discarding too many potentially useful hypotheses.</p>
<p>It is possible to specify explicitly that FSTs generated for rules with LHS '<code>X</code>' or '<code>V</code>' can be kept as pointers rather then expanded in the FST (RTN) that is built for a higher CYK cell. This is achieved setting </p>
<pre class="fragment">  hifst.replacefstbyarc=X,V
  hifst.replacefstbyarc.exceptions=S,R,T
</pre><p>The second line above prevents substitution for rules with LHS '<code>S</code>', '<code>R</code>' and '<code>T</code>'. It is better to have a fully expanded FST for these rules for more effective optimisation (Determinization and Minimisation).</p>
<h3><a class="anchor" id="chopping_eg"></a>
Converting Grammars and Input Text for Chopping</h3>
<p>The usual Hiero grammar can be converted for chopping, as follows; note that no-cost, 0 valued, weights are added to rules :</p>
<p>First, create the chopping and glue rules: </p>
<pre class="fragment"> &gt; (echo "T T_D_X T_D_X 0" ; echo "T T_X T_X 0" ; echo "T 0 0 0") &gt; tmp/rules.shallow.chop
 &gt; (echo "S S_U S_U 0" ; echo "U T T 0" ; echo "Q R R 0" ; echo "S Q Q 0") &gt;&gt; tmp/rules.shallow.chop
 &gt; cat tmp/rules.shallow.chop
 T T_D_X T_D_X 0
 T T_X T_X 0
 T 0 0 0
 S S_U S_U 0
 U T T 0
 Q R R 0
 S Q Q 0
</pre><p>Next, append all rules, mapping glue rules with LHS S to LHS R: </p>
<pre class="fragment"> &gt; zcat G/rules.shallow.gz | sed 's,S,R,g' &gt;&gt; tmp/rules.shallow.chop
 &gt; gzip tmp/rules.shallow.chop
</pre><p>The source text (<code>RU/RU.set1.chop.idx</code>) will be chopped simply inserting the chopping marker '0' after each comma (integer mapped to 3 in the Russian wordmap); this is a simplistic approach that is easily implemented for this demonstration. We will select sentences with more than 80 words from the source language set: </p>
<pre class="fragment"> # there are 3 source sentences longer than 80 words
 &gt; awk 'NF &gt; 80 {print $0}' RU/RU.tune.idx &gt; tmp/RU.long.idx

 # make a version with the chopping symbol 0 after every comma
 &gt; sed 's, 3 , 3 0 ,g' tmp/RU.long.idx &gt; tmp/RU.long.csym.idx

 # print the first line; the &lt;epsilon&gt; indicates where the chopping symbol has been inserted
 &gt; farcompilestrings  --entry_type=line tmp/RU.long.csym.idx | farprintstrings --symbols=wmaps/wmt13.ru.wmap | head -1
 &lt;s&gt; спад экономической активности вызвал , &lt;epsilon&gt; до ноября , &lt;epsilon&gt; постоянный рост безработицы в штате , &lt;epsilon&gt; по данным национального института статистики и географии , &lt;epsilon&gt; в течение первых трех кварталов был зафиксирован ускоренный рост уровня безработицы , &lt;epsilon&gt; с января по март 55.053 жителя синалоа были безработными , &lt;epsilon&gt; с долей 4.53 процента экономически активного населения , &lt;epsilon&gt; во втором квартале доля возросла до 5.28 процента , &lt;epsilon&gt; а с июля по сентябрь она продолжила расти до 6.19 процента , &lt;epsilon&gt; доли , &lt;epsilon&gt; которая в пересчете на количество людей составляет более чем 74.000 безработных жителей синалоа , &lt;epsilon&gt; что на 18.969 человек больше по сравнению с первой половиной . &lt;/s&gt;

 # Run HiFST, without chopping.  Input is the original source: tmp/RU.long.idx
 # hypotheses are written to output/exp.chopping/nochop/hyps and lattices to output/exp.chopping/nochop/LATS/
 # configuration is otherwise 
 &gt; (time hifst.$TGTBINMK.bin --source.load=tmp/RU.long.idx --target.store=tmp/hyps.long.nochop --hifst.lattice.store=tmp/long.nochop.?.fst.gz --config=configs/CF.baseline ) &amp;&gt; log/log.long.nochop
</pre><p>The decoder requires the following settings to use the chopping grammar:</p>
<ul>
<li>hifst.replacefstbyarc.exceptions=S,R,T # Specifies glue rule types in the grammar</li>
<li>cykparser.ntexceptionsmaxspan=S,Q,R,T,U # List of non-terminals not affected by cykparser.hrmaxheight. S should be here</li>
</ul>
<p>Now run HiFST, with chopping. </p>
<pre class="fragment"> # Input is the source with chopping symbols: tmp/RU.long.csym.idx
 # hypotheses are written to tmp/hyps.long.gchop and lattices to tmp/long.gchop.?.fst.gz
 &gt; (time hifst.$TGTBINMK.bin --source.load=tmp/RU.long.csym.idx --target.store=tmp/hyps.long.gchop --hifst.lattice.store=tmp/long.gchop.?.fst.gz --config=configs/CF.baseline --grammar.load=tmp/rules.shallow.chop.gz --cykparser.ntexceptionsmaxspan=S,Q,R,T,U --hifst.replacefstbyarc.exceptions=S,R,T) &amp;&gt; log/log.long.gchop
</pre><p>Comparing the two experiments indicates that chopping requires less memory and runs faster (and note that the LM alone requires ~1GB): </p>
<pre class="fragment"> Input/Grammar  Tot time      Max memory
 --------       --------      ----------
 Unchopped       1m 21s         2.6Gb
 Chopped            39s         1.6Gb  
</pre><p>However, chopping restricts the space of translations. Looking at the scores of the best translation hypotheses, chopping the source sentence prevents the decoder from finding the best scoring hypothesis under the grammar; for the third sentence, the hypothesis produced without chopping has a lower (i.e. better) combined cost (471.123) than the hypothesis produced with chopping (474.753): </p>
<pre class="fragment"> # best hypothesis for the 1st sentence, without chopping
 &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --input=tmp/long.nochop.1.fst.gz -w -m wmaps/wmt13.en.wmap
 &lt;s&gt; the decline in economic activity caused , until november , the permanent rise in unemployment in the state , according to the national institute of statistics and geography , in the first three quarters was recorded accelerated growth in unemployment , from january to march 55.053 resident sinaloa were unemployed , with the share of per cent of the economically active population , in the second quarter , the share rose to percent from july to september , she continued to rise to percent share , if calculated per number of people is more than unemployed people in sinaloa , the 18.969 people more than in the first half . &lt;/s&gt;     471.123,2.46484

 # best hypothesis for the 1st sentence, with chopping
 &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --input=tmp/long.gchop.1.fst.gz -w -m wmaps/wmt13.en.wmap
 &lt;s&gt; the decline in economic activity caused , until november , a permanent rise in unemployment in the state , according to the national institute of statistics and geography , in the first three quarters was recorded accelerated growth in unemployment , from january to march 55.053 resident sinaloa were unemployed , with the share of per cent of the economically active population , in the second quarter , the share rose to percent from july to september , she continued to rise to percent stake , which is recalculated the number of people is more than unemployed people in sinaloa , the 18.969 people more than in the first half . &lt;/s&gt;       474.753,4.52344
</pre><h1><a class="anchor" id="true_casing"></a>
FST-based True Casing</h1>
<p>HiFST includes a tool typically used for true casing the output. It relies on two models:</p>
<ul>
<li>A true-case integer-mapped language model in ARPA or KenLM format.</li>
<li>A flower transducer that transduces uncased words to every true case alternative. This model is loaded from a file with the following format per line, one for each uncased word:<ul>
<li>uncased-case-word true-case-word1 prob1 true-case-word2 prob2 ...</li>
<li>This format is compatible with the unigram model for <a class="el" href="intro.html#SRILM">SRILM</a> <a href="http://www.speech.sri.com/projects/srilm/manpages/disambig.1.html">disambig</a> tool (see <code>--map</code> option).</li>
</ul>
</li>
</ul>
<p>Words must be integer-mapped. A file with this model is available: </p>
<pre class="fragment">&gt; head  G/tc.unimap
1 1 1.0
2 2 1.0
3 5943350 0.00002 3 0.86370 5943349 0.13628
4 4 1.00000
5 5 1.00000
6 5942623 0.00452 5942624 0.00002 6 0.99546
7 5943397 0.00000 5943398 0.01875 7 0.98121 5943399 0.00004
8 5941239 0.00003 8 0.99494 5941238 0.00502
9 5942238 0.06269 9 0.93729 5942239 0.00002
10 5943348 0.00001 10 0.99498 5943347 0.00501
</pre><p>For example, under this model word 4 (comma ",") transduces to itself with probability 1. The uncased word 3 ("the") has three upper-case alternatives: "the", "THE", and "The", with the following probabilities </p>
<pre class="fragment"> P(the | the) = 0.86
 P(THE | the) = 0.00002
 P(The | the) = 0.13628
</pre><p>To generate these probabilities, you just need counts of truecased words. You can extract these unigrams with <a class="el" href="intro.html#SRILM">SRILM</a> <a href="http://www.speech.sri.com/projects/srilm/manpages/ngram-count.1.html">ngram-count</a> tool, and calculate the probability of each particular true-cased form given the aggregated number of lower-cased instances.</p>
<p>These models are provided to the recaser module via the following configuration options </p>
<pre class="fragment">&gt; cat configs/CF.recaser
[recaser]
lm.load=M/lm.tc.gz
unimap.load=G/tc.unimap
</pre><p>The true casing procedure is very similar to that of <a class="el" href="intro.html#SRILM">SRILM</a> <a href="http://www.speech.sri.com/projects/srilm/manpages/disambig.1.html">disambig</a> tool. In our case this is accomplished with two subsequent compositions, followed by exact pruning. An acceptable performance vs speed/memory trade-off can be achieved e.g. with offline entropy pruning of the language model.</p>
<p>A range of input lattices can be true-cased in the following way with our fst-based disambig tool: </p>
<pre class="fragment"># re-run the baseline 
&gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline
# recase the output lattices
&gt; disambig.${TGTBINMK}.bin configs/CF.recaser --recaser.input=output/exp.baseline/LATS/?.fst.gz --recaser.output=output/exp.baseline/LATS/?.fst.recase.gz --range=1:2 -s lexstdarc
&gt; printstrings.${TGTBINMK}.bin --input=output/exp.baseline/LATS/?.fst.recase.gz --semiring=lexstdarc --label-map=wmaps/wmt13.en.wmap --range=1:2
&lt;s&gt; Republican strategy of resistance to the renewal of obamas election &lt;/s&gt; 
&lt;s&gt; The leaders of the Republican justified their policies need to deal with the spin on the elections . &lt;/s&gt; 
</pre><p>Note that both models need to be integer-mapped, hence the external target wordmap (&ndash;label-map) must also map true case words.</p>
<p>HiFST can include truecasing as subsequent step following decoding, prior to writing the output hypotheses. For instance: </p>
<pre class="fragment">&gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline --recaser.lm.load=M/lm.tc.gz --recaser.unimap.load=G/tc.unimap

&gt; farcompilestrings --entry_type=line output/exp.baseline/hyps | farprintstrings --symbols=wmaps/wmt13.en.wmap 
&lt;s&gt; Republican strategy of resistance to the renewal of obamas election &lt;/s&gt;
&lt;s&gt; The leaders of the Republican justified their policies need to deal with the spin on the elections . &lt;/s&gt;
</pre><p>However, the output lattices are left in uncased form: </p>
<pre class="fragment">&gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --label-map=wmaps/wmt13.en.wmap --input=output/exp.baseline/LATS/1.fst.gz 
&lt;s&gt; republican strategy of resistance to the renewal of obamas election &lt;/s&gt;
</pre><h1><a class="anchor" id="server"></a>
Client-Server Mode (Experimental)</h1>
<p>HiFST can run in server mode. </p>
<pre class="fragment"> &gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline.server &amp;&gt; log/log.server &amp;
 &gt; pid=$! # catch the server pid
</pre><p>Note that in this particular configuration, both source and target wordmaps are loaded. Hifst can read tokenized Russian text and produce tokenized English translations (see options <code>--prepro.wordmap.load</code> and <code>--postpro.wordmap.load</code>). Also, to ensure that CYK parser never fails, out of vocabulary (OOV) words must be detected (<code>--ssgrammar.addoovs.enable</code>) and sentence markers (<code>&lt;s&gt;</code>,<code>&lt;/s&gt;</code>) have to be added on the fly, as the shallow grammar relies on them (i.e. <code>S 1 1</code>).</p>
<p>With the <code>hifst-client.${TGTBINMK}.bin</code> binary, we can read Russian tokenized text (<code>RU/RU.tune</code>) and submit translation requests to the server. The output is stored in a file specified by the client tool (<code>--target.store</code>). </p>
<pre class="fragment">&gt; sleep 60 # make sure to wait for the server to finish loading, otherwise clients will fail
&gt; hifst-client.${TGTBINMK}.bin --config=configs/CF.baseline.client --range=200:5:300 --target.store=output/exp.clientserver/translation1.txt &amp;&gt; log/log.client1 &amp;
# Connect to localhost, port=1205 and translate a bunch of sentences. Lets do this in background, just for fun
# Note that the localhost setting is in the config file; this can point to another machine, of course
&gt; pid2=$!

&gt; hifst-client.${TGTBINMK}.bin --config=configs/CF.baseline.client --range=1:50,100,1300 --target.store=output/exp.clientserver/translation2.txt &amp;&gt; log/log.client2 &amp;
# In the meantime, we request another 52 translations...
&gt; wait $pid2

&gt; kill -9 $pid
# We are finished -- kill the server

&gt; head -5 output/exp.clientserver/translation2.txt
parliament does not support the amendment , which gives you the freedom of tymoshenko
amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences .
the verdict is not final , the court will consider an appeal of tymoshenko in december .
a proposal to repeal article 365 of the code of criminal procedure , according to which was convicted former prime minister , was supported by the 147 members of parliament .
victory in libya</pre> </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Sat Sep 19 2015 19:03:14 for Cambridge SMT System by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.6 </li>
  </ul>
</div>
</body>
</html>
