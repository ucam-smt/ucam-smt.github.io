<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Cambridge SMT System: Tutorial.010.basic.md Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Cambridge SMT System
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('Tutorial_8010_8basic_8md.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Tutorial.010.basic.md</div>  </div>
</div><!--header-->
<div class="contents">
<a href="Tutorial_8010_8basic_8md.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;Translation and FST Operations {#basictrans}</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;================================================</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;**Notes:** </div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;   * Make sure that environment variables are set as described in \ref tutorial_install and \ref hifst_paths.</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;   * Make sure that the language models are downloaded and uncompressed into the `$DEMO/M/` directory.</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;   * Make sure that the wordmaps are uncompressed in the `$DEMO/wmaps/` directory</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;   * Make sure you&#39;re in the `$DEMO` directory.</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;   &gt; &gt; cd $DEMO</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;\section basic_trans Basic Translation Operations</div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;</div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;The first demonstration exercise is to generate translations of</div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;integer-mapped Russian text using the translation grammar and English</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;n-gram language model provided.  HiFST is configured to generate</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;one-best translation hypotheses as well as translation lattices.</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;The baseline configuration file is `configs/CF.baseline`, which also contains</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;comments giving brief explanations of HiFST options.</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;The following command will translate the first 2 lines in the Russian</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;integer-mapped file `RU/RU.set1.idx`:</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;    # Run HiFST</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;    &gt; mkdir log</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline &amp;&gt; log/log.baseline</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;The log file output can be viewed as:</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;    &gt; tail -n 11 log/log.baseline</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;    Fri Apr  3 12:48:16 2015: run.INF:loading LM=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;    Fri Apr  3 12:48:16 2015: run.INF:Stats for Sentence 1: local pruning, number of times=0</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;    Fri Apr  3 12:48:16 2015: run.INF:End Sentence ******************************************************</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;    Fri Apr  3 12:48:16 2015: run.INF:Writing lattice 1 to ... output/exp.baseline/LATS/1.fst.gz</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;    Fri Apr  3 12:48:16 2015: run.INF:Translation 1best is: 1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 </div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;    Fri Apr  3 12:48:16 2015: run.INF:=====Translate sentence 2:1 16055 3 102 5182 66 18 23602 12611 5 6522 2377 3431 3 98 52858 61 46 2140 4422 15871 25 67408 17658 26 1731 19663 4 2</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;    Fri Apr  3 12:48:16 2015: run.INF:Stats for Sentence 2: local pruning, number of times=0</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;    Fri Apr  3 12:48:22 2015: run.INF:End Sentence ******************************************************</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;    Fri Apr  3 12:48:22 2015: run.INF:Writing lattice 2 to ... output/exp.baseline/LATS/2.fst.gz</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;    Fri Apr  3 12:48:22 2015: run.INF:Translation 1best is: 1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2 </div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;    Fri Apr  3 12:48:22 2015: ~MainClass.INF:hifst.O2.bin ends!</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;The best scoring translation hypotheses are given in integer-mapped</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;form, e.g. for the first  Russian sentence, the best-scoring translation</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;hypothesis is</div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;    run.INF:Translation 1best is: 1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;\section printing_hyps Printing the 1-Best Hypotheses</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;</div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;The  configuration file `configs/CF.baseline` instructs HiFST to write its 1-best</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;translations to the output file `output/exp.baseline/hyps`</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;(see the `target.store=output/exp.baseline/hyps` specification in the config file).  The contents of</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;this file should agree with the Translation 1best entries in the log file</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;(compare these results to the entries in the log file, above):</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;     &gt; cat output/exp.baseline/hyps</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;     1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 </div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;     1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;The FST Archive (FAR) command line tools (in the \ref OpenFst FAR [extensions](http://openfst.org/twiki/bin/view/FST/FstExtensions))</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;can be used to load the English wordmap (`wmaps/wmt13.en.wmap`) and print the hypotheses in readable form:</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;    &gt; farcompilestrings --entry_type=line output/exp.baseline/hyps | farprintstrings --symbols=wmaps/wmt13.en.wmap</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;    &lt;s&gt; amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt;</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;Note that loading the English wordmap can be time consuming due to its</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;size.  Ideally, in processing multiple translation hypotheses, the</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;wordmap should be loaded only once, rather than once for each</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;sentence.</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;\section basic_latshyps Extracting the Best Translation from a Lattice</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;The configuration file also directs HiFST to write translation</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;lattices to `output/exp.baseline/LATS/?.fst.gz` (see the</div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;`hifst.lattice.store=output/exp.baseline/LATS/?.fst.gz` specification in the config file).  Note the</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;use of the placeholder &#39;`?`&#39; in the argument `.../LATS/?.fst.gz` . The</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;placeholder is replaced by the line number of sentence being</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;translated, e.g. so that `.../LATS/2.fst.gz` is a weighted finite state</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;transducer (WFST) containing</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;translations of the second line in the source text file.  Note also</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;the use of the &#39;`.gz`&#39; extension: when this is provided, lattices are</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;written as gzipped files.</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;</div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;\ref OpenFst operations can be used to compute the **shortest path** through each of these output lattices, and the results should agree</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;with the top-scoring hypotheses in the file `output/exp.baseline/hyps` and the log file:</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;    &gt; echo `zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint | awk &#39;{print $3}&#39;`</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;    1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;    &gt; echo `zcat output/exp.baseline/LATS/2.fst.gz | fstshortestpath | fsttopsort | fstprint | awk &#39;{print $3}&#39;`</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;    1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;The English wordmap can be supplied to `fstprint` to convert from integer mapped strings to English:</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;    &gt; echo `zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint --isymbols=wmaps/wmt13.en.wmap | awk &#39;{print $3}&#39;`</div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;    &gt; echo `zcat output/exp.baseline/LATS/2.fst.gz | fstshortestpath | fsttopsort | fstprint --isymbols=wmaps/wmt13.en.wmap | awk &#39;{print $3}&#39;`</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;    &lt;s&gt; amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt;</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;For convenience, the HiFST `printstring` utility programme gathers all these</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;operations into a single binary:</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --range=1:2 --label-map=wmaps/wmt13.en.wmap --input=output/exp.baseline/LATS/?.fst.gz --semiring=lexstdarc         </div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;    ...</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;    &lt;s&gt; amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt; </div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;    ...</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;**Note** the use of the `range=1:2` command line option, which</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;specifies which fsts are to be processed, and also that wordmap file</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;is loaded only once, which can speed operations.  **Note** also that</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;the output fsts are in the `lexstdarc` semiring, described later.</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;\subsection fst_shortestpath_discuss OpenFst ShortestPath Operations</div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;The above \ref OpenFst operations do the following:</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;</div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;-# `zcat` pipes the HiFST output lattice to the \ref OpenFst [Shortest Path](http://openfst.org/twiki/bin/view/FST/ShortestPathDoc) tool which produces an FST containing only the shortest path in the lattice</div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;-# the \ref OpenFst [Topological Sort](http://openfst.org/twiki/bin/view/FST/TopSortDoc) operation renumbers the state IDs so that all arcs are links from lower to higher state IDs</div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;-# the \ref OpenFst [fstprint](http://openfst.org/twiki/bin/view/FST/FstQuickTour#Printing_Drawing_and_Summarizing) operation reads the English wordmap, for the arc input symbols,  and traverses the input fst, writing each arc as it is encountered;  TopSort ensures these arcs are written in the correct order</div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;-# the awk operation prints only the words on the arcs</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;-# wrapping everything inside echo generates a single string</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;To see what is produced at the various steps in the pipeline:</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;Input lattice:</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;     &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstinfo | head -6</div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;     fst type                                          vector</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;     arc type                                          tropical_LT_tropical</div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;     input symbol table                                none</div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;     output symbol table                               none</div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;     # of states                                       656</div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;     # of arcs                                         1513</div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;</div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;Shortest Path:</div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;</div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;    &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fstprint</div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;    16     15      1 1 -2.68554688,-2.68554688</div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;    0</div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;    1         0       2       2        2.83719015,-2.34277344</div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;    2         1       23899   23899    10.2336502,0</div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;    3         2       6       6        1.83065951,0</div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;    4         3       425     425      4.19441986,0</div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;    5         4       3       3        -3.28652811,-5.1171875</div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;    6         5       48      48       3.01827216,0</div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;    7         6       1145    1145     4.64596272,0</div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;    8         7       25      25       1.83065951,0</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;    9         8       4       4        0.548432946,-1.28222656</div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;    10     9       245     245      5.13327551,-0.48828125</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;    11     10      3       3        0.961314559,0</div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;    12     11      103     103      4.64596272,0</div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;    13     12      20      20       0.961314559,0</div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;    14     13      135     135      1.43660688,-4.65039062</div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;    15     14      50      50       6.39422989,-2.88476562</div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;</div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;**Note** that the paired weights are described in \ref basic_scores.</div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;</div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;Topologically Sorted Shortest Path:</div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;</div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;    &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint</div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;    1      1       1          1     -2.68554688,-2.68554688</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;    1      2       50      50    6.39422989,-2.88476562</div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;    2      3       135     135   1.43660688,-4.65039062</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;    3      4       20      20    0.961314559,0</div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;    4      5       103     103   4.64596272,0</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;    5      6       3       3     0.961314559,0</div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;    6      7       245     245   5.13327551,-0.48828125</div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;    7      8       4       4     0.548432946,-1.28222656</div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;    8      9       25      25    1.83065951,0</div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;    9      10      1145    1145  4.64596272,0</div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;    10     11      48      48    3.01827216,0</div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;    11     12      3       3     -3.28652811,-5.1171875</div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;    12     13      425     425   4.19441986,0</div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;    13     14      6       6     1.83065951,0</div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;    14     15      23899   23899 10.2336502,0</div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;    15     16      2       2     2.83719015,-2.34277344</div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;    16</div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;</div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;</div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;Toplogically Sorted Shortest Path, with English words replacing the arc input symbols</div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;</div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;    &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstprint --isymbols=wmaps/wmt13.en.wmap --acceptor</div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;    0      1    &lt;s&gt;         -2.68554688,-2.68554688</div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;    1      2    parliament  6.39422989,-2.88476562</div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;    2      3    does        1.43660688,-4.65039062</div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;    3      4    not         0.961314559,0</div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;    4      5    support     4.64596272,0</div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;    5      6    the         0.961314559,0</div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;    6      7    amendment   5.13327551,-0.48828125</div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;    7      8    ,           0.548432946,-1.28222656</div><div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;    8      9    which       1.83065951,0</div><div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;    9      10   gives       4.64596272,0</div><div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;    10     11   you         3.01827216,0</div><div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;    11     12   the         -3.28652811,-5.1171875</div><div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;    12     13   freedom     4.19441986,0</div><div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;    13     14   of          1.83065951,0</div><div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;    14     15   tymoshenko  10.2336502,0</div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;    15     16   &lt;/s&gt;        2.83719015,-2.34277344</div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;    16</div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;</div><div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;</div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;</div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;</div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;\section basic_nbest Extracting N-Best Translations from Lattices</div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;</div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;The `printstrings` can also can print the top-N hypotheses, using</div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;the </div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;\ref OpenFst [Shortest Path](http://openfst.org/twiki/bin/view/FST/ShortestPathDoc)</div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;operation, with its n-shortest path option:</div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;</div><div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --nbest=10 --unique --input=output/exp.baseline/LATS/1.fst.gz </div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;    ...</div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;    1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 </div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;    1 50 135 20 103 34 245 4 25 1145 48 3 425 6 23899 2 </div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;    1 3 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 </div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;    1 50 311 20 103 3 245 4 25 1145 48 3 425 6 23899 2 </div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;    1 50 135 20 103 3 245 4 25 1145 48 3 425 7 23899 2 </div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;    1 50 135 20 103 3 245 4 25 1145 48 425 6 23899 2 </div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;    1 50 135 20 103 3 245 4 25 1145 425 6 23899 2 </div><div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;    1 50 135 20 103 3 245 4 25 1145 408 23899 2 </div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;    1 50 135 20 103 3 245 4 25 1145 3 425 6 23899 2 </div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;    1 50 135 20 103 3 245 4 25 1145 48 408 23899 2 </div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;    ...</div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;</div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;With the English wordmap,  `printstrings` will map the integer representation to English text:</div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;</div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --nbest=10 --unique --input=output/exp.baseline/LATS/1.fst.gz --label-map=wmaps/wmt13.en.wmap</div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;    ...</div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;    &lt;s&gt; parliament does not support an amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;    &lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;    &lt;s&gt; parliament did not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom to tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives free tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you free tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;    ...</div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;</div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;It often happens that a translation hypothesis can be produced by multiple derivations (i.e. rule sequences),</div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;so the top scoring hypotheses need not be unique. For example, omitting the `--unique` shows repetitions among the top hypotheses:</div><div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;</div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --nbest=10 --input=output/exp.baseline/LATS/1.fst.gz --label-map=wmaps/wmt13.en.wmap</div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;    ...</div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; </div><div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;    ...</div><div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;</div><div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;\section vector_feature_grammars Weight Vectors and Feature Vectors</div><div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;</div><div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;HiFST computes a translation score `S(e)` for each hypothesis `e` by applying a weight vector `P` to a feature vector `F(e)`.</div><div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;The relationship of feature vectors and scores at the hypothesis level is as follows:</div><div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;- Suppose there are m language models, with weights s_1 ...,s_m .</div><div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;  - These weights are specified by the HiFST parameters `lm.featureweights=s_1,s_2,..,s_m`</div><div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;- Suppose there are n-dimensional feature vectors for each rule in the translation grammar,</div><div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;  - The weights to be applied are specified by the HiFST parameters `grammar.featureweights=w_1,..,w_n`</div><div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;- A feature weight vector is formed as P = [s_1 ... s_m w_1 ... w_n] </div><div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;- A translation hypothesis e has a feature vector F(e) = [lm_1(e) ... lm_m(e) f_1(e) ... f_n(e)]</div><div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;  - lm_i(e): the i-th language model score for e</div><div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;  - f_j(e): j-th grammar feature (see Section 3.2.1, [\ref deGispert2010])</div><div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;- The score of translation hypothesis e can be found as S(e) = F(e) . P (dot product)</div><div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;</div><div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;\section weight_feature_vector_examples Applying Weight Vectors in Translation</div><div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;</div><div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;As an example,  </div><div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;the translation grammar `G/rules.shallow.vecfea.gz` has unweighted 11-dimensional (`n=11`) feature vectors associated with each rule:</div><div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;</div><div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;    &gt; gzip -d -c G/rules.shallow.vecfea.gz | head -3</div><div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;    V 3 4 0.223527 0.116794 -1 -1 0 0 0 0 -1 1.268789 0.687159</div><div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;    V 3 4_3 3.333756 0.338107 -2 -1 0 0 0 0 -1 1.662178 3.363062</div><div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;    V 3 8 3.74095 3.279819 -1 -1 0 0 0 0 -1 3.741382 2.271445</div><div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;</div><div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;We have run lattice MERT (\ref lmert) to generate a parameter weight vector for this grammar with these features and the language model `M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap`</div><div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;</div><div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;    &gt; P=1.0,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136</div><div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;</div><div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;Note that `P` is 12-dimensional: the weighting of the language model score is `1.0`, and the weights applied to the grammar feature vectors are </div><div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;</div><div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;    GW=0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136</div><div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;</div><div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;There are (at least) three different ways to apply the feature weights to the unweighted feature vectors in the grammar:</div><div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;</div><div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;**Feature weights can be applied to the grammar, prior to translation**:</div><div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;</div><div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;    &gt; mkdir -p tmp/</div><div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;    &gt; gzip -dc G/rules.shallow.vecfea.gz | ./scripts/weightgrammar -w=$GW | gzip  &gt; tmp/rules.shallow.RS.gz</div><div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.nogrammar --grammar.load=tmp/rules.shallow.RS.gz --target.store=tmp/hyps.1</div><div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;</div><div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;**Feature weights can be applied in translation**, using the options `--grammar.featureweights=$GW` and `--lm.featureweights=1.0`.  </div><div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;</div><div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;HiFST loads the grammar with unweighted feature vectors, and applies the feature weights on-the-fly:</div><div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;</div><div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.nogrammar --grammar.load=G/rules.shallow.vecfea.gz --grammar.featureweights=$GW --lm.featureweights=1.0 --target.store=tmp/hyps.2 </div><div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;</div><div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;**Feature weights can be applied in translation**, using the `--featureweight=$P` option.  </div><div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;</div><div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;HiFST loads the grammar with unweighted feature vectors, and applies the feature weights on-the-fly. HiFST automatically determines which elements of `P` should be applied to the language model scores, and which should be applied to the unweighted feature vectors in the translation grammar:</div><div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;</div><div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;    &gt; hifst.${TGTBINMK}.bin --featureweights=$P --config=configs/CF.nogrammar --target.store=tmp/hyps.3 --grammar.load=G/rules.shallow.vecfea.gz</div><div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;</div><div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;</div><div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;These three alternative methods should yield identical results (to verify, compare `tmp/hyps.[123]`).</div><div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;Note that the last alternative is useful for iterative parameter estimation procedures, such as MERT.</div><div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;</div><div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;\section basic_scores Lexicographic Semirings: Translation Grammar and Language Model Scores </div><div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;</div><div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;HiFST follows the formalism in which rule probabilities are represented as arc weights (see Section 2 of [\ref deGispert2010]).</div><div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;A rule with probability *p* is represented as a negative log probability, i.e.</div><div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;</div><div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;     X -&gt; &lt; A , B &gt; / - log(p)</div><div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;</div><div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;with n-gram language model scores encoded similarly,</div><div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;as costs  -log P(w|h)  </div><div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;for word *w* with LM history *h*. Costs are accumulated at the</div><div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;path level, so that the shortest path through the output FSA accepts</div><div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;the highest scoring hypothesis under the translation grammar and the</div><div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;language model (with feature weights applied as described in \ref vector_feature_grammars).  </div><div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;With this mapping of scores to costs, </div><div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;the \ref OpenFst [ShortestPath](http://openfst.org/twiki/bin/view/FST/ShortestPathDoc) can be used to extract the best scoring</div><div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;hypothesis under the tropical semiring.</div><div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;</div><div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;HiFST uses a **lexicographic semiring  of two tropical weights** [\ref Roark2011].  For example,</div><div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;</div><div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;     &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstinfo | head -2</div><div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;     fst type                                          vector</div><div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;     arc type                                          tropical_LT_tropical</div><div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;</div><div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;     &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstprint | head -n 10</div><div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;     0 1    1       1   -2.68554688,-2.68554688</div><div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;     1 4    170     170 11.4819975,4.25097656</div><div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;     1 3    50      50  6.39422989,-2.88476562</div><div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;     1 2    3       3   1.87243128,-2.70800781</div><div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;     1 197  50      50  3.70087051,-5.578125</div><div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;     1 196  3       3   -0.820928097,-5.40136719</div><div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;     1 456  50      50  7.04462051,-2.234375</div><div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;     1 455  3       3   1.72790003,-2.85253906</div><div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;     1 195  50      50  6.01825333,-3.26074219</div><div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;     1 81   170     170 5.89898968,-1.33203125</div><div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;</div><div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;The `arc type` of `tropical_LT_tropical` indicates that the</div><div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;lexicographic semiring is a pair of tropical weights.  As produced by</div><div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;HiFST, the pairs of tropical weights are (`G+M`, `G`), where the first</div><div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;weight (`G+M`) contains the complete translation score (the translation</div><div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;grammar score `G` + the language model score `M`), and the second weight `G` only</div><div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;contains the translation grammar score.  The advantage of using the</div><div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;lexicographic semiring in this way is that the language model scores</div><div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;can be removed and reapplied very efficiently (see \ref</div><div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;rescoring_lm).</div><div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;</div><div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;</div><div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;In the above example, scores are distributed over the arcs in the FST.  </div><div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;The \ref OpenFst [Push](http://openfst.org/twiki/bin/view/FST/PushDoc)  operation can be used to accumulate weights at the path level within the shortest path fst:</div><div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;</div><div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;     &gt; zcat output/exp.baseline/LATS/1.fst.gz | fstshortestpath | fsttopsort | fstpush --push_weights --to_final | fstprint --isymbols=wmaps/wmt13.en.wmap --acceptor</div><div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;     0      1    &lt;s&gt;</div><div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;     1      2    parliament</div><div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;     2      3    does</div><div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160;     3      4    not</div><div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160;     4      5    support</div><div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160;     5      6    the</div><div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160;     6      7    amendment</div><div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160;     7      8    ,</div><div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160;     8      9    which</div><div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160;     9      10   gives</div><div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160;     10     11   you</div><div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;     11     12   the</div><div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;     12     13   freedom</div><div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;     13     14   of</div><div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;     14     15   tymoshenko</div><div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;     15     16   &lt;/s&gt;</div><div class="line"><a name="l00384"></a><span class="lineno">  384</span>&#160;     16     42.6998749,-19.4511719</div><div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;</div><div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;</div><div class="line"><a name="l00387"></a><span class="lineno">  387</span>&#160;In this example, `(G+M, G)` is `(42.6998749,-19.4511719)`.  The first</div><div class="line"><a name="l00388"></a><span class="lineno">  388</span>&#160;component, 42.6998749, contains the sum of the translation grammar</div><div class="line"><a name="l00389"></a><span class="lineno">  389</span>&#160;scores and the language model - this is the score assigned to the</div><div class="line"><a name="l00390"></a><span class="lineno">  390</span>&#160;hypotheses by the decoder.  The second component, `-19.4511719`, is the</div><div class="line"><a name="l00391"></a><span class="lineno">  391</span>&#160;translation grammar score alone, i.e. contains the score assigned to</div><div class="line"><a name="l00392"></a><span class="lineno">  392</span>&#160;the hypotheses under the translation grammar without the language</div><div class="line"><a name="l00393"></a><span class="lineno">  393</span>&#160;model (see Section 5.1 of [\ref Allauzen2014]).  The lexicographic</div><div class="line"><a name="l00394"></a><span class="lineno">  394</span>&#160;semiring is such that these scores are computed correctly at the path</div><div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;level:</div><div class="line"><a name="l00396"></a><span class="lineno">  396</span>&#160;</div><div class="line"><a name="l00397"></a><span class="lineno">  397</span>&#160;- The cost of the shortest path found by [ShortestPath](http://openfst.cs.nyu.edu/twiki/bin/view/FST/ShortestPathDoc) is that of the best hypothesis under the sum of the translation grammar score and the language model score(s)</div><div class="line"><a name="l00398"></a><span class="lineno">  398</span>&#160;- In the lexicographic semiring, when the path weight is pushed to the final state:</div><div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;    - the first weight component is the correct combined translation grammar and language model score</div><div class="line"><a name="l00400"></a><span class="lineno">  400</span>&#160;    - the second weight component is the best grammar score over all possible derivations that could have generated this hypothesis</div><div class="line"><a name="l00401"></a><span class="lineno">  401</span>&#160;</div><div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;</div><div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;The HiFST utility `printstrings` also works with the lexicographic semiring, and gives the same results as using the ShortestPath and Push operations:</div><div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;</div><div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --input=output/exp.baseline/LATS/1.fst.gz --label-map=wmaps/wmt13.en.wmap --semiring=lexstdarc --weight</div><div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;    ...</div><div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;&lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;  42.6999,-19.4512</div><div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;    ...</div><div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;</div><div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;</div><div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;Printing the top 5 hypotheses shows that hypotheses are scored and ranked under the combined grammar and language model score `G+M`:</div><div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;</div><div class="line"><a name="l00413"></a><span class="lineno">  413</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --input=output/exp.baseline/LATS/1.fst.gz --label-map=wmaps/wmt13.en.wmap --semiring=lexstdarc --weight --nbest=5 --unique</div><div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;    ...</div><div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;  42.7003,-19.4512</div><div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;    &lt;s&gt; parliament does not support an amendment , which gives you the freedom of tymoshenko &lt;/s&gt;   44.2802,-20.8945</div><div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;    &lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      45.4819,-19.9385</div><div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;    &lt;s&gt; parliament did not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;   45.7096,-15.5635</div><div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom to tymoshenko &lt;/s&gt;  45.8136,-19.0361</div><div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;</div><div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;</div><div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;</div><div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;\section basic_toplevelpruning Admissible Pruning</div><div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;</div><div class="line"><a name="l00425"></a><span class="lineno">  425</span>&#160;HiFST can prune translation lattices prior to saving them to disk; this is most often a practical necessity.  Pruning is</div><div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;done using the \ref OpenFst [Prune](http://openfst.cs.nyu.edu/twiki/bin/view/FST/PruneDoc) operation.</div><div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;Pruning in this case is **admissible**, since it is</div><div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;performed after grammar and language model scores have been</div><div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;completely applied.  Low-scoring hypotheses are discarded, but no search</div><div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;errors are introduced by this pruning.  This is also referred to as **top-level pruning**, as described in detail in Section 2.2.2, [\ref deGispert2010].</div><div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;</div><div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;Top-level pruning is controlled by the `--hifst.prune` option.  In the</div><div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;previous examples, `--hifst.prune` was set to 9. If we use the default</div><div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;(3.40282347e+38), then the output lattice size becomes very large. For</div><div class="line"><a name="l00435"></a><span class="lineno">  435</span>&#160;example, compare lattices in `exp1/` generated with `prune=9` vs.</div><div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;unpruned lattices in `exp2/`:</div><div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;</div><div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline.outputnoprune &amp;&gt; log/log.baseline.outputnoprune</div><div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;    &gt; du -sh output/exp.baseline/LATS/1.fst.gz output/exp.baseline.outputnoprune/LATS/1.fst.gz</div><div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;    12K    output/exp.baseline/LATS/1.fst.gz</div><div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;    1.1M   output/exp.baseline.outputnoprune/LATS/1.fst.gz</div><div class="line"><a name="l00442"></a><span class="lineno">  442</span>&#160;    &gt; du -sh output/exp.baseline/LATS/2.fst.gz output/exp.baseline.outputnoprune/LATS/2.fst.gz</div><div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;    148K   output/exp.baseline/LATS/2.fst.gz</div><div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;    16M    output/exp.baseline.outputnoprune/LATS/2.fst.gz</div><div class="line"><a name="l00445"></a><span class="lineno">  445</span>&#160;</div><div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;</div><div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;The \ref OpenFst</div><div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;[fstinfo](http://openfst.org/twiki/bin/view/FST/FstQuickTour#Printing_Drawing_and_Summarizing)</div><div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;command also indicates much larger outputs:</div><div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;</div><div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;    &gt; zcat output/exp.baseline/LATS/2.fst.gz | fstinfo | grep \# | head -2</div><div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;    # of states                                       5833</div><div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;    # of arcs                                         22845</div><div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;</div><div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;    &gt; zcat output/exp.baseline.outputnoprune/LATS/2.fst.gz | fstinfo | grep \# | head -2</div><div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;    # of states                                       172843</div><div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;    # of arcs                                         2481006</div><div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;</div><div class="line"><a name="l00459"></a><span class="lineno">  459</span>&#160;</div><div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;The unpruned lattices are much bigger, and contain many translation</div><div class="line"><a name="l00461"></a><span class="lineno">  461</span>&#160;hypotheses, although the top scoring hypotheses should be unchanged</div><div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;by this form of pruning, as is the case in this example:</div><div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;</div><div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;    &gt; head output/exp.baseline/hyps output/exp.baseline.outputnoprune/hyps</div><div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;    ==&gt; output/exp.baseline/hyps &lt;==</div><div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;    1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 </div><div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;    1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2 </div><div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;</div><div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;    ==&gt; output/exp.baseline.outputnoprune/hyps &lt;==</div><div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;    1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 </div><div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;    1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 4 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2 </div><div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;</div><div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;</div><div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;\section lpruning Inadmissible Pruning</div><div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;</div><div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;Inadmissible pruning, or **local pruning**, controls processing speed and memory use during translation.  </div><div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;Only enough details are reviewed here to describe how HiFST performs pruning in search;</div><div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;for a detailed discussion of local pruning and pruning in search, see Section 2.2.2 of [\ref deGispert2010].  </div><div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;</div><div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;Given a translation grammar and a source language sentence, HiFST first</div><div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;constructs a Recursive Transition Network (RTN) representing</div><div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;the translation hypotheses [\ref Iglesias2009a, \ref Iglesias2011].</div><div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;This is done as part of a modified CYK algorithm used to parse the</div><div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;source sentence under the translation grammar.</div><div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;The RTN is then *expanded* to an equivalent WFSA via the \ref OpenFst [Replace](http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc)</div><div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;operation. This WFSA contains the translation hypotheses along with their scores under the translation grammar.  </div><div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;We refer to this as the `top-level&#39; WFSA, because it is associated with the top-most cell in the CYK grid.</div><div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;This top-level WFSA can be pruned after composition with the language</div><div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;model, as described in the discussion of \ref basic_toplevelpruning.</div><div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;We refer to this as *exact search* or *exact translation*.</div><div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;In exact translation, no translation hypotheses are discarded prior to applying the complete grammar and language model scores.</div><div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;</div><div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;Exact translation can be done under some combinations of translation grammars, language models, and language pairs.</div><div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;In particular, the \ref tgrammars_shallow  were designed for exact search.</div><div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;However attempting exact translation under many translation grammars would cause</div><div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;either the [Replace](http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc)</div><div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;operation or the subsequent language model composition to</div><div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;become computationally intractable.</div><div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;We therefore have developed a pruning strategy that prunes the RTN during its construction.</div><div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;</div><div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;The RTN created by HiFST can be described as follows:</div><div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;  - \f$X\f$ is the set of non-terminals in the translation grammar, with \f$S\f$ as the root</div><div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;  - \f$\Sigma\f$ is the target language vocabulary, i.e. the terminals in the target language</div><div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;  - \f$I\f$ is the length of the source sentence \f$s\f$, i.e. \f$s = s_0...s_{I - 1}\f$</div><div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;  - A new set of non-terminals is defined as \f$N = \{ (x,i,j) : x \in X , 0 &lt;= i &lt;= j &lt; I \}\f$</div><div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;     - Note that \f$(S,0,I-1) \in N\f$</div><div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;  - \f$(T_u)_{u \in N}\f$, is a family of WFSAs with input alphabet \f$\Sigma \cup N\f$</div><div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;     - Each \f$T_u\f$ with \f$u = (x, i, j)\f$, is a WFSA that describes all applications of translation rules with left-hand side non-terminal \f$x\f$ that</div><div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;span the substring \f$s_i ... s_j\f$</div><div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;     - \f$T_u\f$ is associated with the CYK grid cell associated with source space \f$[i,j]\f$ and headed by non-terminal \f$x\f$</div><div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;  - The top-level RTN is defined as \f$R_{(S,0,I-1)} = (N, \Sigma, (T_u)_{u \in N}, (S,0,I-1))\f$.</div><div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160;     - The root symbol of this RTN is \f$(S,0,I-1)\f$.</div><div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160;     - The WFSA \f$T_{(S,0,I-1)}\f$ represents all applications of translation rules that span the entire sentence and are rooted with non-terminal \f$S\f$.</div><div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160;</div><div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160;Exact translation is achieved if every \f$T_u\f$ is complete (i.e. if no pruning is done) prior to the \ref OpenFst [Replace](http://openfst.org/twiki/bin/view/FST/ReplaceDoc)</div><div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160;operation</div><div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160;on the RTN \f$R_{(S,0,I-1)}\f$. This produces a WFSA that contains all translations</div><div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;that can be produced under the translation grammar.  </div><div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;</div><div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;The RTN pruning strategy relies on noting</div><div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;that each of the WFSAs \f$T_{u&#39;}\f$, \f$u&#39; = (x&#39;, i&#39;, j&#39;)\f$, also defines an RTN \f$R_{u&#39;}\f$, as follows:</div><div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;  - Define a subset of non-terminals \f$N&#39; = \{ (x,i,j) : x \in X , i&#39; &lt;= i &lt;=j &lt; j&#39; \}\f$ , i.e. \f$N&#39; \subset N\f$</div><div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;  - \f$R_{u&#39;} = (N&#39;, (T_u)_{u \in N&#39;}, (x&#39;, i&#39;, j&#39;) )\f$</div><div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;      - The root symbol of this RTN is \f$(x&#39;, i&#39;, j&#39;)\f$</div><div class="line"><a name="l00525"></a><span class="lineno">  525</span>&#160;</div><div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;The [Replace](http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc)</div><div class="line"><a name="l00527"></a><span class="lineno">  527</span>&#160;operation can be applied to the RTNs \f$R_{u&#39;}\f$ to produce an WFSA</div><div class="line"><a name="l00528"></a><span class="lineno">  528</span>&#160;containing all translations of the source string \f$S_{i&#39;} ... S_{j&#39;}\f$ using</div><div class="line"><a name="l00529"></a><span class="lineno">  529</span>&#160;derivations rooted in the non-terminal \f$x&#39;\f$.  This WFSA can be pruned and used in place of the original \f$T_{u&#39;}\f$.</div><div class="line"><a name="l00530"></a><span class="lineno">  530</span>&#160;</div><div class="line"><a name="l00531"></a><span class="lineno">  531</span>&#160;Because of the possibility of search errors we refer to this as &#39;local pruning&#39; or inadmissible pruning.</div><div class="line"><a name="l00532"></a><span class="lineno">  532</span>&#160;There is the possibility that pruning any of the \f$T_u\f$ may possibly cause some good translations to be discarded.</div><div class="line"><a name="l00533"></a><span class="lineno">  533</span>&#160;For this reason it is important to tune the pruning strategy for the translation grammar and language model.</div><div class="line"><a name="l00534"></a><span class="lineno">  534</span>&#160;Once pruning has been set, the benefits are</div><div class="line"><a name="l00535"></a><span class="lineno">  535</span>&#160;  - faster creation of the top-level WFSA via the [Replace](http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc) operation</div><div class="line"><a name="l00536"></a><span class="lineno">  536</span>&#160;  - faster composition of the translation WFSA with the language model</div><div class="line"><a name="l00537"></a><span class="lineno">  537</span>&#160;  - less memory used in RTN construction and language model composition</div><div class="line"><a name="l00538"></a><span class="lineno">  538</span>&#160;</div><div class="line"><a name="l00539"></a><span class="lineno">  539</span>&#160;Local pruning should be done under the combined</div><div class="line"><a name="l00540"></a><span class="lineno">  540</span>&#160;grammar and the language model scores, rather than under the translation grammar scores alone</div><div class="line"><a name="l00541"></a><span class="lineno">  541</span>&#160;alone. However, the LM used in local pruning can be relatively weak. For</div><div class="line"><a name="l00542"></a><span class="lineno">  542</span>&#160;example, if the main language model used in translation is a 4-gram,</div><div class="line"><a name="l00543"></a><span class="lineno">  543</span>&#160;perhaps a 3-gram or even a bigram language model could be used in</div><div class="line"><a name="l00544"></a><span class="lineno">  544</span>&#160;local pruning.  Using a smaller language model will make pruning faster, as</div><div class="line"><a name="l00545"></a><span class="lineno">  545</span>&#160;will an efficient scheme to remove the scores of the language models used in pruning.  </div><div class="line"><a name="l00546"></a><span class="lineno">  546</span>&#160;The lexicographic semiring, see \ref basic_scores, makes this last operation easy.</div><div class="line"><a name="l00547"></a><span class="lineno">  547</span>&#160;</div><div class="line"><a name="l00548"></a><span class="lineno">  548</span>&#160;\subsection local_prune Local Pruning Algorithm</div><div class="line"><a name="l00549"></a><span class="lineno">  549</span>&#160;</div><div class="line"><a name="l00550"></a><span class="lineno">  550</span>&#160;HiFST monitors the size of the \f$T_u\f$ during translation. Any of these automata that exceed specified thresholds</div><div class="line"><a name="l00551"></a><span class="lineno">  551</span>&#160;are converted to WFSAs and pruned.  Subsequent expansion of the RTN  \f$R_{(S,0,I-1)}\f$ is then done with respect to the pruned</div><div class="line"><a name="l00552"></a><span class="lineno">  552</span>&#160;versions of \f$T_u\f$.</div><div class="line"><a name="l00553"></a><span class="lineno">  553</span>&#160;</div><div class="line"><a name="l00554"></a><span class="lineno">  554</span>&#160;</div><div class="line"><a name="l00555"></a><span class="lineno">  555</span>&#160;Local pruning is controlled via the following HiFST parameters:</div><div class="line"><a name="l00556"></a><span class="lineno">  556</span>&#160;</div><div class="line"><a name="l00557"></a><span class="lineno">  557</span>&#160;    hifst.localprune.enable=yes # must be set to activate local pruning</div><div class="line"><a name="l00558"></a><span class="lineno">  558</span>&#160;    hifst.localprune.conditions=NT_1,span_1,size_1,threshold_1,...,NT_N,span_N,size_N,threshold_N</div><div class="line"><a name="l00559"></a><span class="lineno">  559</span>&#160;    hifst.localprune.lm.load=lm_1,...lm_K</div><div class="line"><a name="l00560"></a><span class="lineno">  560</span>&#160;    hifst.localprune.lm.featureweights=scale_1,...,scale_K</div><div class="line"><a name="l00561"></a><span class="lineno">  561</span>&#160;    hifst.localprune.lm.wps=wp_1,...,wp_K</div><div class="line"><a name="l00562"></a><span class="lineno">  562</span>&#160;</div><div class="line"><a name="l00563"></a><span class="lineno">  563</span>&#160;In the above, an arbitrary number N of tuples (`NT_n`, `span_n`, `size_n`, `threshold_n`) can be provided;</div><div class="line"><a name="l00564"></a><span class="lineno">  564</span>&#160;similarly, an arbitrary number K of language model parameters (`lm_k`, `scale_k`, `wp_k`) can also be used in pruning.</div><div class="line"><a name="l00565"></a><span class="lineno">  565</span>&#160;</div><div class="line"><a name="l00566"></a><span class="lineno">  566</span>&#160;Pruning is applied during construction of the RTN, as follows:</div><div class="line"><a name="l00567"></a><span class="lineno">  567</span>&#160;</div><div class="line"><a name="l00568"></a><span class="lineno">  568</span>&#160;  - If any \f$T_u\f$ satisfies the following conditions for any parameter set (`NT_n`, `span_n`, `size_n`, `threshold_n`), n=1,...,N</div><div class="line"><a name="l00569"></a><span class="lineno">  569</span>&#160;        - NT_n = X</div><div class="line"><a name="l00570"></a><span class="lineno">  570</span>&#160;        - span_n &lt;= j-i</div><div class="line"><a name="l00571"></a><span class="lineno">  571</span>&#160;        - size_n &lt;= number of states of \f$T_u\f$, computed via \ref OpenFst `NumStates()`</div><div class="line"><a name="l00572"></a><span class="lineno">  572</span>&#160;  - then \f$T_u\f$ is pruned as follows:</div><div class="line"><a name="l00573"></a><span class="lineno">  573</span>&#160;      - OpenFst [Replace](http://openfst.cs.nyu.edu/twiki/bin/view/FST/ReplaceDoc) converts \f$R_u\f$ to a WFSA</div><div class="line"><a name="l00574"></a><span class="lineno">  574</span>&#160;      - [RmEpsilon](http://www.openfst.org/twiki/bin/view/FST/RmEpsilonDoc),[Deteminize](http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc), and [Minimize](http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc) generate a compacted WFSA</div><div class="line"><a name="l00575"></a><span class="lineno">  575</span>&#160;      - [Composition](http://www.openfst.org/twiki/bin/view/FST/ComposeDoc) with K language model(s) WFSAs</div><div class="line"><a name="l00576"></a><span class="lineno">  576</span>&#160;          - The parameters (`lm_k`, `scale_k`, `wp_k`) specify the language models, language model scale factors, and word penalties to be applied</div><div class="line"><a name="l00577"></a><span class="lineno">  577</span>&#160;      - OpenFst [Prune](http://www.openfst.org/twiki/bin/view/FST/PruneDoc) is applied with threshold `threshold_n`</div><div class="line"><a name="l00578"></a><span class="lineno">  578</span>&#160;      - Language model scores are removed by copying component weights in the lexicographic semiring, see \ref basic_scores</div><div class="line"><a name="l00579"></a><span class="lineno">  579</span>&#160;      - [RmEpsilon](http://www.openfst.org/twiki/bin/view/FST/RmEpsilonDoc),[Deteminize](http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc), and [Minimize](http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc), yielding a pruned WFSA \f$T_u\f$ with only translation scores and target language symbols</div><div class="line"><a name="l00580"></a><span class="lineno">  580</span>&#160;  - The pruned version of \f$T_u\f$ is then used in place of the original version in the RTN</div><div class="line"><a name="l00581"></a><span class="lineno">  581</span>&#160;</div><div class="line"><a name="l00582"></a><span class="lineno">  582</span>&#160;\subsection lpruning_effects Effect on Speed, Memory, Scores</div><div class="line"><a name="l00583"></a><span class="lineno">  583</span>&#160;</div><div class="line"><a name="l00584"></a><span class="lineno">  584</span>&#160;Pruning in search is particularly important when running HiFST with</div><div class="line"><a name="l00585"></a><span class="lineno">  585</span>&#160;grammars that are more powerful than the shallow grammar used in</div><div class="line"><a name="l00586"></a><span class="lineno">  586</span>&#160;these examples.  However, we can still see speed and memory use improvements through local pruning, particularly with long sentences.</div><div class="line"><a name="l00587"></a><span class="lineno">  587</span>&#160;</div><div class="line"><a name="l00588"></a><span class="lineno">  588</span>&#160;First, pick some long source language sentences to translate:</div><div class="line"><a name="l00589"></a><span class="lineno">  589</span>&#160;</div><div class="line"><a name="l00590"></a><span class="lineno">  590</span>&#160;     &gt; awk &#39;NF&gt;80&#39; RU/RU.tune.idx  &gt; tmp/RU.long.idx # should be 3 sentences</div><div class="line"><a name="l00591"></a><span class="lineno">  591</span>&#160;</div><div class="line"><a name="l00592"></a><span class="lineno">  592</span>&#160;Translate these long sentences under the baseline Shallow-1 grammar; note this will overwrite the output in `output/exp.baseline/`</div><div class="line"><a name="l00593"></a><span class="lineno">  593</span>&#160;</div><div class="line"><a name="l00594"></a><span class="lineno">  594</span>&#160;     &gt; (time hifst.${TGTBINMK}.bin --config=configs/CF.baseline --range=1:3 --source.load=tmp/RU.long.idx ) &amp;&gt; log/log.long</div><div class="line"><a name="l00595"></a><span class="lineno">  595</span>&#160;</div><div class="line"><a name="l00596"></a><span class="lineno">  596</span>&#160;The maximum memory use is approximately 2.5GB and translation takes approximately 1m30s.</div><div class="line"><a name="l00597"></a><span class="lineno">  597</span>&#160;(The resource consumption may vary depending on your hardware, we provide these</div><div class="line"><a name="l00598"></a><span class="lineno">  598</span>&#160;numbers to illustrate the effect of local pruning.)</div><div class="line"><a name="l00599"></a><span class="lineno">  599</span>&#160;</div><div class="line"><a name="l00600"></a><span class="lineno">  600</span>&#160;If translation is performed with the same grammar and language model, but with aggressive local pruning,</div><div class="line"><a name="l00601"></a><span class="lineno">  601</span>&#160;</div><div class="line"><a name="l00602"></a><span class="lineno">  602</span>&#160;     &gt; (time hifst.${TGTBINMK}.bin --config=configs/CF.baseline.localprune) &amp;&gt; log/log.baseline.localprune</div><div class="line"><a name="l00603"></a><span class="lineno">  603</span>&#160;</div><div class="line"><a name="l00604"></a><span class="lineno">  604</span>&#160;then the memory consumption is reduced to under 2.2GB and the</div><div class="line"><a name="l00605"></a><span class="lineno">  605</span>&#160;processing time to approximately 40s.  Note that local pruning does</div><div class="line"><a name="l00606"></a><span class="lineno">  606</span>&#160;not have as great an effect on translation under the Shallow-1 grammar</div><div class="line"><a name="l00607"></a><span class="lineno">  607</span>&#160;as it would with e.g. a full Hiero grammar.</div><div class="line"><a name="l00608"></a><span class="lineno">  608</span>&#160;</div><div class="line"><a name="l00609"></a><span class="lineno">  609</span>&#160;Inspecting the log file indicates that local pruning was applied extensively in translation:</div><div class="line"><a name="l00610"></a><span class="lineno">  610</span>&#160;</div><div class="line"><a name="l00611"></a><span class="lineno">  611</span>&#160;</div><div class="line"><a name="l00612"></a><span class="lineno">  612</span>&#160;     &gt; grep pruning log/log.baseline.localprune</div><div class="line"><a name="l00613"></a><span class="lineno">  613</span>&#160;     Mon Apr  6 19:06:46 2015: run.INF:Stats for Sentence 1: local pruning, number of times=266</div><div class="line"><a name="l00614"></a><span class="lineno">  614</span>&#160;     Mon Apr  6 19:07:01 2015: run.INF:Stats for Sentence 2: local pruning, number of times=279</div><div class="line"><a name="l00615"></a><span class="lineno">  615</span>&#160;     Mon Apr  6 19:07:16 2015: run.INF:Stats for Sentence 3: local pruning, number of times=154</div><div class="line"><a name="l00616"></a><span class="lineno">  616</span>&#160;</div><div class="line"><a name="l00617"></a><span class="lineno">  617</span>&#160;Pruning is aggressive enough here that the translations are affected:</div><div class="line"><a name="l00618"></a><span class="lineno">  618</span>&#160;</div><div class="line"><a name="l00619"></a><span class="lineno">  619</span>&#160;     &gt; printstrings.${TGTBINMK}.bin -w --semiring=lexstdarc -m wmaps/wmt13.en.wmap --input=output/exp.baseline/LATS/1.fst.gz</div><div class="line"><a name="l00620"></a><span class="lineno">  620</span>&#160;     &lt;s&gt; the decline in economic activity caused , until november , the permanent rise in unemployment in the state , according to the national institute of statistics and geography , in the first three quarters was recorded accelerated growth in unemployment , from january to march 55.053 resident sinaloa were unemployed , with the share of per cent of the economically active population , in the second quarter , the share rose to percent from july to september , she continued to rise to percent share , if calculated per number of people is more than unemployed people in sinaloa , the 18.969 people more than in the first half . &lt;/s&gt;   471.123,2.46484</div><div class="line"><a name="l00621"></a><span class="lineno">  621</span>&#160;</div><div class="line"><a name="l00622"></a><span class="lineno">  622</span>&#160;     &gt; printstrings.${TGTBINMK}.bin -w --semiring=lexstdarc -m wmaps/wmt13.en.wmap --input=output/exp.baseline.localprune/LATS/1.fst.gz</div><div class="line"><a name="l00623"></a><span class="lineno">  623</span>&#160;     &lt;s&gt; the decline in economic activity caused , until november , the permanent rise in unemployment in the state , according to the national institute of statistics and geography , in the first three quarters was recorded accelerated growth in unemployment , from january to march 55.053 resident sinaloa were unemployed , with the share of per cent of the economically active population , in the second quarter share rose to percent from july to september , she continued to rise to percent stake , which is recalculated the number of people is more than unemployed people in sinaloa , the 18.969 people more than in the first half . &lt;/s&gt;         472.86,3.74023</div><div class="line"><a name="l00624"></a><span class="lineno">  624</span>&#160;</div><div class="line"><a name="l00625"></a><span class="lineno">  625</span>&#160;</div><div class="line"><a name="l00626"></a><span class="lineno">  626</span>&#160;The best hypothesis generated under the baseline system without local pruning has a combined</div><div class="line"><a name="l00627"></a><span class="lineno">  627</span>&#160;grammar and language model score of `471.123`.  This hypothesis</div><div class="line"><a name="l00628"></a><span class="lineno">  628</span>&#160;does not survive more aggressive local pruning under these parameters , where the best hypothesis has a higher</div><div class="line"><a name="l00629"></a><span class="lineno">  629</span>&#160;combined score of `472.86`.</div><div class="line"><a name="l00630"></a><span class="lineno">  630</span>&#160;</div><div class="line"><a name="l00631"></a><span class="lineno">  631</span>&#160;\section rescoring_lm Language Model Rescoring</div><div class="line"><a name="l00632"></a><span class="lineno">  632</span>&#160;</div><div class="line"><a name="l00633"></a><span class="lineno">  633</span>&#160;As discussed above, HiFST uses a lexicographic semiring (\ref basic_scores, [\ref Roark2011])</div><div class="line"><a name="l00634"></a><span class="lineno">  634</span>&#160;of two tropical weights.  In each arc of a lattice generated by HiFST,</div><div class="line"><a name="l00635"></a><span class="lineno">  635</span>&#160;the first weight `(G+M)` contains the correct score (translation grammar score + language model score).  The second weight `G` only contains the</div><div class="line"><a name="l00636"></a><span class="lineno">  636</span>&#160;translation grammar score.  An example is repeated here:</div><div class="line"><a name="l00637"></a><span class="lineno">  637</span>&#160;</div><div class="line"><a name="l00638"></a><span class="lineno">  638</span>&#160;    # re-run the translation baseline</div><div class="line"><a name="l00639"></a><span class="lineno">  639</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline &amp;&gt; log/log.baseline</div><div class="line"><a name="l00640"></a><span class="lineno">  640</span>&#160;</div><div class="line"><a name="l00641"></a><span class="lineno">  641</span>&#160;    &gt; zcat output/exp.baseline/LATS/1.fst.gz | fsttopsort | fstprint | head -n 10</div><div class="line"><a name="l00642"></a><span class="lineno">  642</span>&#160;    0     1     1     1   -2.68554688,-2.68554688</div><div class="line"><a name="l00643"></a><span class="lineno">  643</span>&#160;    1   319   170   170   11.4819975,4.25097656</div><div class="line"><a name="l00644"></a><span class="lineno">  644</span>&#160;    1   118    50    50   6.39422989,-2.88476562</div><div class="line"><a name="l00645"></a><span class="lineno">  645</span>&#160;    1   104     3     3   1.87243128,-2.70800781</div><div class="line"><a name="l00646"></a><span class="lineno">  646</span>&#160;    1    91    50    50   3.70087051,-5.578125</div><div class="line"><a name="l00647"></a><span class="lineno">  647</span>&#160;    1    87     3     3   -0.820928097,-5.40136719</div><div class="line"><a name="l00648"></a><span class="lineno">  648</span>&#160;    1    65    50    50   7.04462051,-2.234375</div><div class="line"><a name="l00649"></a><span class="lineno">  649</span>&#160;    1    62     3     3   1.72790003,-2.85253906</div><div class="line"><a name="l00650"></a><span class="lineno">  650</span>&#160;    1    47    50    50   6.01825333,-3.26074219</div><div class="line"><a name="l00651"></a><span class="lineno">  651</span>&#160;    1    43   170   170   5.89898968,-1.33203125</div><div class="line"><a name="l00652"></a><span class="lineno">  652</span>&#160;</div><div class="line"><a name="l00653"></a><span class="lineno">  653</span>&#160;The advantage of using the lexicographic semiring to represent</div><div class="line"><a name="l00654"></a><span class="lineno">  654</span>&#160;`(G+M,G)` weights is that the language model score can be removed very</div><div class="line"><a name="l00655"></a><span class="lineno">  655</span>&#160;efficiently: the second field is simply copied over the first field.</div><div class="line"><a name="l00656"></a><span class="lineno">  656</span>&#160;HiFST binaries do this mapping internally with an \ref OpenFst</div><div class="line"><a name="l00657"></a><span class="lineno">  657</span>&#160;[fstmap](http://www.openfst.org/twiki/bin/view/FST/ArcMapDoc)</div><div class="line"><a name="l00658"></a><span class="lineno">  658</span>&#160;operation.  The result is a WFSA whose weights contain only the</div><div class="line"><a name="l00659"></a><span class="lineno">  659</span>&#160;translation grammar scores.  The lexmap tool can be used to do this</div><div class="line"><a name="l00660"></a><span class="lineno">  660</span>&#160;mapping, as follows, yielding lexicographic weights `(G,G)`:</div><div class="line"><a name="l00661"></a><span class="lineno">  661</span>&#160;</div><div class="line"><a name="l00662"></a><span class="lineno">  662</span>&#160;    &gt; zcat output/exp.baseline/LATS/1.fst.gz | lexmap.${TGTBINMK}.bin | fsttopsort | fstprint | head -n 10</div><div class="line"><a name="l00663"></a><span class="lineno">  663</span>&#160;    0     1     1     1   -2.68554688,-2.68554688</div><div class="line"><a name="l00664"></a><span class="lineno">  664</span>&#160;    1   319   170   170   4.25097656,4.25097656</div><div class="line"><a name="l00665"></a><span class="lineno">  665</span>&#160;    1   118    50    50   -2.88476562,-2.88476562</div><div class="line"><a name="l00666"></a><span class="lineno">  666</span>&#160;    1   104     3     3   -2.70800781,-2.70800781</div><div class="line"><a name="l00667"></a><span class="lineno">  667</span>&#160;    1    91    50    50   -5.578125,-5.578125</div><div class="line"><a name="l00668"></a><span class="lineno">  668</span>&#160;    1    87     3     3   -5.40136719,-5.40136719</div><div class="line"><a name="l00669"></a><span class="lineno">  669</span>&#160;    1    65    50    50   -2.234375,-2.234375</div><div class="line"><a name="l00670"></a><span class="lineno">  670</span>&#160;    1    62     3     3   -2.85253906,-2.85253906</div><div class="line"><a name="l00671"></a><span class="lineno">  671</span>&#160;    1    47    50    50   -3.26074219,-3.26074219</div><div class="line"><a name="l00672"></a><span class="lineno">  672</span>&#160;    1    43   170   170   -1.33203125,-1.33203125</div><div class="line"><a name="l00673"></a><span class="lineno">  673</span>&#160;</div><div class="line"><a name="l00674"></a><span class="lineno">  674</span>&#160;Using this facility to remove language model scores, the HiFST</div><div class="line"><a name="l00675"></a><span class="lineno">  675</span>&#160;`applylm` tool can be used to rescore lattices under a different</div><div class="line"><a name="l00676"></a><span class="lineno">  676</span>&#160;language model than was used in first-pass translation.  Operations</div><div class="line"><a name="l00677"></a><span class="lineno">  677</span>&#160;are as follows:</div><div class="line"><a name="l00678"></a><span class="lineno">  678</span>&#160;</div><div class="line"><a name="l00679"></a><span class="lineno">  679</span>&#160;   -# A lattice with lexicographic `(G+M,G)` weights is loaded</div><div class="line"><a name="l00680"></a><span class="lineno">  680</span>&#160;   -# Weights are converted to `(G,G)` via [fstmap](http://www.openfst.org/twiki/bin/view/FST/ArcMapDoc)</div><div class="line"><a name="l00681"></a><span class="lineno">  681</span>&#160;   -# The new language model(s) are applied via composition under the lexicographic semiring, with optional scale factors and word insertion penalties.  The new WFSAs weights are of the form `(G+M2,G)`, where `M2` are the new language model weights.</div><div class="line"><a name="l00682"></a><span class="lineno">  682</span>&#160;   -# The reweighted WFSA is written to disk, with either lexicographic or standard tropical weights</div><div class="line"><a name="l00683"></a><span class="lineno">  683</span>&#160;</div><div class="line"><a name="l00684"></a><span class="lineno">  684</span>&#160;The following example uses applylm to rescore lattices generated with</div><div class="line"><a name="l00685"></a><span class="lineno">  685</span>&#160;almost no pruning (`output/exp.baseline.outputnoprune/LATS`). Rescoring</div><div class="line"><a name="l00686"></a><span class="lineno">  686</span>&#160;uses the same 4-gram language model originally used to generate the</div><div class="line"><a name="l00687"></a><span class="lineno">  687</span>&#160;lattice, but with a different scale factor (`lm.scale=0.9`).</div><div class="line"><a name="l00688"></a><span class="lineno">  688</span>&#160;</div><div class="line"><a name="l00689"></a><span class="lineno">  689</span>&#160;     &gt; applylm.${TGTBINMK}.bin --config=configs/CF.baseline.outputnoprune.lmrescore  &amp;&gt; log/log.lmrescore</div><div class="line"><a name="l00690"></a><span class="lineno">  690</span>&#160;</div><div class="line"><a name="l00691"></a><span class="lineno">  691</span>&#160;For the second sentence, the original 1-best hypothesis was:</div><div class="line"><a name="l00692"></a><span class="lineno">  692</span>&#160;</div><div class="line"><a name="l00693"></a><span class="lineno">  693</span>&#160;     &gt; zcat output/exp.baseline.outputnoprune/LATS/2.fst.gz | printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null</div><div class="line"><a name="l00694"></a><span class="lineno">  694</span>&#160;     &lt;s&gt; amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt;     101.582,-37.5605</div><div class="line"><a name="l00695"></a><span class="lineno">  695</span>&#160;</div><div class="line"><a name="l00696"></a><span class="lineno">  696</span>&#160;Rescoring yields a slightly different 1-best:</div><div class="line"><a name="l00697"></a><span class="lineno">  697</span>&#160;</div><div class="line"><a name="l00698"></a><span class="lineno">  698</span>&#160;     &gt; zcat output/exp.baseline.lmrescore/LATS/2.fst.gz | printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null</div><div class="line"><a name="l00699"></a><span class="lineno">  699</span>&#160;     &lt;s&gt; amendment , which would have led to the release of which is in jail of former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences . &lt;/s&gt;  87.5249,-39.5508</div><div class="line"><a name="l00700"></a><span class="lineno">  700</span>&#160;</div><div class="line"><a name="l00701"></a><span class="lineno">  701</span>&#160;</div><div class="line"><a name="l00702"></a><span class="lineno">  702</span>&#160;Note the `load.deletelmcost` option in the configuration file, which</div><div class="line"><a name="l00703"></a><span class="lineno">  703</span>&#160;instructs the tool to subtract old lm scores first.</div><div class="line"><a name="l00704"></a><span class="lineno">  704</span>&#160;If the scaling is not changed, both lattices should be identical</div><div class="line"><a name="l00705"></a><span class="lineno">  705</span>&#160;</div><div class="line"><a name="l00706"></a><span class="lineno">  706</span>&#160;    (`applylm.${TGTBINMK}.bin --config=configs/CF.baseline.outputnoprune.lmrescore --lm.featureweights=1`).</div><div class="line"><a name="l00707"></a><span class="lineno">  707</span>&#160;</div><div class="line"><a name="l00708"></a><span class="lineno">  708</span>&#160;</div><div class="line"><a name="l00709"></a><span class="lineno">  709</span>&#160;\section multithread Multithreading</div><div class="line"><a name="l00710"></a><span class="lineno">  710</span>&#160;</div><div class="line"><a name="l00711"></a><span class="lineno">  711</span>&#160;**Note** that the timing results here are illustrative only.</div><div class="line"><a name="l00712"></a><span class="lineno">  712</span>&#160;</div><div class="line"><a name="l00713"></a><span class="lineno">  713</span>&#160;HiFST uses </div><div class="line"><a name="l00714"></a><span class="lineno">  714</span>&#160;[Boost.Thread](http://www.boost.org/doc/libs/1_38_0/doc/html/thread.html)</div><div class="line"><a name="l00715"></a><span class="lineno">  715</span>&#160;to enable multithreading.  This is disabled by default, but can enabled using</div><div class="line"><a name="l00716"></a><span class="lineno">  716</span>&#160;the flag `--nthreads=N` .  If set, each source language sentence is</div><div class="line"><a name="l00717"></a><span class="lineno">  717</span>&#160;translated simultaneously on its own thread (trimmed to the number of</div><div class="line"><a name="l00718"></a><span class="lineno">  718</span>&#160;CPUs available).  The translation grammar and language model are kept in</div><div class="line"><a name="l00719"></a><span class="lineno">  719</span>&#160;shared memory.</div><div class="line"><a name="l00720"></a><span class="lineno">  720</span>&#160;</div><div class="line"><a name="l00721"></a><span class="lineno">  721</span>&#160;To see the effects of multithreading on speed and memory use,</div><div class="line"><a name="l00722"></a><span class="lineno">  722</span>&#160;the baseline configuration is run over the first twenty sentences without multithreading:</div><div class="line"><a name="l00723"></a><span class="lineno">  723</span>&#160;</div><div class="line"><a name="l00724"></a><span class="lineno">  724</span>&#160;     &gt; time hifst.${TGTBINMK}.bin --config=configs/CF.baseline --range=1:20</div><div class="line"><a name="l00725"></a><span class="lineno">  725</span>&#160;</div><div class="line"><a name="l00726"></a><span class="lineno">  726</span>&#160;Processing time is 105 seconds and maximum memory use is about 2GB.  </div><div class="line"><a name="l00727"></a><span class="lineno">  727</span>&#160;In the same decoder configuration but with 2 threads</div><div class="line"><a name="l00728"></a><span class="lineno">  728</span>&#160;</div><div class="line"><a name="l00729"></a><span class="lineno">  729</span>&#160;     &gt; time hifst.${TGTBINMK}.bin --config=configs/CF.baseline --range=1:20 --nthreads=2</div><div class="line"><a name="l00730"></a><span class="lineno">  730</span>&#160;</div><div class="line"><a name="l00731"></a><span class="lineno">  731</span>&#160;processing time is reduced to 60 seconds with maximum memory use of about 2.5GB.</div><div class="line"><a name="l00732"></a><span class="lineno">  732</span>&#160;</div><div class="line"><a name="l00733"></a><span class="lineno">  733</span>&#160;In these examples, both the LM and translation grammar are relatively</div><div class="line"><a name="l00734"></a><span class="lineno">  734</span>&#160;small, and so there is not a great deal of gain from keeping them in</div><div class="line"><a name="l00735"></a><span class="lineno">  735</span>&#160;shared memory.   But in larger tasks,  multithreading can be a significant advantage.</div><div class="line"><a name="l00736"></a><span class="lineno">  736</span>&#160;</div><div class="line"><a name="l00737"></a><span class="lineno">  737</span>&#160;</div><div class="line"><a name="l00738"></a><span class="lineno">  738</span>&#160;</div><div class="line"><a name="l00739"></a><span class="lineno">  739</span>&#160;\section lmbr Lattice Minimum Bayes Risk Decoding</div><div class="line"><a name="l00740"></a><span class="lineno">  740</span>&#160;</div><div class="line"><a name="l00741"></a><span class="lineno">  741</span>&#160;For a detailed discussion of LMBR, see Chapters 7 and 8 in [\ref BlackwoodPhD].</div><div class="line"><a name="l00742"></a><span class="lineno">  742</span>&#160;</div><div class="line"><a name="l00743"></a><span class="lineno">  743</span>&#160;LMBR is a decoding procedure, based on the following:</div><div class="line"><a name="l00744"></a><span class="lineno">  744</span>&#160;</div><div class="line"><a name="l00745"></a><span class="lineno">  745</span>&#160;- Evidence space: a lattice (WFSA) containing weighted translations produced by the SMT system.</div><div class="line"><a name="l00746"></a><span class="lineno">  746</span>&#160;    - N-gram posterior distributions, with pathwise posteriors, are extracted from this WFSA.</div><div class="line"><a name="l00747"></a><span class="lineno">  747</span>&#160;- The hypotheses space: an unweighted lattice (FSA) containing hypotheses to be rescored.</div><div class="line"><a name="l00748"></a><span class="lineno">  748</span>&#160;</div><div class="line"><a name="l00749"></a><span class="lineno">  749</span>&#160;The following steps are carried out in LMBR decoding:</div><div class="line"><a name="l00750"></a><span class="lineno">  750</span>&#160;</div><div class="line"><a name="l00751"></a><span class="lineno">  751</span>&#160;1. The evidence space is normalised after applying a grammar scale factor (`--alpha=`). Scaling is done by the \ref OpenFst [Push](http://www.openfst.org/twiki/bin/view/FST/PushDoc) towards final states, and setting the final state probability to 1.0.</div><div class="line"><a name="l00752"></a><span class="lineno">  752</span>&#160;3. N-grams are extracted from the hypothesis space.</div><div class="line"><a name="l00753"></a><span class="lineno">  753</span>&#160;4. N-gram path-posterior probabilities are computed over the evidence space using a modified Forward procedure (see \ref Blackwood2010)</div><div class="line"><a name="l00754"></a><span class="lineno">  754</span>&#160;5. Cyclic WFSAs are built to represent posterior probability distribution  of each n-gram order and compose with the original hypotheses space. A word insertion penalty (`--wps=`) is also included in the costs of the cyclic WFSAs.</div><div class="line"><a name="l00755"></a><span class="lineno">  755</span>&#160;6. Risk is computed through a sequence of compositions.</div><div class="line"><a name="l00756"></a><span class="lineno">  756</span>&#160;7. The result for LMBR decoding is a WFSA; each weighted path represents a hypothesis and its risk.</div><div class="line"><a name="l00757"></a><span class="lineno">  757</span>&#160;</div><div class="line"><a name="l00758"></a><span class="lineno">  758</span>&#160;The weighted hypothesis space can be save as a WFSA, or the minimum risk hypothesis can be</div><div class="line"><a name="l00759"></a><span class="lineno">  759</span>&#160;generated via the \ref OpenFst</div><div class="line"><a name="l00760"></a><span class="lineno">  760</span>&#160;[ShortestPath](http://www.openfst.org/twiki/bin/view/FST/ShortestPathDoc)</div><div class="line"><a name="l00761"></a><span class="lineno">  761</span>&#160;operation.</div><div class="line"><a name="l00762"></a><span class="lineno">  762</span>&#160;</div><div class="line"><a name="l00763"></a><span class="lineno">  763</span>&#160;The following example applies LMBR decoding to the baseline lattices</div><div class="line"><a name="l00764"></a><span class="lineno">  764</span>&#160;</div><div class="line"><a name="l00765"></a><span class="lineno">  765</span>&#160;    &gt; lmbr.${TGTBINMK}.bin --config=configs/CF.baseline.lmbr &amp;&gt; log/log.baseline.lmbr</div><div class="line"><a name="l00766"></a><span class="lineno">  766</span>&#160;</div><div class="line"><a name="l00767"></a><span class="lineno">  767</span>&#160;The LMBR output hyppthesis file keeps the scale factor, word penalty, and sentence id at the start of the file;</div><div class="line"><a name="l00768"></a><span class="lineno">  768</span>&#160;the hypothesis follows the colon</div><div class="line"><a name="l00769"></a><span class="lineno">  769</span>&#160;</div><div class="line"><a name="l00770"></a><span class="lineno">  770</span>&#160;    &gt; cat output/exp.baseline.lmbr/HYPS/0.40_0.02.hyp</div><div class="line"><a name="l00771"></a><span class="lineno">  771</span>&#160;    0.4 0.02 1:1 50 135 20 103 3 245 4 25 1145 48 3 425 6 23899 2 </div><div class="line"><a name="l00772"></a><span class="lineno">  772</span>&#160;    0.4 0.02 2:1 245 4 25 35 23 1028 7 3 2295 6 25 12 9 2666 6 972 1052 564 4 51 1284 317 3 312 734 6 3 3423 7 4922 2057 14 119 3570 5 2 </div><div class="line"><a name="l00773"></a><span class="lineno">  773</span>&#160;</div><div class="line"><a name="l00774"></a><span class="lineno">  774</span>&#160;LMBR can be optimised by tuning the grammar scale factor and word insertion penalty.</div><div class="line"><a name="l00775"></a><span class="lineno">  775</span>&#160;Once lattices are loaded into memory and n-grams are extracted (steps 1 - 5), rescoring is fast enough that</div><div class="line"><a name="l00776"></a><span class="lineno">  776</span>&#160;it is practical and efficient to perform a grid search over a range of</div><div class="line"><a name="l00777"></a><span class="lineno">  777</span>&#160;parameter values (see config file).</div><div class="line"><a name="l00778"></a><span class="lineno">  778</span>&#160;</div><div class="line"><a name="l00779"></a><span class="lineno">  779</span>&#160;Hypotheses are written to different files, with names based on parameter values (e.g. as</div><div class="line"><a name="l00780"></a><span class="lineno">  780</span>&#160; `--writeonebest=output/exp.baseline.lmbr/HYPS/%%alpha%%_%%wps%%.hyp` ).</div><div class="line"><a name="l00781"></a><span class="lineno">  781</span>&#160;The best set of values can be selected based on BLEU score, after</div><div class="line"><a name="l00782"></a><span class="lineno">  782</span>&#160;mapping each integer mapped output back to words, detokenizing, and scoring on against references.</div><div class="line"><a name="l00783"></a><span class="lineno">  783</span>&#160;</div><div class="line"><a name="l00784"></a><span class="lineno">  784</span>&#160;LMBR relies on a unigram precision (p) and precision ratio (r) that are computed over a development set,</div><div class="line"><a name="l00785"></a><span class="lineno">  785</span>&#160;e.g. with verbose logs of a BLEU scorer such as NIST mteval1.</div><div class="line"><a name="l00786"></a><span class="lineno">  786</span>&#160;The script `$HiFSTROOT/scripts/lmbr/compute-testset-precisions.pl` is included for this purpose.</div><div class="line"><a name="l00787"></a><span class="lineno">  787</span>&#160;</div><div class="line"><a name="l00788"></a><span class="lineno">  788</span>&#160;</div><div class="line"><a name="l00789"></a><span class="lineno">  789</span>&#160;If the option `--preprune=` is specified, the evidence space is pruned prior to computing posterior probabilities (i.e. pruning is done at threshold 7 in this example).  If this option is not defined, the full evidence space will be passed through.</div><div class="line"><a name="l00790"></a><span class="lineno">  790</span>&#160;</div><div class="line"><a name="l00791"></a><span class="lineno">  791</span>&#160;</div><div class="line"><a name="l00792"></a><span class="lineno">  792</span>&#160;\section mert MERT (Features Only)</div><div class="line"><a name="l00793"></a><span class="lineno">  793</span>&#160;</div><div class="line"><a name="l00794"></a><span class="lineno">  794</span>&#160;This section describes how to generate N-Best lists of features for</div><div class="line"><a name="l00795"></a><span class="lineno">  795</span>&#160;use with MERT</div><div class="line"><a name="l00796"></a><span class="lineno">  796</span>&#160;[[Och 2003](http://aclweb.org/anthology/P/P03/P03-1021.pdf)].</div><div class="line"><a name="l00797"></a><span class="lineno">  797</span>&#160;</div><div class="line"><a name="l00798"></a><span class="lineno">  798</span>&#160;The following sequence of operations will generate hypotheses and feature vectors</div><div class="line"><a name="l00799"></a><span class="lineno">  799</span>&#160;that can be used by MERT. We use N-Best lists of depth `N = 100`, set by</div><div class="line"><a name="l00800"></a><span class="lineno">  800</span>&#160;the `prunereferenceshortestpath=` option in `configs/CF.mert.alilats.nbest`.</div><div class="line"><a name="l00801"></a><span class="lineno">  801</span>&#160;For this tutorial, the configuration files specify multithreading, and</div><div class="line"><a name="l00802"></a><span class="lineno">  802</span>&#160;N-Best lists will be generated only for the first 2 sentences in</div><div class="line"><a name="l00803"></a><span class="lineno">  803</span>&#160;RU/RU.tune.idx.</div><div class="line"><a name="l00804"></a><span class="lineno">  804</span>&#160;</div><div class="line"><a name="l00805"></a><span class="lineno">  805</span>&#160;    &gt; M=2</div><div class="line"><a name="l00806"></a><span class="lineno">  806</span>&#160;    # Step 1.  Generate hyps</div><div class="line"><a name="l00807"></a><span class="lineno">  807</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.mert.hyps --range=1:$M &amp;&gt; log/log.mert.hyps</div><div class="line"><a name="l00808"></a><span class="lineno">  808</span>&#160;    # Step 2. Generate alignment lats</div><div class="line"><a name="l00809"></a><span class="lineno">  809</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.mert.alilats.nbest --range=1:$M &amp;&gt; log/log.mert.alilats.nbest</div><div class="line"><a name="l00810"></a><span class="lineno">  810</span>&#160;    # Step 3. Generate feature vectors</div><div class="line"><a name="l00811"></a><span class="lineno">  811</span>&#160;    &gt; alilats2splats.${TGTBINMK}.bin --config=configs/CF.mert.vecfea.nbest --range=1:$M &amp;&gt; log/log.mert.nbest</div><div class="line"><a name="l00812"></a><span class="lineno">  812</span>&#160;</div><div class="line"><a name="l00813"></a><span class="lineno">  813</span>&#160;</div><div class="line"><a name="l00814"></a><span class="lineno">  814</span>&#160;The process takes the following as its input:</div><div class="line"><a name="l00815"></a><span class="lineno">  815</span>&#160;- `RU/RU.tune.idx` -- tuning set source language sentences</div><div class="line"><a name="l00816"></a><span class="lineno">  816</span>&#160;- `G/rules.shallow.vecfea.gz` -- translation grammar, with unweighted feature vectors</div><div class="line"><a name="l00817"></a><span class="lineno">  817</span>&#160;- `M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap` -- target language model</div><div class="line"><a name="l00818"></a><span class="lineno">  818</span>&#160;- Initial feature weights for the language model and translation grammar (see `configs/CF.mert.hyps` and \ref weight_feature_vector_examples)</div><div class="line"><a name="l00819"></a><span class="lineno">  819</span>&#160;  - P=1.0,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136</div><div class="line"><a name="l00820"></a><span class="lineno">  820</span>&#160;</div><div class="line"><a name="l00821"></a><span class="lineno">  821</span>&#160;The output is written to two linked sets of files, for `m=1,...,$M`</div><div class="line"><a name="l00822"></a><span class="lineno">  822</span>&#160;  - `output/exp.mert/nbest/VECFEA/$m.nbest.gz` -- word hypotheses</div><div class="line"><a name="l00823"></a><span class="lineno">  823</span>&#160;  - `output/exp.mert/nbest/VECFEA/$m.vecfea.gz` -- unweighted feature vectors</div><div class="line"><a name="l00824"></a><span class="lineno">  824</span>&#160;</div><div class="line"><a name="l00825"></a><span class="lineno">  825</span>&#160;The output files contain 1) the top `N` hypotheses for each source</div><div class="line"><a name="l00826"></a><span class="lineno">  826</span>&#160;sentence and 2) the corresponding unweighted feature vector obtained</div><div class="line"><a name="l00827"></a><span class="lineno">  827</span>&#160;from the best derivation of each of those hypotheses:</div><div class="line"><a name="l00828"></a><span class="lineno">  828</span>&#160;</div><div class="line"><a name="l00829"></a><span class="lineno">  829</span>&#160;    &gt; zcat output/exp.mert/nbest/VECFEA/1.nbest.gz | head -2</div><div class="line"><a name="l00830"></a><span class="lineno">  830</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;  43.0904</div><div class="line"><a name="l00831"></a><span class="lineno">  831</span>&#160;    &lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;      43.1757</div><div class="line"><a name="l00832"></a><span class="lineno">  832</span>&#160;</div><div class="line"><a name="l00833"></a><span class="lineno">  833</span>&#160;    &gt; zcat output/exp.mert/nbest/VECFEA/1.vecfea.gz | head -2</div><div class="line"><a name="l00834"></a><span class="lineno">  834</span>&#160;    62.5442 10.8672 8.3936  -16.0000        -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293</div><div class="line"><a name="l00835"></a><span class="lineno">  835</span>&#160;    63.1159 12.8613 8.7959  -17.0000        -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482</div><div class="line"><a name="l00836"></a><span class="lineno">  836</span>&#160;</div><div class="line"><a name="l00837"></a><span class="lineno">  837</span>&#160;As a sanity check,  computing the inner product between the vector `$P` and the first unweighted feature vector yields the following score</div><div class="line"><a name="l00838"></a><span class="lineno">  838</span>&#160;</div><div class="line"><a name="l00839"></a><span class="lineno">  839</span>&#160;    43.09045369 = $P . [62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000 -1.0000 0.0000 -7.0000 16.3076 40.5293]</div><div class="line"><a name="l00840"></a><span class="lineno">  840</span>&#160;</div><div class="line"><a name="l00841"></a><span class="lineno">  841</span>&#160;which agrees with the score assigned to the top hypotheses by the decoder:</div><div class="line"><a name="l00842"></a><span class="lineno">  842</span>&#160;</div><div class="line"><a name="l00843"></a><span class="lineno">  843</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --input=output/exp.mert/LATS/1.fst.gz --weight --semiring=lexstdarc --label-map=wmaps/wmt13.en.wmap</div><div class="line"><a name="l00844"></a><span class="lineno">  844</span>&#160;    ...</div><div class="line"><a name="l00845"></a><span class="lineno">  845</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 43.093,-19.4512</div><div class="line"><a name="l00846"></a><span class="lineno">  846</span>&#160;    ...</div><div class="line"><a name="l00847"></a><span class="lineno">  847</span>&#160;</div><div class="line"><a name="l00848"></a><span class="lineno">  848</span>&#160;**Note** that there can be minor numerical differences in the scores of the best derivations and the score found in the initial decoding, as the above example shows.</div><div class="line"><a name="l00849"></a><span class="lineno">  849</span>&#160;</div><div class="line"><a name="l00850"></a><span class="lineno">  850</span>&#160;The three steps to generating the n-best hypotheses and unweighted feature vectors are described next.</div><div class="line"><a name="l00851"></a><span class="lineno">  851</span>&#160;</div><div class="line"><a name="l00852"></a><span class="lineno">  852</span>&#160;\subsection mert_hyps Step 1. Hypotheses for MERT</div><div class="line"><a name="l00853"></a><span class="lineno">  853</span>&#160;</div><div class="line"><a name="l00854"></a><span class="lineno">  854</span>&#160;- Input:</div><div class="line"><a name="l00855"></a><span class="lineno">  855</span>&#160;  - `RU/RU.tune.idx` -- tuning set source language sentences</div><div class="line"><a name="l00856"></a><span class="lineno">  856</span>&#160;  - `G/rules.shallow.vecfea.gz` -- translation grammar</div><div class="line"><a name="l00857"></a><span class="lineno">  857</span>&#160;  - `M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap` -- target language model</div><div class="line"><a name="l00858"></a><span class="lineno">  858</span>&#160;  - language model and translation grammar feature weights (see `configs/CF.mert.hyps`)</div><div class="line"><a name="l00859"></a><span class="lineno">  859</span>&#160;- Output:</div><div class="line"><a name="l00860"></a><span class="lineno">  860</span>&#160;  - `output/exp.mert/LATS/?.fst.gz` -- word lattices (WFSAs), determinized and minimized</div><div class="line"><a name="l00861"></a><span class="lineno">  861</span>&#160;</div><div class="line"><a name="l00862"></a><span class="lineno">  862</span>&#160;This step runs HiFST in the usual way to generate a set of translation</div><div class="line"><a name="l00863"></a><span class="lineno">  863</span>&#160;hypotheses which will be used in MERT.  </div><div class="line"><a name="l00864"></a><span class="lineno">  864</span>&#160;</div><div class="line"><a name="l00865"></a><span class="lineno">  865</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.mert.hyps --range=1:$M &amp;&gt; log/log.mert.hyps</div><div class="line"><a name="l00866"></a><span class="lineno">  866</span>&#160;</div><div class="line"><a name="l00867"></a><span class="lineno">  867</span>&#160;In this configuration, the grammar feature weights and the language</div><div class="line"><a name="l00868"></a><span class="lineno">  868</span>&#160;model feature weights are applied on-the-fly to the grammar and</div><div class="line"><a name="l00869"></a><span class="lineno">  869</span>&#160;language model as they are loaded.  This allows feature vector weights</div><div class="line"><a name="l00870"></a><span class="lineno">  870</span>&#160;to be changed at each iteration of MERT.  This behaviour is specified</div><div class="line"><a name="l00871"></a><span class="lineno">  871</span>&#160;through the following options in the `CF.mert.hyps` file, where we use</div><div class="line"><a name="l00872"></a><span class="lineno">  872</span>&#160;the parameters from the baseline system:</div><div class="line"><a name="l00873"></a><span class="lineno">  873</span>&#160;</div><div class="line"><a name="l00874"></a><span class="lineno">  874</span>&#160;    [lm]</div><div class="line"><a name="l00875"></a><span class="lineno">  875</span>&#160;    load=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap</div><div class="line"><a name="l00876"></a><span class="lineno">  876</span>&#160;    featureweights=1.0</div><div class="line"><a name="l00877"></a><span class="lineno">  877</span>&#160;    # Note that for only one language model, this parameter will always be set to 1.</div><div class="line"><a name="l00878"></a><span class="lineno">  878</span>&#160;    # If there are multiple language models, the language model weights will be updated</div><div class="line"><a name="l00879"></a><span class="lineno">  879</span>&#160;    # after each iteration of MERT</div><div class="line"><a name="l00880"></a><span class="lineno">  880</span>&#160;</div><div class="line"><a name="l00881"></a><span class="lineno">  881</span>&#160;    [grammar]</div><div class="line"><a name="l00882"></a><span class="lineno">  882</span>&#160;    load=G/rules.shallow.vecfea.gz</div><div class="line"><a name="l00883"></a><span class="lineno">  883</span>&#160;    featureweights=0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136</div><div class="line"><a name="l00884"></a><span class="lineno">  884</span>&#160;    # Note that this parameter vector should be updated after each iteration of MERT</div><div class="line"><a name="l00885"></a><span class="lineno">  885</span>&#160;    # Updated versions can be provided via command line arguments</div><div class="line"><a name="l00886"></a><span class="lineno">  886</span>&#160;</div><div class="line"><a name="l00887"></a><span class="lineno">  887</span>&#160;The translation grammar has its rules with unweighted feature vectors:</div><div class="line"><a name="l00888"></a><span class="lineno">  888</span>&#160;</div><div class="line"><a name="l00889"></a><span class="lineno">  889</span>&#160;    &gt; zcat G/rules.shallow.vecfea.gz | head -n 3</div><div class="line"><a name="l00890"></a><span class="lineno">  890</span>&#160;    V 3 4 0.223527 0.116794 -1 -1 0 0 0 0 -1 1.268789 0.687159</div><div class="line"><a name="l00891"></a><span class="lineno">  891</span>&#160;    V 3 4_3 3.333756 0.338107 -2 -1 0 0 0 0 -1 1.662178 3.363062</div><div class="line"><a name="l00892"></a><span class="lineno">  892</span>&#160;    V 3 8 3.74095 3.279819 -1 -1 0 0 0 0 -1 3.741382 2.271445</div><div class="line"><a name="l00893"></a><span class="lineno">  893</span>&#160;</div><div class="line"><a name="l00894"></a><span class="lineno">  894</span>&#160;The output lattices in `output/exp.mert/LATS` are acceptors containing</div><div class="line"><a name="l00895"></a><span class="lineno">  895</span>&#160;word hypotheses, with weights in the form of the lexicographic</div><div class="line"><a name="l00896"></a><span class="lineno">  896</span>&#160;semiring as described earlier.</div><div class="line"><a name="l00897"></a><span class="lineno">  897</span>&#160;</div><div class="line"><a name="l00898"></a><span class="lineno">  898</span>&#160;    &gt; zcat output/exp.mert/LATS/1.fst.gz | fstinfo | head -n 2</div><div class="line"><a name="l00899"></a><span class="lineno">  899</span>&#160;    fst type                                          vector</div><div class="line"><a name="l00900"></a><span class="lineno">  900</span>&#160;    arc type                                          tropical_LT_tropical</div><div class="line"><a name="l00901"></a><span class="lineno">  901</span>&#160;</div><div class="line"><a name="l00902"></a><span class="lineno">  902</span>&#160;    &gt; zcat output/exp.mert/LATS/1.fst.gz | printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2&gt;/dev/null</div><div class="line"><a name="l00903"></a><span class="lineno">  903</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 43.093,-19.4512</div><div class="line"><a name="l00904"></a><span class="lineno">  904</span>&#160;</div><div class="line"><a name="l00905"></a><span class="lineno">  905</span>&#160;</div><div class="line"><a name="l00906"></a><span class="lineno">  906</span>&#160;\subsection mert_nblist_derivations Step 2. Guided Translation / Forced Alignment</div><div class="line"><a name="l00907"></a><span class="lineno">  907</span>&#160;</div><div class="line"><a name="l00908"></a><span class="lineno">  908</span>&#160;- Input:</div><div class="line"><a name="l00909"></a><span class="lineno">  909</span>&#160;   - `RU/RU.tune.idx` -- tuning set source language sentences</div><div class="line"><a name="l00910"></a><span class="lineno">  910</span>&#160;   - `G/rules.shallow.gz` -- translation grammar</div><div class="line"><a name="l00911"></a><span class="lineno">  911</span>&#160;   - `M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap` -- target language model</div><div class="line"><a name="l00912"></a><span class="lineno">  912</span>&#160;   - `output/exp.mert/LATS/?.fst.gz` -- word lattices (WFSAs), determinized and minimized (from \ref mert_hyps)</div><div class="line"><a name="l00913"></a><span class="lineno">  913</span>&#160;- Output:</div><div class="line"><a name="l00914"></a><span class="lineno">  914</span>&#160;   - `output/exp.mert/nbest/ALILATS/?.fst.gz` -- transducers mapping derivations to translations (e.g. Fig. 7, [\ref deGispert2010])</div><div class="line"><a name="l00915"></a><span class="lineno">  915</span>&#160;</div><div class="line"><a name="l00916"></a><span class="lineno">  916</span>&#160;Alignment is the task of finding the derivations (sequences of rules)</div><div class="line"><a name="l00917"></a><span class="lineno">  917</span>&#160;that can produce a given translation.  HiFST performs alignment via</div><div class="line"><a name="l00918"></a><span class="lineno">  918</span>&#160;constrained translation (see Section 2.3, [\ref deGispert2010] for a</div><div class="line"><a name="l00919"></a><span class="lineno">  919</span>&#160;detailed description).  This command runs HiFST in alignment mode:</div><div class="line"><a name="l00920"></a><span class="lineno">  920</span>&#160;</div><div class="line"><a name="l00921"></a><span class="lineno">  921</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.mert.alilats.nbest --range=1:$M &amp;&gt; log/log.mert.alilats.nbest</div><div class="line"><a name="l00922"></a><span class="lineno">  922</span>&#160;</div><div class="line"><a name="l00923"></a><span class="lineno">  923</span>&#160;In alignment mode, HiFST constructs *substring acceptors* (see Fig. 8,</div><div class="line"><a name="l00924"></a><span class="lineno">  924</span>&#160;[\ref deGispert2010]). These are constructed for each sentence as follows:</div><div class="line"><a name="l00925"></a><span class="lineno">  925</span>&#160;</div><div class="line"><a name="l00926"></a><span class="lineno">  926</span>&#160;- the lattice from Step 1 is loaded by HiFST</div><div class="line"><a name="l00927"></a><span class="lineno">  927</span>&#160;- an N-Best list in the form of a WFSA is extracted (using `fstshortestpath`) under the grammar and language model score from Step 1</div><div class="line"><a name="l00928"></a><span class="lineno">  928</span>&#160;- weights are removed from N-Best WFSA</div><div class="line"><a name="l00929"></a><span class="lineno">  929</span>&#160;- the WFSA is transformed to a substring acceptor</div><div class="line"><a name="l00930"></a><span class="lineno">  930</span>&#160;</div><div class="line"><a name="l00931"></a><span class="lineno">  931</span>&#160;The translation grammar is applied in the usual way, but the</div><div class="line"><a name="l00932"></a><span class="lineno">  932</span>&#160;translations are intersected with the substring acceptors so that only</div><div class="line"><a name="l00933"></a><span class="lineno">  933</span>&#160;translations in the N-Best lists are retained.  This generates every</div><div class="line"><a name="l00934"></a><span class="lineno">  934</span>&#160;possible derivation of each N-Best list entry.</div><div class="line"><a name="l00935"></a><span class="lineno">  935</span>&#160;</div><div class="line"><a name="l00936"></a><span class="lineno">  936</span>&#160;This behaviour is specified by the following config file parameters:</div><div class="line"><a name="l00937"></a><span class="lineno">  937</span>&#160;</div><div class="line"><a name="l00938"></a><span class="lineno">  938</span>&#160;    [referencefilter]</div><div class="line"><a name="l00939"></a><span class="lineno">  939</span>&#160;    load=output/exp.mert/LATS/?.fst.gz</div><div class="line"><a name="l00940"></a><span class="lineno">  940</span>&#160;    # perform alignment against these reference lattices containing initial hypotheses</div><div class="line"><a name="l00941"></a><span class="lineno">  941</span>&#160;    prunereferenceshortestpath=100</div><div class="line"><a name="l00942"></a><span class="lineno">  942</span>&#160;    # on loading the reference lattices, transform them to n-best lists prior to alignment.</div><div class="line"><a name="l00943"></a><span class="lineno">  943</span>&#160;    # uses fstshortestpath</div><div class="line"><a name="l00944"></a><span class="lineno">  944</span>&#160;</div><div class="line"><a name="l00945"></a><span class="lineno">  945</span>&#160;Note that application of the substring acceptors is very efficient;</div><div class="line"><a name="l00946"></a><span class="lineno">  946</span>&#160;and this alignment step should be much faster than the translation</div><div class="line"><a name="l00947"></a><span class="lineno">  947</span>&#160;operation of Step 1.</div><div class="line"><a name="l00948"></a><span class="lineno">  948</span>&#160;The alignment lattices (referred to as ALILATS) map rule sequences</div><div class="line"><a name="l00949"></a><span class="lineno">  949</span>&#160;(derivations) to translation hypotheses.  Weights remain in</div><div class="line"><a name="l00950"></a><span class="lineno">  950</span>&#160;lexicographic semiring form.</div><div class="line"><a name="l00951"></a><span class="lineno">  951</span>&#160;</div><div class="line"><a name="l00952"></a><span class="lineno">  952</span>&#160;    &gt; zcat output/exp.mert/nbest/ALILATS/1.fst.gz | fstinfo | head -n 2</div><div class="line"><a name="l00953"></a><span class="lineno">  953</span>&#160;    fst type                                          vector</div><div class="line"><a name="l00954"></a><span class="lineno">  954</span>&#160;    arc type                                          tropical_LT_tropical</div><div class="line"><a name="l00955"></a><span class="lineno">  955</span>&#160;</div><div class="line"><a name="l00956"></a><span class="lineno">  956</span>&#160;Individual rules are identified by their line number in the translation grammar file.  A rule map can be created as</div><div class="line"><a name="l00957"></a><span class="lineno">  957</span>&#160;</div><div class="line"><a name="l00958"></a><span class="lineno">  958</span>&#160;    &gt; zcat G/rules.shallow.gz | awk &#39;BEGIN{print &quot;0\t0&quot;}{printf &quot;%s-&gt;&lt;%s,%s&gt;\t%d\n&quot;, $1, $2, $3, NR}&#39;  &gt; tmp/rules.shallow.map</div><div class="line"><a name="l00959"></a><span class="lineno">  959</span>&#160;    &gt; head -5 tmp/rules.shallow.map</div><div class="line"><a name="l00960"></a><span class="lineno">  960</span>&#160;    0       0</div><div class="line"><a name="l00961"></a><span class="lineno">  961</span>&#160;    V-&gt;&lt;3,4&gt;        1</div><div class="line"><a name="l00962"></a><span class="lineno">  962</span>&#160;    V-&gt;&lt;3,4_3&gt;      2</div><div class="line"><a name="l00963"></a><span class="lineno">  963</span>&#160;    V-&gt;&lt;3,8&gt;        3</div><div class="line"><a name="l00964"></a><span class="lineno">  964</span>&#160;    V-&gt;&lt;3,10&gt;       4</div><div class="line"><a name="l00965"></a><span class="lineno">  965</span>&#160;</div><div class="line"><a name="l00966"></a><span class="lineno">  966</span>&#160;</div><div class="line"><a name="l00967"></a><span class="lineno">  967</span>&#160;The ALILATS transducers are not determinised: they contain every possible derivation</div><div class="line"><a name="l00968"></a><span class="lineno">  968</span>&#160;for each N-Best list entry.  The following</div><div class="line"><a name="l00969"></a><span class="lineno">  969</span>&#160;example prints some of the alternative derivations of the top-scoring</div><div class="line"><a name="l00970"></a><span class="lineno">  970</span>&#160;hypothesis:</div><div class="line"><a name="l00971"></a><span class="lineno">  971</span>&#160;</div><div class="line"><a name="l00972"></a><span class="lineno">  972</span>&#160;    # create a simple, unweighted acceptor for the top-scoring hypothesis</div><div class="line"><a name="l00973"></a><span class="lineno">  973</span>&#160;    &gt; zcat output/exp.mert/LATS/1.fst.gz | fstshortestpath | fstmap --map_type=rmweight &gt; tmp/1.fst </div><div class="line"><a name="l00974"></a><span class="lineno">  974</span>&#160;</div><div class="line"><a name="l00975"></a><span class="lineno">  975</span>&#160;    # print the two best derivations for the top-scoring hypothesis</div><div class="line"><a name="l00976"></a><span class="lineno">  976</span>&#160;    &gt; zcat output/exp.mert/nbest/ALILATS/1.fst.gz | fstcompose - tmp/1.fst | fstproject | printstrings.${TGTBINMK}.bin --nbest=2 --semiring=lexstdarc -m tmp/rules.shallow.map</div><div class="line"><a name="l00977"></a><span class="lineno">  977</span>&#160;    S-&gt;&lt;1,1&gt; V-&gt;&lt;3526,50&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;28847,245&gt; X-&gt;&lt;10_1278_V,135_20_103_3_V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;3_64570,4_25_1145_48&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;1857,3_425_6&gt; X-&gt;&lt;V_7786,V_23899&gt; S-&gt;&lt;S_X,S_X&gt; X-&gt;&lt;2,&lt;/s&gt;&gt; S-&gt;&lt;S_X,S_X&gt;</div><div class="line"><a name="l00978"></a><span class="lineno">  978</span>&#160;    S-&gt;&lt;1,1&gt; V-&gt;&lt;3526,50&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;28847,245&gt; X-&gt;&lt;10_1278_V,135_20_103_3_V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;3_64570,4_25_1145_48&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;1857,3_425_6&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; V-&gt;&lt;7786,23899&gt; X-&gt;&lt;V,V&gt; S-&gt;&lt;S_X,S_X&gt; X-&gt;&lt;2,&lt;/s&gt;&gt; S-&gt;&lt;S_X,S_X&gt;</div><div class="line"><a name="l00979"></a><span class="lineno">  979</span>&#160;</div><div class="line"><a name="l00980"></a><span class="lineno">  980</span>&#160;The order of the rules in these rule sequences correspond to HiFST&#39;s</div><div class="line"><a name="l00981"></a><span class="lineno">  981</span>&#160;bottom-up (left-to-right) CYK grid structure.  Rule IDs are added as</div><div class="line"><a name="l00982"></a><span class="lineno">  982</span>&#160;input symbols to the component WFSTs in the RTN following the</div><div class="line"><a name="l00983"></a><span class="lineno">  983</span>&#160;translation rule (with its non-terminals).  This leads to the</div><div class="line"><a name="l00984"></a><span class="lineno">  984</span>&#160;bottom-up ordering after Replacement.</div><div class="line"><a name="l00985"></a><span class="lineno">  985</span>&#160;</div><div class="line"><a name="l00986"></a><span class="lineno">  986</span>&#160;</div><div class="line"><a name="l00987"></a><span class="lineno">  987</span>&#160;\subsection mert_alilats Step 3. Hypotheses with Unweighted Feature Vectors</div><div class="line"><a name="l00988"></a><span class="lineno">  988</span>&#160;</div><div class="line"><a name="l00989"></a><span class="lineno">  989</span>&#160;- Input:</div><div class="line"><a name="l00990"></a><span class="lineno">  990</span>&#160;   - `G/rules.shallow.vecfea.gz` -- translation grammar, rules with (unweighted) feature vectors</div><div class="line"><a name="l00991"></a><span class="lineno">  991</span>&#160;   - `output/exp.mert/nbest/ALILATS/?.fst.gz` -- transducers mapping derivations to translations, for n-best entries (from \ref mert_nblist_derivations)</div><div class="line"><a name="l00992"></a><span class="lineno">  992</span>&#160;   - language model and translation grammar feature weights (see `configs/CF.mert.vecfea.nbest`)</div><div class="line"><a name="l00993"></a><span class="lineno">  993</span>&#160;- Output:</div><div class="line"><a name="l00994"></a><span class="lineno">  994</span>&#160;   - `output/exp.mert/lats/VECFEA/?.nbest.gz` -- N-best hypotheses</div><div class="line"><a name="l00995"></a><span class="lineno">  995</span>&#160;   - `output/exp.mert/lats/VECFEA/?.vecfea.gz` -- N-best unweighted features</div><div class="line"><a name="l00996"></a><span class="lineno">  996</span>&#160;</div><div class="line"><a name="l00997"></a><span class="lineno">  997</span>&#160;The alilats2splats tool transforms ALILATS alignment lattices</div><div class="line"><a name="l00998"></a><span class="lineno">  998</span>&#160;(transducers) to sparse vector weight lattices; see Section 2.3.1, [\ref deGispert2010] for a detailed explanation.</div><div class="line"><a name="l00999"></a><span class="lineno">  999</span>&#160;</div><div class="line"><a name="l01000"></a><span class="lineno"> 1000</span>&#160;    &gt; alilats2splats.${TGTBINMK}.bin --config=configs/CF.mert.vecfea.nbest --range=1:$M &amp;&gt; log/log.mert.nbest</div><div class="line"><a name="l01001"></a><span class="lineno"> 1001</span>&#160;</div><div class="line"><a name="l01002"></a><span class="lineno"> 1002</span>&#160;The output is written to two sets of files:  </div><div class="line"><a name="l01003"></a><span class="lineno"> 1003</span>&#160;</div><div class="line"><a name="l01004"></a><span class="lineno"> 1004</span>&#160;N-best lists:</div><div class="line"><a name="l01005"></a><span class="lineno"> 1005</span>&#160;</div><div class="line"><a name="l01006"></a><span class="lineno"> 1006</span>&#160;     &gt; zcat -f output/exp.mert/nbest/VECFEA/1.nbest.gz | head -n 2</div><div class="line"><a name="l01007"></a><span class="lineno"> 1007</span>&#160;     &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;        43.0904</div><div class="line"><a name="l01008"></a><span class="lineno"> 1008</span>&#160;     &lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;    43.1757</div><div class="line"><a name="l01009"></a><span class="lineno"> 1009</span>&#160;</div><div class="line"><a name="l01010"></a><span class="lineno"> 1010</span>&#160;Vecfea files:</div><div class="line"><a name="l01011"></a><span class="lineno"> 1011</span>&#160;</div><div class="line"><a name="l01012"></a><span class="lineno"> 1012</span>&#160;     &gt; zcat -f output/exp.mert/nbest/VECFEA/1.vecfea | head -n 2 </div><div class="line"><a name="l01013"></a><span class="lineno"> 1013</span>&#160;     62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293</div><div class="line"><a name="l01014"></a><span class="lineno"> 1014</span>&#160;     63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482</div><div class="line"><a name="l01015"></a><span class="lineno"> 1015</span>&#160;</div><div class="line"><a name="l01016"></a><span class="lineno"> 1016</span>&#160;- Line `n` in the nbest list is the `n-th&#39; translation hypotheses, as ranked under the combined grammar and language model scores.</div><div class="line"><a name="l01017"></a><span class="lineno"> 1017</span>&#160;- Line `n` in the vecfea file is a vector obtained by summing the unweighted feature vectors of each rule in the best derivation of the `n-th` hypothesis</div><div class="line"><a name="l01018"></a><span class="lineno"> 1018</span>&#160;</div><div class="line"><a name="l01019"></a><span class="lineno"> 1019</span>&#160;The alilats2splats tool works as follows:</div><div class="line"><a name="l01020"></a><span class="lineno"> 1020</span>&#160;</div><div class="line"><a name="l01021"></a><span class="lineno"> 1021</span>&#160;- The translation grammar with (unweighted) feature vectors is loaded</div><div class="line"><a name="l01022"></a><span class="lineno"> 1022</span>&#160;- a Rule Flower acceptor, R, is created.  This is an acceptor for rule sequences that applies vector weights (specifically, the feature vector for each rule).  See Fig. 9 of [\ref deGispert2010] for an example and an explanation.</div><div class="line"><a name="l01023"></a><span class="lineno"> 1023</span>&#160;- For each source sentence, the ALILATS derivation-to-translation transducer from Step 2 is loaded, and its weights are removed.  Call this T_u</div><div class="line"><a name="l01024"></a><span class="lineno"> 1024</span>&#160;- The unweighted derivations-to-translation trandsducer T_u is composed with the Rule Flower acceptor R under the  tropical sparse tuple weight semiring with the same feature vectors as are used to generate the translation.  </div><div class="line"><a name="l01025"></a><span class="lineno"> 1025</span>&#160;- The feature vector for the best scoring derivation for every translation is found as Determinise(Project_output(R o T_u) )</div><div class="line"><a name="l01026"></a><span class="lineno"> 1026</span>&#160;- Language model scores M_1, ..., M_m are applied (again in the  tropical sparse tuple weight semiring, so that each score ends up in a separate element in the vector) as Determinise(Project_output(R o T_u) ) o M_1 o ... o M_m</div><div class="line"><a name="l01027"></a><span class="lineno"> 1027</span>&#160;</div><div class="line"><a name="l01028"></a><span class="lineno"> 1028</span>&#160;Writing of N-Best lists and features is</div><div class="line"><a name="l01029"></a><span class="lineno"> 1029</span>&#160;controlled by the `sparseweightvectorlattice` options `storenbestfile`</div><div class="line"><a name="l01030"></a><span class="lineno"> 1030</span>&#160;and `storefeaturefile`:</div><div class="line"><a name="l01031"></a><span class="lineno"> 1031</span>&#160;</div><div class="line"><a name="l01032"></a><span class="lineno"> 1032</span>&#160;    [sparseweightvectorlattice]</div><div class="line"><a name="l01033"></a><span class="lineno"> 1033</span>&#160;    loadalilats=output/exp.mert/nbest/ALILATS/?.fst.gz</div><div class="line"><a name="l01034"></a><span class="lineno"> 1034</span>&#160;    storenbestfile=output/exp.mert/nbest/VECFEA/?.nbest</div><div class="line"><a name="l01035"></a><span class="lineno"> 1035</span>&#160;    storefeaturefile=output/exp.mert/nbest/VECFEA/?.vecfea</div><div class="line"><a name="l01036"></a><span class="lineno"> 1036</span>&#160;    wordmap=wmaps/wmt13.en.wmap</div><div class="line"><a name="l01037"></a><span class="lineno"> 1037</span>&#160;</div><div class="line"><a name="l01038"></a><span class="lineno"> 1038</span>&#160;With the wordmap specified, the output of alilats2splats is in readable form in the</div><div class="line"><a name="l01039"></a><span class="lineno"> 1039</span>&#160;target language. Note that the sentence boundary symbols and the combined</div><div class="line"><a name="l01040"></a><span class="lineno"> 1040</span>&#160;grammar and language model score appear in the nbest file.</div><div class="line"><a name="l01041"></a><span class="lineno"> 1041</span>&#160;The N-best lists have the format</div><div class="line"><a name="l01042"></a><span class="lineno"> 1042</span>&#160;- wordindex1 wordindex2 ... translation_score</div><div class="line"><a name="l01043"></a><span class="lineno"> 1043</span>&#160;</div><div class="line"><a name="l01044"></a><span class="lineno"> 1044</span>&#160;The relationship of feature vectors and scores at the hypothesis level is as follows:</div><div class="line"><a name="l01045"></a><span class="lineno"> 1045</span>&#160;- Suppose there are m language models, with weights s_1 ...,s_m .</div><div class="line"><a name="l01046"></a><span class="lineno"> 1046</span>&#160;  - These weights are specified by the HiFST parameters `lm.featureweights=s_1,s_2,..,s_m`</div><div class="line"><a name="l01047"></a><span class="lineno"> 1047</span>&#160;- Suppose there are n dimensional feature vectors for each rule,</div><div class="line"><a name="l01048"></a><span class="lineno"> 1048</span>&#160;  - The weights to be applied are specified by the HiFST parameters `grammar.featureweights=w_1,..,w_n`</div><div class="line"><a name="l01049"></a><span class="lineno"> 1049</span>&#160;- A feature weight vector is formed as P = [s_1 ... s_m w_1 ... w_n]</div><div class="line"><a name="l01050"></a><span class="lineno"> 1050</span>&#160;- A translation hypothesis e has a feature vector F(e) = [lm_1(e) ... lm_m(e) f_1(e) ... f_n(e)]</div><div class="line"><a name="l01051"></a><span class="lineno"> 1051</span>&#160;  - lm_i(e): the i-th language model score for e</div><div class="line"><a name="l01052"></a><span class="lineno"> 1052</span>&#160;  - f_j(e): j-th grammar feature (see Section 3.2.1, [\ref deGispert2010])</div><div class="line"><a name="l01053"></a><span class="lineno"> 1053</span>&#160;- The score of translation hypothesis e can be found as S(e) = F(e) . P (dot product)</div><div class="line"><a name="l01054"></a><span class="lineno"> 1054</span>&#160;</div><div class="line"><a name="l01055"></a><span class="lineno"> 1055</span>&#160;Each line k in the feature file has the format</div><div class="line"><a name="l01056"></a><span class="lineno"> 1056</span>&#160;- lm_1(e_k) ... lm_m(e_k) f_1(e_k) ... f_n(e_k)</div><div class="line"><a name="l01057"></a><span class="lineno"> 1057</span>&#160;</div><div class="line"><a name="l01058"></a><span class="lineno"> 1058</span>&#160;which are the unweighted feature values for the k-th hypothesis, e.g.</div><div class="line"><a name="l01059"></a><span class="lineno"> 1059</span>&#160;</div><div class="line"><a name="l01060"></a><span class="lineno"> 1060</span>&#160;     &gt; zcat -f output/exp.mert/nbest/VECFEA/1.vecfea.gz | head -n 2</div><div class="line"><a name="l01061"></a><span class="lineno"> 1061</span>&#160;     62.5442 10.8672 8.3936 -16.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 16.3076 40.5293</div><div class="line"><a name="l01062"></a><span class="lineno"> 1062</span>&#160;     63.1159 12.8613 8.7959 -17.0000 -8.0000 -5.0000 0.0000  -1.0000 0.0000  -7.0000 17.0010 43.9482</div><div class="line"><a name="l01063"></a><span class="lineno"> 1063</span>&#160;</div><div class="line"><a name="l01064"></a><span class="lineno"> 1064</span>&#160;and the translation_score in line k is F(e_k) . P</div><div class="line"><a name="l01065"></a><span class="lineno"> 1065</span>&#160;</div><div class="line"><a name="l01066"></a><span class="lineno"> 1066</span>&#160;    &gt; zcat -f output/exp.mert/nbest/VECFEA/1.nbest.gz | head -n 2</div><div class="line"><a name="l01067"></a><span class="lineno"> 1067</span>&#160;    &lt;s&gt; parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt; 43.0904</div><div class="line"><a name="l01068"></a><span class="lineno"> 1068</span>&#160;    &lt;s&gt; the parliament does not support the amendment , which gives you the freedom of tymoshenko &lt;/s&gt;     43.1757</div><div class="line"><a name="l01069"></a><span class="lineno"> 1069</span>&#160;</div><div class="line"><a name="l01070"></a><span class="lineno"> 1070</span>&#160;</div><div class="line"><a name="l01071"></a><span class="lineno"> 1071</span>&#160;\section lmert Lattice MERT</div><div class="line"><a name="l01072"></a><span class="lineno"> 1072</span>&#160;</div><div class="line"><a name="l01073"></a><span class="lineno"> 1073</span>&#160;This section describes how to:</div><div class="line"><a name="l01074"></a><span class="lineno"> 1074</span>&#160; </div><div class="line"><a name="l01075"></a><span class="lineno"> 1075</span>&#160;- generate lattices for use with LMERT [\ref Macherey2008]</div><div class="line"><a name="l01076"></a><span class="lineno"> 1076</span>&#160;- run the HiFST implementations of LMERT for iterative parameter estimation  [\ref Waite2012]</div><div class="line"><a name="l01077"></a><span class="lineno"> 1077</span>&#160;</div><div class="line"><a name="l01078"></a><span class="lineno"> 1078</span>&#160;This HiFST release includes an implementation of LMERT [\ref Waite2012].  The</div><div class="line"><a name="l01079"></a><span class="lineno"> 1079</span>&#160;script `HiFST_lmert` runs several iterations of lattice generation and</div><div class="line"><a name="l01080"></a><span class="lineno"> 1080</span>&#160;parameter estimation using the `lmert` tool.</div><div class="line"><a name="l01081"></a><span class="lineno"> 1081</span>&#160;</div><div class="line"><a name="l01082"></a><span class="lineno"> 1082</span>&#160;    &gt; export M=10 # run lmert over 10 sentences; for exposition only.   </div><div class="line"><a name="l01083"></a><span class="lineno"> 1083</span>&#160;    &gt; scripts/HiFST_lmert</div><div class="line"><a name="l01084"></a><span class="lineno"> 1084</span>&#160;</div><div class="line"><a name="l01085"></a><span class="lineno"> 1085</span>&#160;This script runs 3 iterations of LMERT, with each iteration consisting of the four steps that follow in the sections below.</div><div class="line"><a name="l01086"></a><span class="lineno"> 1086</span>&#160;In our experience, on a X86_64 Linux computer with 4 2.8GHz CPUs and 24GB RAM, each iteration takes ca. 3 hours over the full 1502 sentence tuning set.  </div><div class="line"><a name="l01087"></a><span class="lineno"> 1087</span>&#160;Output from each iteration is written to files `log/log.lmert.[1,2,3,4]`.  For example,</div><div class="line"><a name="l01088"></a><span class="lineno"> 1088</span>&#160;</div><div class="line"><a name="l01089"></a><span class="lineno"> 1089</span>&#160;    &gt; tail -n 8 log/log.lmert.?</div><div class="line"><a name="l01090"></a><span class="lineno"> 1090</span>&#160;    ==&gt; log/log.lmert.1 &lt;==</div><div class="line"><a name="l01091"></a><span class="lineno"> 1091</span>&#160;    Fri Apr  3 21:04:07 2015: RandomLineSearch.INF:Bleu gain less than threshold. Exiting.</div><div class="line"><a name="l01092"></a><span class="lineno"> 1092</span>&#160;    Fri Apr  3 21:04:07 2015: RandomLineSearch.INF:Initial Bleu: 0.308047 (0.990986)</div><div class="line"><a name="l01093"></a><span class="lineno"> 1093</span>&#160;    Fri Apr  3 21:04:07 2015: RandomLineSearch.INF:Final Bleu:   0.326275 (0.996078)</div><div class="line"><a name="l01094"></a><span class="lineno"> 1094</span>&#160;    Fri Apr  3 21:04:07 2015: RandomLineSearch.INF:Final Lambda: 1 0.804341 0.857127 2.94424 -1.08039 1.31302 41.8783 -6.55013 -5.76312 -0.843428 0.294197 0.300588 </div><div class="line"><a name="l01095"></a><span class="lineno"> 1095</span>&#160;    Fri Apr  3 21:04:07 2015: main.INF:lmert.O2.bin finished!</div><div class="line"><a name="l01096"></a><span class="lineno"> 1096</span>&#160;    ==Params</div><div class="line"><a name="l01097"></a><span class="lineno"> 1097</span>&#160;    Fri Apr  3 21:04:10 BST 2015</div><div class="line"><a name="l01098"></a><span class="lineno"> 1098</span>&#160;    1,0.804341,0.857127,2.94424,-1.08039,1.31302,41.8783,-6.55013,-5.76312,-0.843428,0.294197,0.300588</div><div class="line"><a name="l01099"></a><span class="lineno"> 1099</span>&#160;    ==&gt; log/log.lmert.2 &lt;==</div><div class="line"><a name="l01100"></a><span class="lineno"> 1100</span>&#160;    Sat Apr  4 00:12:01 2015: RandomLineSearch.INF:Bleu gain less than threshold. Exiting.</div><div class="line"><a name="l01101"></a><span class="lineno"> 1101</span>&#160;    Sat Apr  4 00:12:01 2015: RandomLineSearch.INF:Initial Bleu: 0.323506 (0.998028)</div><div class="line"><a name="l01102"></a><span class="lineno"> 1102</span>&#160;    Sat Apr  4 00:12:01 2015: RandomLineSearch.INF:Final Bleu:   0.326441 (0.999893)</div><div class="line"><a name="l01103"></a><span class="lineno"> 1103</span>&#160;    Sat Apr  4 00:12:01 2015: RandomLineSearch.INF:Final Lambda: 1 1.05897 0.893133 2.81302 -0.752161 1.2221 35.8963 -6.62349 -4.37219 -0.583849 0.242874 0.194865 </div><div class="line"><a name="l01104"></a><span class="lineno"> 1104</span>&#160;    Sat Apr  4 00:12:01 2015: main.INF:lmert.O2.bin finished!</div><div class="line"><a name="l01105"></a><span class="lineno"> 1105</span>&#160;    ==Params</div><div class="line"><a name="l01106"></a><span class="lineno"> 1106</span>&#160;    Sat Apr  4 00:12:04 BST 2015</div><div class="line"><a name="l01107"></a><span class="lineno"> 1107</span>&#160;    1,1.05897,0.893133,2.81302,-0.752161,1.2221,35.8963,-6.62349,-4.37219,-0.583849,0.242874,0.194865</div><div class="line"><a name="l01108"></a><span class="lineno"> 1108</span>&#160;</div><div class="line"><a name="l01109"></a><span class="lineno"> 1109</span>&#160;    ==&gt; log/log.lmert.3 &lt;==</div><div class="line"><a name="l01110"></a><span class="lineno"> 1110</span>&#160;    Sat Apr  4 04:55:40 2015: RandomLineSearch.INF:Bleu gain less than threshold. Exiting.</div><div class="line"><a name="l01111"></a><span class="lineno"> 1111</span>&#160;    Sat Apr  4 04:55:40 2015: RandomLineSearch.INF:Initial Bleu: 0.326105 (0.999734)</div><div class="line"><a name="l01112"></a><span class="lineno"> 1112</span>&#160;    Sat Apr  4 04:55:40 2015: RandomLineSearch.INF:Final Bleu:   0.327024 (0.999893)</div><div class="line"><a name="l01113"></a><span class="lineno"> 1113</span>&#160;    Sat Apr  4 04:55:40 2015: RandomLineSearch.INF:Final Lambda: 1 1.05159 0.88871 2.80248 -0.750582 1.22436 30.0357 -5.14523 -2.57261 -0.585201 0.249596 0.196453 </div><div class="line"><a name="l01114"></a><span class="lineno"> 1114</span>&#160;    Sat Apr  4 04:55:40 2015: main.INF:lmert.O2.bin finished!</div><div class="line"><a name="l01115"></a><span class="lineno"> 1115</span>&#160;    ==Params</div><div class="line"><a name="l01116"></a><span class="lineno"> 1116</span>&#160;    Sat Apr  4 04:55:43 BST 2015</div><div class="line"><a name="l01117"></a><span class="lineno"> 1117</span>&#160;    1,1.05159,0.88871,2.80248,-0.750582,1.22436,30.0357,-5.14523,-2.57261,-0.585201,0.249596,0.196453</div><div class="line"><a name="l01118"></a><span class="lineno"> 1118</span>&#160;</div><div class="line"><a name="l01119"></a><span class="lineno"> 1119</span>&#160;</div><div class="line"><a name="l01120"></a><span class="lineno"> 1120</span>&#160;</div><div class="line"><a name="l01121"></a><span class="lineno"> 1121</span>&#160;This indicates:</div><div class="line"><a name="l01122"></a><span class="lineno"> 1122</span>&#160;- The initial set of parameters yields a tuning set BLEU score of 0.308047, with brevity penalty 0.990986</div><div class="line"><a name="l01123"></a><span class="lineno"> 1123</span>&#160;- The first iteration of LMERT improves the tuning set BLEU score to 0.32652 over the tuning set lattices generated with the initial parameters. </div><div class="line"><a name="l01124"></a><span class="lineno"> 1124</span>&#160;- Retranslation with the parameters found at iteration 1 yields a tuning set BLEU score of 0.32488, with brevity penalty 0.999654</div><div class="line"><a name="l01125"></a><span class="lineno"> 1125</span>&#160;- The second iteration of LMERT improves the tuning set BLEU score to 0.326884 over the tuning set lattices generated with the parameters from iteration 1</div><div class="line"><a name="l01126"></a><span class="lineno"> 1126</span>&#160;</div><div class="line"><a name="l01127"></a><span class="lineno"> 1127</span>&#160;Notes:</div><div class="line"><a name="l01128"></a><span class="lineno"> 1128</span>&#160;- The parameters used to initialise this demo are taken from the baseline system, and so few iterations are needed for LMERT to converge.  Even when started from a flat start, LMERT tends to converge in fewer iterations than N-Best MERT.</div><div class="line"><a name="l01129"></a><span class="lineno"> 1129</span>&#160;- The n-gram language model is set by default to</div><div class="line"><a name="l01130"></a><span class="lineno"> 1130</span>&#160;`M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap`.</div><div class="line"><a name="l01131"></a><span class="lineno"> 1131</span>&#160;This is a quantized version of</div><div class="line"><a name="l01132"></a><span class="lineno"> 1132</span>&#160;`M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap`.</div><div class="line"><a name="l01133"></a><span class="lineno"> 1133</span>&#160;Slightly higher tuning set BLEU scores can be gotten with the unquantized</div><div class="line"><a name="l01134"></a><span class="lineno"> 1134</span>&#160;LM, although memory use in tuning will be higher.</div><div class="line"><a name="l01135"></a><span class="lineno"> 1135</span>&#160;- The script `HiFST_lmert` can be modified to perform</div><div class="line"><a name="l01136"></a><span class="lineno"> 1136</span>&#160;LMERT over only the first (e.g.) 100 tuning set sentences.  This can</div><div class="line"><a name="l01137"></a><span class="lineno"> 1137</span>&#160;be done for debugging / demonstration, in that processing will be much</div><div class="line"><a name="l01138"></a><span class="lineno"> 1138</span>&#160;faster, although the estimated parameters will not be as robust.</div><div class="line"><a name="l01139"></a><span class="lineno"> 1139</span>&#160;</div><div class="line"><a name="l01140"></a><span class="lineno"> 1140</span>&#160;The operations done by the `HiFST_lmert` script are described next.</div><div class="line"><a name="l01141"></a><span class="lineno"> 1141</span>&#160;</div><div class="line"><a name="l01142"></a><span class="lineno"> 1142</span>&#160;\subsection lmert_hyps Step 1. Hypotheses for LMERT</div><div class="line"><a name="l01143"></a><span class="lineno"> 1143</span>&#160;</div><div class="line"><a name="l01144"></a><span class="lineno"> 1144</span>&#160;Note that this step is also done in MERT (\ref mert), although the settings here are slightly different.</div><div class="line"><a name="l01145"></a><span class="lineno"> 1145</span>&#160;</div><div class="line"><a name="l01146"></a><span class="lineno"> 1146</span>&#160;The following command is run at iteration `$it` ,  and will generate lattices for `$M` files.</div><div class="line"><a name="l01147"></a><span class="lineno"> 1147</span>&#160;</div><div class="line"><a name="l01148"></a><span class="lineno"> 1148</span>&#160;- Input:</div><div class="line"><a name="l01149"></a><span class="lineno"> 1149</span>&#160;  - `$it` -- lmert iteration (1, 2, ...)</div><div class="line"><a name="l01150"></a><span class="lineno"> 1150</span>&#160;  - `$M` -- number of sentences to process</div><div class="line"><a name="l01151"></a><span class="lineno"> 1151</span>&#160;  - `RU/RU.tune.idx` -- tuning set source language sentences</div><div class="line"><a name="l01152"></a><span class="lineno"> 1152</span>&#160;  - `G/rules.shallow.vecfea.gz` -- translation grammar</div><div class="line"><a name="l01153"></a><span class="lineno"> 1153</span>&#160;  - `M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.union.mmap` -- target language model</div><div class="line"><a name="l01154"></a><span class="lineno"> 1154</span>&#160;  - language model and translation grammar feature weights, provided via command line option</div><div class="line"><a name="l01155"></a><span class="lineno"> 1155</span>&#160;- Output</div><div class="line"><a name="l01156"></a><span class="lineno"> 1156</span>&#160;  - `output/exp.lmert/$it/LATS/?.fst.gz` -- word lattices (WFSAs), determinized and minimized</div><div class="line"><a name="l01157"></a><span class="lineno"> 1157</span>&#160;  - `output/exp.lmert/$it/hyps` -- translation hyps (discarded)</div><div class="line"><a name="l01158"></a><span class="lineno"> 1158</span>&#160;</div><div class="line"><a name="l01159"></a><span class="lineno"> 1159</span>&#160;The feature weights are gathered into a single vector, and set via the `--featureweights` command line option</div><div class="line"><a name="l01160"></a><span class="lineno"> 1160</span>&#160;</div><div class="line"><a name="l01161"></a><span class="lineno"> 1161</span>&#160;- The first parameter is the grammar scale factor (in the MERT demo, this is set via `lm.featureweights`)</div><div class="line"><a name="l01162"></a><span class="lineno"> 1162</span>&#160;- The remaining parameters are weights for the grammar features (in the MERT demo, these are set via  `grammar.featureweights`)</div><div class="line"><a name="l01163"></a><span class="lineno"> 1163</span>&#160;</div><div class="line"><a name="l01164"></a><span class="lineno"> 1164</span>&#160;The initial parameters are</div><div class="line"><a name="l01165"></a><span class="lineno"> 1165</span>&#160;</div><div class="line"><a name="l01166"></a><span class="lineno"> 1166</span>&#160;    it=1</div><div class="line"><a name="l01167"></a><span class="lineno"> 1167</span>&#160;    export M=10</div><div class="line"><a name="l01168"></a><span class="lineno"> 1168</span>&#160;    FW=1.0,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136</div><div class="line"><a name="l01169"></a><span class="lineno"> 1169</span>&#160;</div><div class="line"><a name="l01170"></a><span class="lineno"> 1170</span>&#160;HiFST is run in translation mode as</div><div class="line"><a name="l01171"></a><span class="lineno"> 1171</span>&#160;</div><div class="line"><a name="l01172"></a><span class="lineno"> 1172</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.lmert.hyps --range=1:$M --featureweights=$FW --target.store=output/exp.lmert/$it/hyps --hifst.lattice.store=output/exp.lmert/$it/LATS/?.fst.gz</div><div class="line"><a name="l01173"></a><span class="lineno"> 1173</span>&#160;</div><div class="line"><a name="l01174"></a><span class="lineno"> 1174</span>&#160;</div><div class="line"><a name="l01175"></a><span class="lineno"> 1175</span>&#160;\subsection lmert_veclats Step 2. Guided Translation / Forced Alignment</div><div class="line"><a name="l01176"></a><span class="lineno"> 1176</span>&#160;</div><div class="line"><a name="l01177"></a><span class="lineno"> 1177</span>&#160;- Input:</div><div class="line"><a name="l01178"></a><span class="lineno"> 1178</span>&#160;  - `$it` -- lmert iteration (1, 2, ...)</div><div class="line"><a name="l01179"></a><span class="lineno"> 1179</span>&#160;  - `$M` -- number of sentences to process</div><div class="line"><a name="l01180"></a><span class="lineno"> 1180</span>&#160;  - `output/exp.lmert/$it/LATS/?.fst.gz` -- word lattices (WFSAs), determinized and minimized (from \ref lmert_hyps)</div><div class="line"><a name="l01181"></a><span class="lineno"> 1181</span>&#160;- Output:</div><div class="line"><a name="l01182"></a><span class="lineno"> 1182</span>&#160;  - `output/exp.lmert/$it/ALILATS/?.fst.gz` -- transducers mapping derivations to translations (e.g. Fig. 7, [\ref deGispert2010])</div><div class="line"><a name="l01183"></a><span class="lineno"> 1183</span>&#160;  - `output/exp.lmert/$it/hyps` -- translation hyps (discarded)</div><div class="line"><a name="l01184"></a><span class="lineno"> 1184</span>&#160;</div><div class="line"><a name="l01185"></a><span class="lineno"> 1185</span>&#160;HiFST is run in alignment mode.  Lattices (from `output/exp.lmert/$it/LATS/?.fst.gz`)</div><div class="line"><a name="l01186"></a><span class="lineno"> 1186</span>&#160;read and transformed into substring acceptors used to constrain the</div><div class="line"><a name="l01187"></a><span class="lineno"> 1187</span>&#160;space of alignments</div><div class="line"><a name="l01188"></a><span class="lineno"> 1188</span>&#160;</div><div class="line"><a name="l01189"></a><span class="lineno"> 1189</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.lmert.alilats --range=1:$M --referencefilter.load=output/exp.lmert/$it/LATS/?.fst.gz --target.store=output/exp.lmert/$it/hyps --hifst.lattice.store=output/exp.lmert/$it/ALILATS/?.fst.gz</div><div class="line"><a name="l01190"></a><span class="lineno"> 1190</span>&#160;</div><div class="line"><a name="l01191"></a><span class="lineno"> 1191</span>&#160;Note the following parameters in the configuration file:</div><div class="line"><a name="l01192"></a><span class="lineno"> 1192</span>&#160;</div><div class="line"><a name="l01193"></a><span class="lineno"> 1193</span>&#160;    [referencefilter]</div><div class="line"><a name="l01194"></a><span class="lineno"> 1194</span>&#160;    prunereferenceweight=4</div><div class="line"><a name="l01195"></a><span class="lineno"> 1195</span>&#160;    # pruning threshold to be applied to input lattice prior to alignment</div><div class="line"><a name="l01196"></a><span class="lineno"> 1196</span>&#160;    prunereferenceshortestpath=10000</div><div class="line"><a name="l01197"></a><span class="lineno"> 1197</span>&#160;    # extract n-best list from input lattice to use as hypotheses</div><div class="line"><a name="l01198"></a><span class="lineno"> 1198</span>&#160;</div><div class="line"><a name="l01199"></a><span class="lineno"> 1199</span>&#160;Reference lattices are read from `output/exp.lmert/$it/LATS/?.fst.gz`. For each lattice:</div><div class="line"><a name="l01200"></a><span class="lineno"> 1200</span>&#160;- an N-Best list of depth 10000 is extracted using `fstshortestpath`</div><div class="line"><a name="l01201"></a><span class="lineno"> 1201</span>&#160;- the lattices are pruned with threshold `prunereferenceweight=4`</div><div class="line"><a name="l01202"></a><span class="lineno"> 1202</span>&#160;- the pruned lattice is unioned with the N-Best list</div><div class="line"><a name="l01203"></a><span class="lineno"> 1203</span>&#160;- the resulting WFSA is transformed (after removing weights, minimization and determinization) into a substring acceptor to be used in alignment</div><div class="line"><a name="l01204"></a><span class="lineno"> 1204</span>&#160;</div><div class="line"><a name="l01205"></a><span class="lineno"> 1205</span>&#160;A simpler approach could be simply to prune the reference lattices.</div><div class="line"><a name="l01206"></a><span class="lineno"> 1206</span>&#160;However in practice it can be difficult to find a global pruning threshold that always</div><div class="line"><a name="l01207"></a><span class="lineno"> 1207</span>&#160;yields a reference lattice that is big enough, but not too big.</div><div class="line"><a name="l01208"></a><span class="lineno"> 1208</span>&#160;Including the n-best list ensures that there will always be a rich set of candidate hypotheses.</div><div class="line"><a name="l01209"></a><span class="lineno"> 1209</span>&#160;</div><div class="line"><a name="l01210"></a><span class="lineno"> 1210</span>&#160;\subsection lmert_alilats Step 3. WFSAs with Unweighted Feature Vectors</div><div class="line"><a name="l01211"></a><span class="lineno"> 1211</span>&#160;</div><div class="line"><a name="l01212"></a><span class="lineno"> 1212</span>&#160;- Input:</div><div class="line"><a name="l01213"></a><span class="lineno"> 1213</span>&#160;  - `$it` -- lmert iteration (1, 2, ...)</div><div class="line"><a name="l01214"></a><span class="lineno"> 1214</span>&#160;  - `$M` -- number of sentences to process</div><div class="line"><a name="l01215"></a><span class="lineno"> 1215</span>&#160;  - language model and translation grammar feature weights, provided via command line options</div><div class="line"><a name="l01216"></a><span class="lineno"> 1216</span>&#160;  - `output/exp.lmert/$it/ALILATS/?.fst.gz` -- transducers mapping derivations to translations</div><div class="line"><a name="l01217"></a><span class="lineno"> 1217</span>&#160;- Output:</div><div class="line"><a name="l01218"></a><span class="lineno"> 1218</span>&#160;  - output/exp.lmert/$it/VECFEA/?.fst.gz -- translation lattices (WFSAs) with unweighted feature vectors</div><div class="line"><a name="l01219"></a><span class="lineno"> 1219</span>&#160;</div><div class="line"><a name="l01220"></a><span class="lineno"> 1220</span>&#160;`alilats2splats` transforms ALILATS alignment lattices to sparse</div><div class="line"><a name="l01221"></a><span class="lineno"> 1221</span>&#160;vector weight lattices; see Section 2.3.1, [\ref deGispert2010] for a</div><div class="line"><a name="l01222"></a><span class="lineno"> 1222</span>&#160;detailed explanation.  A single output WFSA with sparse vector weights</div><div class="line"><a name="l01223"></a><span class="lineno"> 1223</span>&#160;is written for each translation.  Note that this is different from the</div><div class="line"><a name="l01224"></a><span class="lineno"> 1224</span>&#160;MERT case, where two N-best lists of hypotheses</div><div class="line"><a name="l01225"></a><span class="lineno"> 1225</span>&#160;and features are written for each translation.</div><div class="line"><a name="l01226"></a><span class="lineno"> 1226</span>&#160;</div><div class="line"><a name="l01227"></a><span class="lineno"> 1227</span>&#160;The HiFST `alilats2splats` command is</div><div class="line"><a name="l01228"></a><span class="lineno"> 1228</span>&#160;</div><div class="line"><a name="l01229"></a><span class="lineno"> 1229</span>&#160;    &gt; alilats2splats.${TGTBINMK}.bin --config=configs/CF.lmert.vecfea --range=1:$M --featureweights=$FW --sparseweightvectorlattice.loadalilats=output/exp.lmert/$it/ALILATS/?.fst.gz --sparseweightvectorlattice.store=output/exp.lmert/$it/VECFEA/?.fst.gz</div><div class="line"><a name="l01230"></a><span class="lineno"> 1230</span>&#160;</div><div class="line"><a name="l01231"></a><span class="lineno"> 1231</span>&#160;\subsection lmert_lmert Step 4. LMERT</div><div class="line"><a name="l01232"></a><span class="lineno"> 1232</span>&#160;</div><div class="line"><a name="l01233"></a><span class="lineno"> 1233</span>&#160;- Input:</div><div class="line"><a name="l01234"></a><span class="lineno"> 1234</span>&#160;  - `$it` -- lmert iteration (1, 2, ...)</div><div class="line"><a name="l01235"></a><span class="lineno"> 1235</span>&#160;  - `$M` -- number of sentences to process</div><div class="line"><a name="l01236"></a><span class="lineno"> 1236</span>&#160;  - `output/exp.lmert/$it/VECFEA/?.fst.gz` -- translation lattices (WFSAs) with unweighted feature vectors (from \ref lmert_alilats)</div><div class="line"><a name="l01237"></a><span class="lineno"> 1237</span>&#160;  - `EN/EN.tune.idx` -- target language references in integer format</div><div class="line"><a name="l01238"></a><span class="lineno"> 1238</span>&#160;- Output:</div><div class="line"><a name="l01239"></a><span class="lineno"> 1239</span>&#160;  - `output/exp.lmert/params.$it` -- reestimated feature vector under LMERT with BLEU</div><div class="line"><a name="l01240"></a><span class="lineno"> 1240</span>&#160;</div><div class="line"><a name="l01241"></a><span class="lineno"> 1241</span>&#160;`lmert` runs as follows</div><div class="line"><a name="l01242"></a><span class="lineno"> 1242</span>&#160;</div><div class="line"><a name="l01243"></a><span class="lineno"> 1243</span>&#160;    &gt; lmert.${TGTBINMK}.bin --config=configs/CF.lmert.lmert --range=1:$M \</div><div class="line"><a name="l01244"></a><span class="lineno"> 1244</span>&#160;    --input=output/exp.lmert/$it/VECFEA/?.fst.gz  --initial_params=$FW \</div><div class="line"><a name="l01245"></a><span class="lineno"> 1245</span>&#160;    --write_params=output/exp.lmert/params.$it </div><div class="line"><a name="l01246"></a><span class="lineno"> 1246</span>&#160;</div><div class="line"><a name="l01247"></a><span class="lineno"> 1247</span>&#160;</div><div class="line"><a name="l01248"></a><span class="lineno"> 1248</span>&#160;\subsection lmert_references BLEU, References, and De/Tokenization</div><div class="line"><a name="l01249"></a><span class="lineno"> 1249</span>&#160;</div><div class="line"><a name="l01250"></a><span class="lineno"> 1250</span>&#160;`lmert` computes the BLEU score with respect to one or more reference translations.</div><div class="line"><a name="l01251"></a><span class="lineno"> 1251</span>&#160;References can be provided either as integer-mapped sequences, or as plain text.   </div><div class="line"><a name="l01252"></a><span class="lineno"> 1252</span>&#160;</div><div class="line"><a name="l01253"></a><span class="lineno"> 1253</span>&#160;The example earlier in this section uses integer-mapped reference translations:</div><div class="line"><a name="l01254"></a><span class="lineno"> 1254</span>&#160;</div><div class="line"><a name="l01255"></a><span class="lineno"> 1255</span>&#160;    &gt; head -2 EN/EN.tune.idx </div><div class="line"><a name="l01256"></a><span class="lineno"> 1256</span>&#160;    50 135 20 103 245 9445 23899</div><div class="line"><a name="l01257"></a><span class="lineno"> 1257</span>&#160;    3 245 10 35 578 7 9445 3 5073 972 1052 564 51 13011 317 312 734 6 3 122 14 16306 6 4448 14 119 3570 5</div><div class="line"><a name="l01258"></a><span class="lineno"> 1258</span>&#160;    &gt; lmert.${TGTBINMK}.bin --int_refs=EN/EN.tune.idx --range=1:$M \</div><div class="line"><a name="l01259"></a><span class="lineno"> 1259</span>&#160;    --input=output/exp.lmert/1/VECFEA/?.fst.gz --initial_params=$FW --random_seed=17 \</div><div class="line"><a name="l01260"></a><span class="lineno"> 1260</span>&#160;    --write_params=tmp/params.1</div><div class="line"><a name="l01261"></a><span class="lineno"> 1261</span>&#160;    ...</div><div class="line"><a name="l01262"></a><span class="lineno"> 1262</span>&#160;    Sat Apr  4 11:35:48 2015: RandomLineSearch.INF:Initial Bleu: 0.244382 (1)</div><div class="line"><a name="l01263"></a><span class="lineno"> 1263</span>&#160;    Sat Apr  4 11:35:48 2015: RandomLineSearch.INF:Final Bleu:   0.379273 (1)</div><div class="line"><a name="l01264"></a><span class="lineno"> 1264</span>&#160;    Sat Apr  4 11:35:48 2015: RandomLineSearch.INF:Final Lambda: 1 1.23568 0.883693 3.50594 -0.200368 -1.51527 41.5974 -4.49443 -5.2153 4.82334 -0.343277 0.45311 </div><div class="line"><a name="l01265"></a><span class="lineno"> 1265</span>&#160;    ...    </div><div class="line"><a name="l01266"></a><span class="lineno"> 1266</span>&#160;</div><div class="line"><a name="l01267"></a><span class="lineno"> 1267</span>&#160;An alternative is to use plain-text reference files with the word map; with the same random seed, the results should agree with using integer mapped references:</div><div class="line"><a name="l01268"></a><span class="lineno"> 1268</span>&#160;</div><div class="line"><a name="l01269"></a><span class="lineno"> 1269</span>&#160;    &gt; head -2 EN/EN.tune     </div><div class="line"><a name="l01270"></a><span class="lineno"> 1270</span>&#160;    parliament does not support amendment freeing tymoshenko</div><div class="line"><a name="l01271"></a><span class="lineno"> 1271</span>&#160;    the amendment that would lead to freeing the imprisoned former prime minister was revoked during second reading of the proposal for mitigation of sentences for economic offences .</div><div class="line"><a name="l01272"></a><span class="lineno"> 1272</span>&#160;    &gt; lmert.${TGTBINMK}.bin --word_refs=EN/EN.tune --word_map=wmaps/wmt13.en.wmap --range=1:$M \</div><div class="line"><a name="l01273"></a><span class="lineno"> 1273</span>&#160;    --input=output/exp.lmert/1/VECFEA/?.fst.gz --initial_params=$FW --random_seed=17 \</div><div class="line"><a name="l01274"></a><span class="lineno"> 1274</span>&#160;    --write_params=tmp/params.1</div><div class="line"><a name="l01275"></a><span class="lineno"> 1275</span>&#160;    ...</div><div class="line"><a name="l01276"></a><span class="lineno"> 1276</span>&#160;    Sat Apr  4 11:39:25 2015: RandomLineSearch.INF:Initial Bleu: 0.244382 (1)</div><div class="line"><a name="l01277"></a><span class="lineno"> 1277</span>&#160;    Sat Apr  4 11:39:25 2015: RandomLineSearch.INF:Final Bleu:   0.379273 (1)</div><div class="line"><a name="l01278"></a><span class="lineno"> 1278</span>&#160;    Sat Apr  4 11:39:25 2015: RandomLineSearch.INF:Final Lambda: 1 1.23568 0.883693 3.50594 -0.200368 -1.51527 41.5974 -4.49443 -5.2153 4.82334 -0.343277 0.45311 </div><div class="line"><a name="l01279"></a><span class="lineno"> 1279</span>&#160;    ...</div><div class="line"><a name="l01280"></a><span class="lineno"> 1280</span>&#160;</div><div class="line"><a name="l01281"></a><span class="lineno"> 1281</span>&#160;It is also possible to including de/tokenization of hypotheses prior</div><div class="line"><a name="l01282"></a><span class="lineno"> 1282</span>&#160;to BLEU computation.  For example, references are processed such that</div><div class="line"><a name="l01283"></a><span class="lineno"> 1283</span>&#160;apostrophes are treated as separate tokens:</div><div class="line"><a name="l01284"></a><span class="lineno"> 1284</span>&#160;</div><div class="line"><a name="l01285"></a><span class="lineno"> 1285</span>&#160;    &gt; awk &#39;NR==3&#39; EN/EN.tune</div><div class="line"><a name="l01286"></a><span class="lineno"> 1286</span>&#160;    &gt; the verdict is not yet final ; the court will hear tymoshenko &#39; s appeal in december .</div><div class="line"><a name="l01287"></a><span class="lineno"> 1287</span>&#160;</div><div class="line"><a name="l01288"></a><span class="lineno"> 1288</span>&#160;As an alternative,  a set of references can be created which attach apostrophes to words, and the script used to process the references can be provided to `lmert` as an `external tokenizer`:</div><div class="line"><a name="l01289"></a><span class="lineno"> 1289</span>&#160;</div><div class="line"><a name="l01290"></a><span class="lineno"> 1290</span>&#160;    &gt; echo &quot;s/[ ]*&#39;[ ]*/&#39;/&quot; &gt; tmp/sed.apos </div><div class="line"><a name="l01291"></a><span class="lineno"> 1291</span>&#160;    &gt; sed -f tmp/sed.apos EN/EN.tune &gt; tmp/EN.tune.apos</div><div class="line"><a name="l01292"></a><span class="lineno"> 1292</span>&#160;    &gt; awk &#39;NR==3&#39; tmp/EN.tune.apos</div><div class="line"><a name="l01293"></a><span class="lineno"> 1293</span>&#160;    the verdict is not yet final ; the court will hear tymoshenko&#39;s appeal in december .</div><div class="line"><a name="l01294"></a><span class="lineno"> 1294</span>&#160;    &gt; lmert.${TGTBINMK}.bin --word_refs=EN/EN.tune --word_map=wmaps/wmt13.en.wmap --range=1:$M \</div><div class="line"><a name="l01295"></a><span class="lineno"> 1295</span>&#160;    --input=output/exp.lmert/1/VECFEA/?.fst.gz --initial_params=$FW --random_seed=17 \</div><div class="line"><a name="l01296"></a><span class="lineno"> 1296</span>&#160;    --write_params=tmp/params.1 --external_tokenizer=&quot;tee tmp/before | sed -u -f tmp/sed.apos | tee tmp/after&quot;</div><div class="line"><a name="l01297"></a><span class="lineno"> 1297</span>&#160;    ...</div><div class="line"><a name="l01298"></a><span class="lineno"> 1298</span>&#160;    Sat Apr  4 11:41:38 2015: RandomLineSearch.INF:Initial Bleu: 0.24523 (1)</div><div class="line"><a name="l01299"></a><span class="lineno"> 1299</span>&#160;    Sat Apr  4 11:41:38 2015: RandomLineSearch.INF:Final Bleu:   0.380598 (1)</div><div class="line"><a name="l01300"></a><span class="lineno"> 1300</span>&#160;    Sat Apr  4 11:41:38 2015: RandomLineSearch.INF:Final Lambda: 1 1.23568 0.883693 3.50594 -0.200368 -1.51527 41.5974 -4.49443 -5.2153 4.82334 -0.343277 0.45311 </div><div class="line"><a name="l01301"></a><span class="lineno"> 1301</span>&#160;    ...</div><div class="line"><a name="l01302"></a><span class="lineno"> 1302</span>&#160;</div><div class="line"><a name="l01303"></a><span class="lineno"> 1303</span>&#160;The `tee` command makes it possible to compare hypotheses before and after processing:</div><div class="line"><a name="l01304"></a><span class="lineno"> 1304</span>&#160;</div><div class="line"><a name="l01305"></a><span class="lineno"> 1305</span>&#160;     &gt; diff tmp/before tmp/after | head -4</div><div class="line"><a name="l01306"></a><span class="lineno"> 1306</span>&#160;     9c9</div><div class="line"><a name="l01307"></a><span class="lineno"> 1307</span>&#160;     &lt; instead of the dictator &#39;s society is composed of rival clans , will be merged the koran .</div><div class="line"><a name="l01308"></a><span class="lineno"> 1308</span>&#160;     ---</div><div class="line"><a name="l01309"></a><span class="lineno"> 1309</span>&#160;     &gt; instead of the dictator&#39;s society is composed of rival clans , will be merged the koran .</div><div class="line"><a name="l01310"></a><span class="lineno"> 1310</span>&#160;</div><div class="line"><a name="l01311"></a><span class="lineno"> 1311</span>&#160;Lmert optimises the BLEU score over the latter sets of hypotheses.</div><div class="line"><a name="l01312"></a><span class="lineno"> 1312</span>&#160;**Note** that it is possible to use the</div><div class="line"><a name="l01313"></a><span class="lineno"> 1313</span>&#160;`external_tokenizer` with integer references, but the external</div><div class="line"><a name="l01314"></a><span class="lineno"> 1314</span>&#160;tokenizer will have to be able to read integer sequences at its input</div><div class="line"><a name="l01315"></a><span class="lineno"> 1315</span>&#160;and write integer sequences at its output, i.e. it will have to apply</div><div class="line"><a name="l01316"></a><span class="lineno"> 1316</span>&#160;a word map internally.</div><div class="line"><a name="l01317"></a><span class="lineno"> 1317</span>&#160;</div><div class="line"><a name="l01318"></a><span class="lineno"> 1318</span>&#160;\section lmert_veclats_tst Tropical Sparse Tuple Semiring</div><div class="line"><a name="l01319"></a><span class="lineno"> 1319</span>&#160;</div><div class="line"><a name="l01320"></a><span class="lineno"> 1320</span>&#160;The lattices generated by `alilats2splats` in `output/exp.lmert/1/lats/VECFEA` are `tropicalsparsetuple` vector weight lattices.</div><div class="line"><a name="l01321"></a><span class="lineno"> 1321</span>&#160;</div><div class="line"><a name="l01322"></a><span class="lineno"> 1322</span>&#160;    &gt; zcat output/exp.lmert/1/VECFEA/1.fst.gz | fstinfo | head -n 2</div><div class="line"><a name="l01323"></a><span class="lineno"> 1323</span>&#160;    fst type                                          vector</div><div class="line"><a name="l01324"></a><span class="lineno"> 1324</span>&#160;    arc type                                          tropicalsparsetuple</div><div class="line"><a name="l01325"></a><span class="lineno"> 1325</span>&#160;</div><div class="line"><a name="l01326"></a><span class="lineno"> 1326</span>&#160;The scores in these lattices are unweighted by the feature vector</div><div class="line"><a name="l01327"></a><span class="lineno"> 1327</span>&#160;weights, i.e. they are the raw feature scores against which L/MERT finds</div><div class="line"><a name="l01328"></a><span class="lineno"> 1328</span>&#160;the optimal parameter vector values.  Distances under these unweighted</div><div class="line"><a name="l01329"></a><span class="lineno"> 1329</span>&#160;vectors do not agree with the initial translation hypotheses, e.g. the</div><div class="line"><a name="l01330"></a><span class="lineno"> 1330</span>&#160;shortest-path does not agree with the best translation:</div><div class="line"><a name="l01331"></a><span class="lineno"> 1331</span>&#160;</div><div class="line"><a name="l01332"></a><span class="lineno"> 1332</span>&#160;    &gt; unset TUPLEARC_WEIGHT_VECTOR</div><div class="line"><a name="l01333"></a><span class="lineno"> 1333</span>&#160;    &gt; zcat output/exp.lmert/1/VECFEA/1.fst.gz | fstshortestpath | fsttopsort | fstpush --to_final --push_weights | fstprint -isymbols=wmaps/wmt13.en.wmap</div><div class="line"><a name="l01334"></a><span class="lineno"> 1334</span>&#160;    Warning: cannot find parameter vector. Defaulting to flat parameters</div><div class="line"><a name="l01335"></a><span class="lineno"> 1335</span>&#160;    Warning: cannot find parameter vector. Defaulting to flat parameters</div><div class="line"><a name="l01336"></a><span class="lineno"> 1336</span>&#160;    0       1       &lt;s&gt;     1</div><div class="line"><a name="l01337"></a><span class="lineno"> 1337</span>&#160;    1       2       parliament      50</div><div class="line"><a name="l01338"></a><span class="lineno"> 1338</span>&#160;    2       3       not     20</div><div class="line"><a name="l01339"></a><span class="lineno"> 1339</span>&#160;    3       4       supports        1463</div><div class="line"><a name="l01340"></a><span class="lineno"> 1340</span>&#160;    4       5       amendment       245</div><div class="line"><a name="l01341"></a><span class="lineno"> 1341</span>&#160;    5       6       ,       4</div><div class="line"><a name="l01342"></a><span class="lineno"> 1342</span>&#160;    6       7       gives   1145</div><div class="line"><a name="l01343"></a><span class="lineno"> 1343</span>&#160;    7       8       freedom 425</div><div class="line"><a name="l01344"></a><span class="lineno"> 1344</span>&#160;    8       9       tymoshenko      23899</div><div class="line"><a name="l01345"></a><span class="lineno"> 1345</span>&#160;    9       10      &lt;/s&gt;    2</div><div class="line"><a name="l01346"></a><span class="lineno"> 1346</span>&#160;    10      0,10,1,63.460289,2,7.08984375,3,10.9941406,4,-13,5,-9,6,-6,9,-1,10,-8,11,13.1025391,12,29.7294922,</div><div class="line"><a name="l01347"></a><span class="lineno"> 1347</span>&#160;</div><div class="line"><a name="l01348"></a><span class="lineno"> 1348</span>&#160;The sparse vector weight format is</div><div class="line"><a name="l01349"></a><span class="lineno"> 1349</span>&#160;</div><div class="line"><a name="l01350"></a><span class="lineno"> 1350</span>&#160;    0,N,idx_1,fea_1,...,idx_N,fea_N</div><div class="line"><a name="l01351"></a><span class="lineno"> 1351</span>&#160;</div><div class="line"><a name="l01352"></a><span class="lineno"> 1352</span>&#160;where N is the number of non-zero elements in that weight vector.  </div><div class="line"><a name="l01353"></a><span class="lineno"> 1353</span>&#160;</div><div class="line"><a name="l01354"></a><span class="lineno"> 1354</span>&#160;To compute semiring costs correctly, the `TUPLEARC_WEIGHT_VECTOR`</div><div class="line"><a name="l01355"></a><span class="lineno"> 1355</span>&#160;environment variable should be set to contain the correct feature</div><div class="line"><a name="l01356"></a><span class="lineno"> 1356</span>&#160;vector weight; this should be the same feature vector weight applied</div><div class="line"><a name="l01357"></a><span class="lineno"> 1357</span>&#160;in translation in steps 1 and 2:</div><div class="line"><a name="l01358"></a><span class="lineno"> 1358</span>&#160;</div><div class="line"><a name="l01359"></a><span class="lineno"> 1359</span>&#160;    TUPLEARC_WEIGHT_VECTOR=[s_1 ... s_m w_1 ... w_n]</div><div class="line"><a name="l01360"></a><span class="lineno"> 1360</span>&#160;</div><div class="line"><a name="l01361"></a><span class="lineno"> 1361</span>&#160;which in this particular example is</div><div class="line"><a name="l01362"></a><span class="lineno"> 1362</span>&#160;</div><div class="line"><a name="l01363"></a><span class="lineno"> 1363</span>&#160;    &gt; export TUPLEARC_WEIGHT_VECTOR=&quot;1,0.697263,0.396540,2.270819,-0.145200,0.038503,29.518480,-3.411896,-3.732196,0.217455,0.041551,0.060136&quot;</div><div class="line"><a name="l01364"></a><span class="lineno"> 1364</span>&#160;</div><div class="line"><a name="l01365"></a><span class="lineno"> 1365</span>&#160;The shortest path found through the vector lattice is then</div><div class="line"><a name="l01366"></a><span class="lineno"> 1366</span>&#160;the same hypothesis produced under the initial parameter settings:</div><div class="line"><a name="l01367"></a><span class="lineno"> 1367</span>&#160;</div><div class="line"><a name="l01368"></a><span class="lineno"> 1368</span>&#160;    &gt; zcat output/exp.lmert/1/VECFEA/1.fst.gz | fstshortestpath | fsttopsort | fstpush --to_final --push_weights | fstprint -isymbols=wmaps/wmt13.en.wmap</div><div class="line"><a name="l01369"></a><span class="lineno"> 1369</span>&#160;    0       1       &lt;s&gt;     1</div><div class="line"><a name="l01370"></a><span class="lineno"> 1370</span>&#160;    1       2       parliament      50</div><div class="line"><a name="l01371"></a><span class="lineno"> 1371</span>&#160;    2       3       supports        1463</div><div class="line"><a name="l01372"></a><span class="lineno"> 1372</span>&#160;    3       4       amendment       245</div><div class="line"><a name="l01373"></a><span class="lineno"> 1373</span>&#160;    4       5       giving  803</div><div class="line"><a name="l01374"></a><span class="lineno"> 1374</span>&#160;    5       6       freedom 425</div><div class="line"><a name="l01375"></a><span class="lineno"> 1375</span>&#160;    6       7       tymoshenko      23899</div><div class="line"><a name="l01376"></a><span class="lineno"> 1376</span>&#160;    7       8       &lt;/s&gt;    2</div><div class="line"><a name="l01377"></a><span class="lineno"> 1377</span>&#160;    8       0,10,1,62.1510468,2,10.8671875,3,8.39355469,4,-16,5,-8,6,-5,8,-1,10,-7,11,16.3076172,12,40.5292969,</div><div class="line"><a name="l01378"></a><span class="lineno"> 1378</span>&#160;</div><div class="line"><a name="l01379"></a><span class="lineno"> 1379</span>&#160;</div><div class="line"><a name="l01380"></a><span class="lineno"> 1380</span>&#160;Note that `printstrings` can be used to extract n-best lists from the vector lattices,</div><div class="line"><a name="l01381"></a><span class="lineno"> 1381</span>&#160;if the TUPLEARC_WEIGHT_VECTOR is correctly set:</div><div class="line"><a name="l01382"></a><span class="lineno"> 1382</span>&#160;</div><div class="line"><a name="l01383"></a><span class="lineno"> 1383</span>&#160;    &gt; zcat output/exp.lmert/1/VECFEA/1.fst.gz | printstrings.${TGTBINMK}.bin --semiring=tuplearc --nbest=10 --unique -w -m wmaps/wmt13.en.wmap --tuplearc.weights=$TUPLEARC_WEIGHT_VECTOR 2&gt;/dev/null</div><div class="line"><a name="l01384"></a><span class="lineno"> 1384</span>&#160;    &lt;s&gt; parliament supports amendment giving freedom tymoshenko &lt;/s&gt;        20.7778,7.80957,17.8672,-8,-8,-5,0,0,-1,-7,20.0176,18.2979</div><div class="line"><a name="l01385"></a><span class="lineno"> 1385</span>&#160;    &lt;s&gt; parliament supports amendment gives freedom tymoshenko &lt;/s&gt;         20.7773,8.48828,20.9248,-8,-8,-4,0,-1,0,-7,14.1162,14.1016</div><div class="line"><a name="l01386"></a><span class="lineno"> 1386</span>&#160;    &lt;s&gt; parliament supports amendment giving freedom timoshenko &lt;/s&gt;        20.7778,9.70703,17.7393,-8,-8,-5,0,0,-1,-7,22.166,18.2529</div><div class="line"><a name="l01387"></a><span class="lineno"> 1387</span>&#160;    &lt;s&gt; parliament supports correction giving freedom tymoshenko &lt;/s&gt;       20.7768,9.15527,18.6689,-8,-8,-4,0,0,-1,-7,22.4062,20.7334</div><div class="line"><a name="l01388"></a><span class="lineno"> 1388</span>&#160;    &lt;s&gt; parliament supports amendment giving liberty tymoshenko &lt;/s&gt;        20.7768,10.2051,18.3838,-8,-8,-5,0,0,-1,-7,22.6582,19.4707</div><div class="line"><a name="l01389"></a><span class="lineno"> 1389</span>&#160;    &lt;s&gt; parliament supports amendment gives freedom timoshenko &lt;/s&gt;         20.7773,10.1602,21.0596,-8,-8,-4,0,-1,0,-7,16.2646,14.0566</div><div class="line"><a name="l01390"></a><span class="lineno"> 1390</span>&#160;    &lt;s&gt; parliament supports amendment enables freedom tymoshenko &lt;/s&gt;       20.7768,8.48828,20.0742,-8,-8,-4,0,-1,0,-7,50.2627,17.0137</div><div class="line"><a name="l01391"></a><span class="lineno"> 1391</span>&#160;    &lt;s&gt; parliament supports amendment enable freedom tymoshenko &lt;/s&gt;        20.7768,8.48828,20.6904,-8,-8,-4,0,-1,0,-7,50.2627,15.3457</div><div class="line"><a name="l01392"></a><span class="lineno"> 1392</span>&#160;    &lt;s&gt; parliament not supports amendment giving freedom tymoshenko &lt;/s&gt;    29.3873,5.82324,12.8096,-9,-9,-6,0,0,-1,-8,16.1914,17.9443</div><div class="line"><a name="l01393"></a><span class="lineno"> 1393</span>&#160;    &lt;s&gt; parliament supports amendment providing freedom tymoshenko &lt;/s&gt;     20.7769,8.48828,21.1689,-8,-8,-4,0,-1,0,-7,50.2627,17.3027</div><div class="line"><a name="l01394"></a><span class="lineno"> 1394</span>&#160;</div><div class="line"><a name="l01395"></a><span class="lineno"> 1395</span>&#160;These should agree with n-best lists generated directly by `alilats2splats` (see \ref mert_alilats).</div><div class="line"><a name="l01396"></a><span class="lineno"> 1396</span>&#160;</div><div class="line"><a name="l01397"></a><span class="lineno"> 1397</span>&#160;**Note** that there can be significant numerical differences between</div><div class="line"><a name="l01398"></a><span class="lineno"> 1398</span>&#160;computations under the tropical lexicographic semiring vs the tuplearc</div><div class="line"><a name="l01399"></a><span class="lineno"> 1399</span>&#160;semiring: printstrings and alilats2splats might not give exactly the same results.</div><div class="line"><a name="l01400"></a><span class="lineno"> 1400</span>&#160;In such cases, the alilats2splats result is probably the better choice.</div><div class="line"><a name="l01401"></a><span class="lineno"> 1401</span>&#160;</div><div class="line"><a name="l01402"></a><span class="lineno"> 1402</span>&#160;</div><div class="line"><a name="l01403"></a><span class="lineno"> 1403</span>&#160;</div><div class="line"><a name="l01404"></a><span class="lineno"> 1404</span>&#160;\section chopping Source Sentence Chopping</div><div class="line"><a name="l01405"></a><span class="lineno"> 1405</span>&#160;</div><div class="line"><a name="l01406"></a><span class="lineno"> 1406</span>&#160;Long source sentences make the translation process slow and expensive</div><div class="line"><a name="l01407"></a><span class="lineno"> 1407</span>&#160;in memory consumption; see [\ref Allauzen2014] for a discussion of how</div><div class="line"><a name="l01408"></a><span class="lineno"> 1408</span>&#160;source sentence length affects computational complexity and memory use</div><div class="line"><a name="l01409"></a><span class="lineno"> 1409</span>&#160;by HiFST and HiPDT.  There are various strategies for controlling</div><div class="line"><a name="l01410"></a><span class="lineno"> 1410</span>&#160;translation complexity; pruning has been discussed (\ref lpruning), and it is also</div><div class="line"><a name="l01411"></a><span class="lineno"> 1411</span>&#160;possible to set the maximum span and gap spans allowed in translation</div><div class="line"><a name="l01412"></a><span class="lineno"> 1412</span>&#160;so as to control computational complexity. However translation quality</div><div class="line"><a name="l01413"></a><span class="lineno"> 1413</span>&#160;can be affected if pruning is too heavy or if span constraints are set</div><div class="line"><a name="l01414"></a><span class="lineno"> 1414</span>&#160;too aggressively.</div><div class="line"><a name="l01415"></a><span class="lineno"> 1415</span>&#160;</div><div class="line"><a name="l01416"></a><span class="lineno"> 1416</span>&#160;An alternative approach is to &#39;chop&#39; long sentences into shorter</div><div class="line"><a name="l01417"></a><span class="lineno"> 1417</span>&#160;segements which can then be translated separately. If the sentence</div><div class="line"><a name="l01418"></a><span class="lineno"> 1418</span>&#160;chopping is done carefully, the impact on the translation quality can</div><div class="line"><a name="l01419"></a><span class="lineno"> 1419</span>&#160;be minimized.  The benefits to chopping are faster translation that</div><div class="line"><a name="l01420"></a><span class="lineno"> 1420</span>&#160;consumes less memory.  The potential drawbacks are twofold: chopping</div><div class="line"><a name="l01421"></a><span class="lineno"> 1421</span>&#160;can prevent the search procedure from finding good hypothesis under</div><div class="line"><a name="l01422"></a><span class="lineno"> 1422</span>&#160;the grammar, and care must be taken to correctly apply the target</div><div class="line"><a name="l01423"></a><span class="lineno"> 1423</span>&#160;language model at the sentence level.</div><div class="line"><a name="l01424"></a><span class="lineno"> 1424</span>&#160;</div><div class="line"><a name="l01425"></a><span class="lineno"> 1425</span>&#160;We describe two approaches to source sentence chopping:</div><div class="line"><a name="l01426"></a><span class="lineno"> 1426</span>&#160;1. **Explicit Segmentation**:   Long source sentences are chopped into smaller segments which are each translated separately.   The results are then spliced together, in various ways.</div><div class="line"><a name="l01427"></a><span class="lineno"> 1427</span>&#160;2. **Grammar-based Sentence Chopping**:  Chopping can be done by inserting the special chop symbol `0` in the source sentence, and then translating with a modified grammar. The chopping grammar is constructed so that translation rules are not applied across the chopping points, thus limiting the space of translation that are generated.</div><div class="line"><a name="l01428"></a><span class="lineno"> 1428</span>&#160;</div><div class="line"><a name="l01429"></a><span class="lineno"> 1429</span>&#160;To make the tutorial easy to follow, we will simply chop the Russian source sentences at every comma (`,`) which has index symbol `3`:</div><div class="line"><a name="l01430"></a><span class="lineno"> 1430</span>&#160;</div><div class="line"><a name="l01431"></a><span class="lineno"> 1431</span>&#160;    &gt; awk &#39;NR==4&#39; wmaps/wmt13.ru.wmap </div><div class="line"><a name="l01432"></a><span class="lineno"> 1432</span>&#160;    ,       3</div><div class="line"><a name="l01433"></a><span class="lineno"> 1433</span>&#160;</div><div class="line"><a name="l01434"></a><span class="lineno"> 1434</span>&#160;\subsection chopping_sseg Chopping by Explicit Source Sentence Segmentation</div><div class="line"><a name="l01435"></a><span class="lineno"> 1435</span>&#160;</div><div class="line"><a name="l01436"></a><span class="lineno"> 1436</span>&#160;The original Russian sentence is</div><div class="line"><a name="l01437"></a><span class="lineno"> 1437</span>&#160;chopped into shorter sentences which are to be translated</div><div class="line"><a name="l01438"></a><span class="lineno"> 1438</span>&#160;independently, as follows:</div><div class="line"><a name="l01439"></a><span class="lineno"> 1439</span>&#160;</div><div class="line"><a name="l01440"></a><span class="lineno"> 1440</span>&#160;     # sentence 328 is 51 words long</div><div class="line"><a name="l01441"></a><span class="lineno"> 1441</span>&#160;     &gt; awk &#39;NR==328&#39; RU/RU.tune.idx</div><div class="line"><a name="l01442"></a><span class="lineno"> 1442</span>&#160;     1 17914 3004 169868 123860 45 1246 53414 16617 15 6 215 26993 7704 5 142 680 13640 2481 1195794 16 7390 24705 14 3 1676 24844 33 3 24 1759 16617 5 18091 42 4340 140478 31410 5 13214 144114 6 8 12859 30201 2623 8 5 913863 4 2</div><div class="line"><a name="l01443"></a><span class="lineno"> 1443</span>&#160;</div><div class="line"><a name="l01444"></a><span class="lineno"> 1444</span>&#160;     # chop sentence 328 into separate segments; </div><div class="line"><a name="l01445"></a><span class="lineno"> 1445</span>&#160;     # split at the 3 symbol, and introduce sentence start and end symbols, </div><div class="line"><a name="l01446"></a><span class="lineno"> 1446</span>&#160;     # so that language model and translation grammar are applied correctly</div><div class="line"><a name="l01447"></a><span class="lineno"> 1447</span>&#160;     &gt; awk &#39;NR==328&#39; RU/RU.tune.idx | sed &#39;s, 3 , 3 2\n1 ,g&#39;  &gt; tmp/RU.328.segs</div><div class="line"><a name="l01448"></a><span class="lineno"> 1448</span>&#160;     &gt; cat tmp/RU.328.segs</div><div class="line"><a name="l01449"></a><span class="lineno"> 1449</span>&#160;     1 17914 3004 169868 123860 45 1246 53414 16617 15 6 215 26993 7704 5 142 680 13640 2481 1195794 16 7390 24705 14 3 2</div><div class="line"><a name="l01450"></a><span class="lineno"> 1450</span>&#160;     1 1676 24844 33 3 2</div><div class="line"><a name="l01451"></a><span class="lineno"> 1451</span>&#160;     1 24 1759 16617 5 18091 42 4340 140478 31410 5 13214 144114 6 8 12859 30201 2623 8 5 913863 4 2</div><div class="line"><a name="l01452"></a><span class="lineno"> 1452</span>&#160; </div><div class="line"><a name="l01453"></a><span class="lineno"> 1453</span>&#160;     # run HiFST over all segments</div><div class="line"><a name="l01454"></a><span class="lineno"> 1454</span>&#160;     # lattices for each segment are written to tmp/seg.328.?.fst</div><div class="line"><a name="l01455"></a><span class="lineno"> 1455</span>&#160;     &gt; hifst.${TGTBINMK}.bin --source.load=tmp/RU.328.segs --target.store=tmp/hyps.328.segs --hifst.lattice.store=tmp/seg.328.?.fst --hifst.prune=9 --hifst.replacefstbyarc.nonterminals=X,V --lm.load=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap --grammar.load=G/rules.shallow.gz</div><div class="line"><a name="l01456"></a><span class="lineno"> 1456</span>&#160;</div><div class="line"><a name="l01457"></a><span class="lineno"> 1457</span>&#160;     # concatenate lattices for each segment</div><div class="line"><a name="l01458"></a><span class="lineno"> 1458</span>&#160;     &gt; fstconcat tmp/seg.328.1.fst tmp/seg.328.2.fst | fstconcat - tmp/seg.328.3.fst &gt; tmp/seg.328.123.fst</div><div class="line"><a name="l01459"></a><span class="lineno"> 1459</span>&#160;</div><div class="line"><a name="l01460"></a><span class="lineno"> 1460</span>&#160;     # print output strings</div><div class="line"><a name="l01461"></a><span class="lineno"> 1461</span>&#160;     &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -u -n 3 --input=tmp/seg.328.123.fst -w</div><div class="line"><a name="l01462"></a><span class="lineno"> 1462</span>&#160;     ...</div><div class="line"><a name="l01463"></a><span class="lineno"> 1463</span>&#160;     &lt;s&gt; architectural historian mder toured more than 200 swimming pools , and now gathered experience in his recently published book badefreuden ( joy of swimming ) , &lt;/s&gt; &lt;s&gt; where included , &lt;/s&gt; &lt;s&gt; from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings &quot; . &lt;/s&gt;     313.88,1.68848</div><div class="line"><a name="l01464"></a><span class="lineno"> 1464</span>&#160;     &lt;s&gt; architectural historian mder toured more than 200 swimming pools , and now gathered experience in his recent book badefreuden ( joy of swimming ) , &lt;/s&gt; &lt;s&gt; where included , &lt;/s&gt; &lt;s&gt; from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings &quot; . &lt;/s&gt;     314.045,5.41211</div><div class="line"><a name="l01465"></a><span class="lineno"> 1465</span>&#160;     &lt;s&gt; architectural historian mder toured more than 200 swimming pools , and now gathered experience in his recently published book badefreuden ( joy of swimming ) , &lt;/s&gt; &lt;s&gt; where entered the &lt;/s&gt; &lt;s&gt; from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings &quot; . &lt;/s&gt;    314.115,-0.470703</div><div class="line"><a name="l01466"></a><span class="lineno"> 1466</span>&#160;     ...</div><div class="line"><a name="l01467"></a><span class="lineno"> 1467</span>&#160;</div><div class="line"><a name="l01468"></a><span class="lineno"> 1468</span>&#160;Simply concatenating the output lattices in this way leads to the substrings `&lt;/s&gt; &lt;s&gt;` </div><div class="line"><a name="l01469"></a><span class="lineno"> 1469</span>&#160;in every hypothesis in the concatenated lattice.  A transducer can be built to remove these from the output lattice.  The transducer must</div><div class="line"><a name="l01470"></a><span class="lineno"> 1470</span>&#160;   * keep the initial `1` (`&lt;s&gt;`)</div><div class="line"><a name="l01471"></a><span class="lineno"> 1471</span>&#160;   * keep the final `2` (`&lt;/s&gt;`) </div><div class="line"><a name="l01472"></a><span class="lineno"> 1472</span>&#160;   * delete every `1 2` (`&lt;/s&gt; &lt;s&gt;`) sequence </div><div class="line"><a name="l01473"></a><span class="lineno"> 1473</span>&#160;   * map every word to itself</div><div class="line"><a name="l01474"></a><span class="lineno"> 1474</span>&#160;</div><div class="line"><a name="l01475"></a><span class="lineno"> 1475</span>&#160;The following 3-state transducer will do this</div><div class="line"><a name="l01476"></a><span class="lineno"> 1476</span>&#160;</div><div class="line"><a name="l01477"></a><span class="lineno"> 1477</span>&#160;     &gt; echo -e &quot;0\t1\t1\t1&quot; &gt; tmp/strip_1_2.txt              # keep initial `1`</div><div class="line"><a name="l01478"></a><span class="lineno"> 1478</span>&#160;     &gt; echo -e &quot;1\t2\t2\t0\n2\t1\t1\t0&quot; &gt;&gt; tmp/strip_1_2.txt # map `1 2` to `0 0`</div><div class="line"><a name="l01479"></a><span class="lineno"> 1479</span>&#160;     &gt; echo -e &quot;1\t3\t2\t2&quot; &gt;&gt; tmp/strip_1_2.txt             # keep final `2`</div><div class="line"><a name="l01480"></a><span class="lineno"> 1480</span>&#160;     &gt; echo -e &quot;3&quot; &gt;&gt; tmp/strip_1_2.txt</div><div class="line"><a name="l01481"></a><span class="lineno"> 1481</span>&#160;     &gt; awk &#39;$2 != 1 &amp;&amp; $2 != 2 {printf &quot;1\t1\t%d\t%d\n&quot;, $2,$2}&#39; wmaps/wmt13.en.wmap &gt;&gt; tmp/strip_1_2.txt</div><div class="line"><a name="l01482"></a><span class="lineno"> 1482</span>&#160;     &gt; echo -e &quot;1\t1\t999999998\t999999998&quot; &gt;&gt;  tmp/strip_1_2.txt  # OOV symbol</div><div class="line"><a name="l01483"></a><span class="lineno"> 1483</span>&#160;     &gt; fstcompile  --arc_type=tropical_LT_tropical tmp/strip_1_2.txt | fstarcsort &gt; tmp/strip_1_2.fst</div><div class="line"><a name="l01484"></a><span class="lineno"> 1484</span>&#160;</div><div class="line"><a name="l01485"></a><span class="lineno"> 1485</span>&#160;     # apply the strip_1_2.fst transducer to the fst of the concatenated translation lattices</div><div class="line"><a name="l01486"></a><span class="lineno"> 1486</span>&#160;     &gt; fstcompose tmp/seg.328.123.fst tmp/strip_1_2.fst | fstproject --project_output | fstrmepsilon &gt; tmp/seg.328.no12.fst </div><div class="line"><a name="l01487"></a><span class="lineno"> 1487</span>&#160;</div><div class="line"><a name="l01488"></a><span class="lineno"> 1488</span>&#160;     # look at output</div><div class="line"><a name="l01489"></a><span class="lineno"> 1489</span>&#160;     &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w --input=tmp/seg.328.no12.fst</div><div class="line"><a name="l01490"></a><span class="lineno"> 1490</span>&#160;     &lt;s&gt; architectural historian mder toured more than 200 swimming pools , and now gathered experience in his recently published book badefreuden ( joy of swimming ) , where included , from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings &quot; . &lt;/s&gt;   313.879,1.68848</div><div class="line"><a name="l01491"></a><span class="lineno"> 1491</span>&#160;</div><div class="line"><a name="l01492"></a><span class="lineno"> 1492</span>&#160;The top hypothesis, and its score, are unchanged by removing the `&lt;/s&gt;</div><div class="line"><a name="l01493"></a><span class="lineno"> 1493</span>&#160;&lt;s&gt;` substrings.  **Note** however that the language model score for</div><div class="line"><a name="l01494"></a><span class="lineno"> 1494</span>&#160;this hypothesis are not correct, since the language model histories</div><div class="line"><a name="l01495"></a><span class="lineno"> 1495</span>&#160;are not applied correctly at the segment boundaries.</div><div class="line"><a name="l01496"></a><span class="lineno"> 1496</span>&#160;</div><div class="line"><a name="l01497"></a><span class="lineno"> 1497</span>&#160;To fix this, the applylm tool (see \ref rescoring_lm) can</div><div class="line"><a name="l01498"></a><span class="lineno"> 1498</span>&#160;be used to remove and reapply the language model so that it spans the</div><div class="line"><a name="l01499"></a><span class="lineno"> 1499</span>&#160;source segment translations.  The following example simply removes the</div><div class="line"><a name="l01500"></a><span class="lineno"> 1500</span>&#160;language model scores from</div><div class="line"><a name="l01501"></a><span class="lineno"> 1501</span>&#160;output/exp.chopping.explicit/LATS/sent_no12.fst</div><div class="line"><a name="l01502"></a><span class="lineno"> 1502</span>&#160;and then reapplies them</div><div class="line"><a name="l01503"></a><span class="lineno"> 1503</span>&#160;via composition</div><div class="line"><a name="l01504"></a><span class="lineno"> 1504</span>&#160;</div><div class="line"><a name="l01505"></a><span class="lineno"> 1505</span>&#160;     &gt; applylm.${TGTBINMK}.bin --lm.load=M/interp.4g.arpa.newstest2012.tune.corenlp.ru.idx.withoptions.mmap --lm.featureweights=1 --lm.wps=0.0 --semiring=lexstdarc --lattice.load=tmp/seg.328.no12.fst --lattice.store=tmp/seg.328.no12.relm.fst --lattice.load.deletelmcost </div><div class="line"><a name="l01506"></a><span class="lineno"> 1506</span>&#160;</div><div class="line"><a name="l01507"></a><span class="lineno"> 1507</span>&#160;The rescored output is written to </div><div class="line"><a name="l01508"></a><span class="lineno"> 1508</span>&#160;`tmp/seg.328.no12.relm.fst`</div><div class="line"><a name="l01509"></a><span class="lineno"> 1509</span>&#160;with correctly applied language model scores, i.e. the LM history is no longer broken by any `&lt;/s&gt; &lt;s&gt;` in the lattice.  The total translation cost is much lower (better) than when segment hypotheses are simply combined (i.e. 291.958 vs. 313.879):</div><div class="line"><a name="l01510"></a><span class="lineno"> 1510</span>&#160;</div><div class="line"><a name="l01511"></a><span class="lineno"> 1511</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --input=tmp/seg.328.no12.relm.fst --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w </div><div class="line"><a name="l01512"></a><span class="lineno"> 1512</span>&#160;    &lt;s&gt; architectural historian mder toured more than 200 swimming pools , and now gathered experience in his recently published book badefreuden ( joy of swimming ) , which included everything from a public pool in munich and the historic bathing palaces in the black forest and functional concrete buildings &quot; . &lt;/s&gt;  291.958,-0.329102</div><div class="line"><a name="l01513"></a><span class="lineno"> 1513</span>&#160;</div><div class="line"><a name="l01514"></a><span class="lineno"> 1514</span>&#160;</div><div class="line"><a name="l01515"></a><span class="lineno"> 1515</span>&#160;\subsection chopping_gb Grammar-based Sentence Chopping</div><div class="line"><a name="l01516"></a><span class="lineno"> 1516</span>&#160;</div><div class="line"><a name="l01517"></a><span class="lineno"> 1517</span>&#160;Chopping can also be done by inserting a special &#39;chop&#39; symbol</div><div class="line"><a name="l01518"></a><span class="lineno"> 1518</span>&#160;`0` in the source sentence, and then translating with a modified</div><div class="line"><a name="l01519"></a><span class="lineno"> 1519</span>&#160;grammar.  The chopping grammar is constructed so that translation</div><div class="line"><a name="l01520"></a><span class="lineno"> 1520</span>&#160;rules are not applied across the chopping points, thus limiting the</div><div class="line"><a name="l01521"></a><span class="lineno"> 1521</span>&#160;space of translation that are generated.  Conceptually, translation proceeds as:</div><div class="line"><a name="l01522"></a><span class="lineno"> 1522</span>&#160;   -# the translation grammar is applied separately to source sentence segments demarcated by chop symbols</div><div class="line"><a name="l01523"></a><span class="lineno"> 1523</span>&#160;   -# local pruning can be applied to the translations of these segments</div><div class="line"><a name="l01524"></a><span class="lineno"> 1524</span>&#160;   -# the resulting WFSAs containing translations of the segments are concatenated under the chopping grammar, possibly with local pruning</div><div class="line"><a name="l01525"></a><span class="lineno"> 1525</span>&#160;   -# the language model is applied to the concatenated WFSA</div><div class="line"><a name="l01526"></a><span class="lineno"> 1526</span>&#160;   -# top-level, admissible pruning is done under the combined grammar and language model scores</div><div class="line"><a name="l01527"></a><span class="lineno"> 1527</span>&#160;</div><div class="line"><a name="l01528"></a><span class="lineno"> 1528</span>&#160;In this way the FSTs produced by translating the segments are</div><div class="line"><a name="l01529"></a><span class="lineno"> 1529</span>&#160;concatenated prior to application of the target language model, </div><div class="line"><a name="l01530"></a><span class="lineno"> 1530</span>&#160;and the language model context is not broken by the source</div><div class="line"><a name="l01531"></a><span class="lineno"> 1531</span>&#160;sentence chopping.   As an example, a grammar modified for chopping contains the following rules (without</div><div class="line"><a name="l01532"></a><span class="lineno"> 1532</span>&#160;weights):</div><div class="line"><a name="l01533"></a><span class="lineno"> 1533</span>&#160;</div><div class="line"><a name="l01534"></a><span class="lineno"> 1534</span>&#160;     R 1 1</div><div class="line"><a name="l01535"></a><span class="lineno"> 1535</span>&#160;     R R_D_X R_D_X</div><div class="line"><a name="l01536"></a><span class="lineno"> 1536</span>&#160;     R R_X R_X</div><div class="line"><a name="l01537"></a><span class="lineno"> 1537</span>&#160;</div><div class="line"><a name="l01538"></a><span class="lineno"> 1538</span>&#160;     T 0 0</div><div class="line"><a name="l01539"></a><span class="lineno"> 1539</span>&#160;     T T_D_X T_D_X</div><div class="line"><a name="l01540"></a><span class="lineno"> 1540</span>&#160;     T T_X T_X</div><div class="line"><a name="l01541"></a><span class="lineno"> 1541</span>&#160;</div><div class="line"><a name="l01542"></a><span class="lineno"> 1542</span>&#160;     S S_U S_U</div><div class="line"><a name="l01543"></a><span class="lineno"> 1543</span>&#160;     S Q Q</div><div class="line"><a name="l01544"></a><span class="lineno"> 1544</span>&#160;     Q R R</div><div class="line"><a name="l01545"></a><span class="lineno"> 1545</span>&#160;     U T T</div><div class="line"><a name="l01546"></a><span class="lineno"> 1546</span>&#160;</div><div class="line"><a name="l01547"></a><span class="lineno"> 1547</span>&#160;The rules in the first block above are similar to those used in the</div><div class="line"><a name="l01548"></a><span class="lineno"> 1548</span>&#160;usual Hiero grammar, with the original &#39;`S`&#39; changed to &#39;`R`&#39;. These rules</div><div class="line"><a name="l01549"></a><span class="lineno"> 1549</span>&#160;are responsible for concatenating the partial translations of the source</div><div class="line"><a name="l01550"></a><span class="lineno"> 1550</span>&#160;sentence, starting from the sentence-start symbol</div><div class="line"><a name="l01551"></a><span class="lineno"> 1551</span>&#160;&#39;1&#39;, up to but not including the first instance of the chopping symbol</div><div class="line"><a name="l01552"></a><span class="lineno"> 1552</span>&#160;&#39;0&#39;.</div><div class="line"><a name="l01553"></a><span class="lineno"> 1553</span>&#160;</div><div class="line"><a name="l01554"></a><span class="lineno"> 1554</span>&#160;Each subsequent sequence of source words starting with symbol &#39;0&#39;, is</div><div class="line"><a name="l01555"></a><span class="lineno"> 1555</span>&#160;handled in a similar way by the second block of rules above. Note that</div><div class="line"><a name="l01556"></a><span class="lineno"> 1556</span>&#160;the only rule that can be applied to the input symbol &#39;0&#39; is &#39;`T 0 0`&#39;, making the</div><div class="line"><a name="l01557"></a><span class="lineno"> 1557</span>&#160;translation of each chopped segment independent.  This makes use of</div><div class="line"><a name="l01558"></a><span class="lineno"> 1558</span>&#160;the OpenFST convention of mapping 0 to epsilon: the 0&#39;s in the input</div><div class="line"><a name="l01559"></a><span class="lineno"> 1559</span>&#160;are parsed as regular symbols by HiFST, while 0&#39;s on the output side</div><div class="line"><a name="l01560"></a><span class="lineno"> 1560</span>&#160;are mapped to epsilons and ignored in composition with the language</div><div class="line"><a name="l01561"></a><span class="lineno"> 1561</span>&#160;model.</div><div class="line"><a name="l01562"></a><span class="lineno"> 1562</span>&#160;</div><div class="line"><a name="l01563"></a><span class="lineno"> 1563</span>&#160;The third block of rules above will join together the results obtained</div><div class="line"><a name="l01564"></a><span class="lineno"> 1564</span>&#160;for each chopped segment. As with the glue rule &#39;`S`&#39; in the usual Hiero</div><div class="line"><a name="l01565"></a><span class="lineno"> 1565</span>&#160;grammar, it is necessary to allow this new set of rules to be applied</div><div class="line"><a name="l01566"></a><span class="lineno"> 1566</span>&#160;to any span. This is done by setting</div><div class="line"><a name="l01567"></a><span class="lineno"> 1567</span>&#160;</div><div class="line"><a name="l01568"></a><span class="lineno"> 1568</span>&#160;      cykparser.ntexceptionsmaxspan=S,Q,R,T,U</div><div class="line"><a name="l01569"></a><span class="lineno"> 1569</span>&#160;</div><div class="line"><a name="l01570"></a><span class="lineno"> 1570</span>&#160;The additional mapping provided by the last two rules controls</div><div class="line"><a name="l01571"></a><span class="lineno"> 1571</span>&#160;the pruning applied to the top CYK cell relative to each</div><div class="line"><a name="l01572"></a><span class="lineno"> 1572</span>&#160;chopped segment:</div><div class="line"><a name="l01573"></a><span class="lineno"> 1573</span>&#160;</div><div class="line"><a name="l01574"></a><span class="lineno"> 1574</span>&#160;      hifst.localprune.conditions=Q,1,100,12,U,1,100,12,X,5,10000,9,V,3,20000,9</div><div class="line"><a name="l01575"></a><span class="lineno"> 1575</span>&#160;</div><div class="line"><a name="l01576"></a><span class="lineno"> 1576</span>&#160;In the above example tighter parameters are chosen for &#39;`Q`&#39; and &#39;`U`&#39; to</div><div class="line"><a name="l01577"></a><span class="lineno"> 1577</span>&#160;force pruning.  In this way the final lattice obtained by</div><div class="line"><a name="l01578"></a><span class="lineno"> 1578</span>&#160;concatenation is prevented from growing too large.  However a wider</div><div class="line"><a name="l01579"></a><span class="lineno"> 1579</span>&#160;beam (12) with respect to the other cell types is used, to avoid</div><div class="line"><a name="l01580"></a><span class="lineno"> 1580</span>&#160;discarding too many potentially useful hypotheses.</div><div class="line"><a name="l01581"></a><span class="lineno"> 1581</span>&#160;</div><div class="line"><a name="l01582"></a><span class="lineno"> 1582</span>&#160;It is possible to specify explicitly that FSTs generated for rules</div><div class="line"><a name="l01583"></a><span class="lineno"> 1583</span>&#160;with LHS &#39;`X`&#39; or &#39;`V`&#39; can be kept as pointers rather then expanded in</div><div class="line"><a name="l01584"></a><span class="lineno"> 1584</span>&#160;the FST (RTN) that is built for a higher CYK cell. This is achieved</div><div class="line"><a name="l01585"></a><span class="lineno"> 1585</span>&#160;setting</div><div class="line"><a name="l01586"></a><span class="lineno"> 1586</span>&#160;</div><div class="line"><a name="l01587"></a><span class="lineno"> 1587</span>&#160;      hifst.replacefstbyarc=X,V</div><div class="line"><a name="l01588"></a><span class="lineno"> 1588</span>&#160;      hifst.replacefstbyarc.exceptions=S,R,T</div><div class="line"><a name="l01589"></a><span class="lineno"> 1589</span>&#160;</div><div class="line"><a name="l01590"></a><span class="lineno"> 1590</span>&#160;The second line above prevents substitution for rules with LHS</div><div class="line"><a name="l01591"></a><span class="lineno"> 1591</span>&#160;&#39;`S`&#39;, &#39;`R`&#39; and &#39;`T`&#39;.  It is better to have a fully expanded FST for these</div><div class="line"><a name="l01592"></a><span class="lineno"> 1592</span>&#160;rules for more effective optimisation (Determinization and Minimisation).</div><div class="line"><a name="l01593"></a><span class="lineno"> 1593</span>&#160;</div><div class="line"><a name="l01594"></a><span class="lineno"> 1594</span>&#160;\subsubsection chopping_eg Converting Grammars and Input Text for Chopping</div><div class="line"><a name="l01595"></a><span class="lineno"> 1595</span>&#160;</div><div class="line"><a name="l01596"></a><span class="lineno"> 1596</span>&#160;The usual Hiero grammar can be converted for chopping, as follows; note that no-cost, 0 valued, weights are added to rules :</div><div class="line"><a name="l01597"></a><span class="lineno"> 1597</span>&#160;</div><div class="line"><a name="l01598"></a><span class="lineno"> 1598</span>&#160;First, create the chopping and glue rules:</div><div class="line"><a name="l01599"></a><span class="lineno"> 1599</span>&#160;</div><div class="line"><a name="l01600"></a><span class="lineno"> 1600</span>&#160;     &gt; (echo &quot;T T_D_X T_D_X 0&quot; ; echo &quot;T T_X T_X 0&quot; ; echo &quot;T 0 0 0&quot;) &gt; tmp/rules.shallow.chop</div><div class="line"><a name="l01601"></a><span class="lineno"> 1601</span>&#160;     &gt; (echo &quot;S S_U S_U 0&quot; ; echo &quot;U T T 0&quot; ; echo &quot;Q R R 0&quot; ; echo &quot;S Q Q 0&quot;) &gt;&gt; tmp/rules.shallow.chop</div><div class="line"><a name="l01602"></a><span class="lineno"> 1602</span>&#160;     &gt; cat tmp/rules.shallow.chop</div><div class="line"><a name="l01603"></a><span class="lineno"> 1603</span>&#160;     T T_D_X T_D_X 0</div><div class="line"><a name="l01604"></a><span class="lineno"> 1604</span>&#160;     T T_X T_X 0</div><div class="line"><a name="l01605"></a><span class="lineno"> 1605</span>&#160;     T 0 0 0</div><div class="line"><a name="l01606"></a><span class="lineno"> 1606</span>&#160;     S S_U S_U 0</div><div class="line"><a name="l01607"></a><span class="lineno"> 1607</span>&#160;     U T T 0</div><div class="line"><a name="l01608"></a><span class="lineno"> 1608</span>&#160;     Q R R 0</div><div class="line"><a name="l01609"></a><span class="lineno"> 1609</span>&#160;     S Q Q 0</div><div class="line"><a name="l01610"></a><span class="lineno"> 1610</span>&#160;</div><div class="line"><a name="l01611"></a><span class="lineno"> 1611</span>&#160;Next, append all rules, mapping glue rules with LHS S to LHS R:</div><div class="line"><a name="l01612"></a><span class="lineno"> 1612</span>&#160;</div><div class="line"><a name="l01613"></a><span class="lineno"> 1613</span>&#160;     &gt; zcat G/rules.shallow.gz | sed &#39;s,S,R,g&#39; &gt;&gt; tmp/rules.shallow.chop</div><div class="line"><a name="l01614"></a><span class="lineno"> 1614</span>&#160;     &gt; gzip tmp/rules.shallow.chop</div><div class="line"><a name="l01615"></a><span class="lineno"> 1615</span>&#160;</div><div class="line"><a name="l01616"></a><span class="lineno"> 1616</span>&#160;The source text (`RU/RU.set1.chop.idx`) will be chopped simply inserting</div><div class="line"><a name="l01617"></a><span class="lineno"> 1617</span>&#160;the chopping marker &#39;0&#39; after each comma (integer mapped to 3 in the</div><div class="line"><a name="l01618"></a><span class="lineno"> 1618</span>&#160;Russian wordmap); this</div><div class="line"><a name="l01619"></a><span class="lineno"> 1619</span>&#160;is a simplistic approach that is easily implemented for this</div><div class="line"><a name="l01620"></a><span class="lineno"> 1620</span>&#160;demonstration.   We will select sentences with more than 80 words from the source language set:</div><div class="line"><a name="l01621"></a><span class="lineno"> 1621</span>&#160;</div><div class="line"><a name="l01622"></a><span class="lineno"> 1622</span>&#160;     # there are 3 source sentences longer than 80 words</div><div class="line"><a name="l01623"></a><span class="lineno"> 1623</span>&#160;     &gt; awk &#39;NF &gt; 80 {print $0}&#39; RU/RU.tune.idx &gt; tmp/RU.long.idx</div><div class="line"><a name="l01624"></a><span class="lineno"> 1624</span>&#160;</div><div class="line"><a name="l01625"></a><span class="lineno"> 1625</span>&#160;     # make a version with the chopping symbol 0 after every comma</div><div class="line"><a name="l01626"></a><span class="lineno"> 1626</span>&#160;     &gt; sed &#39;s, 3 , 3 0 ,g&#39; tmp/RU.long.idx &gt; tmp/RU.long.csym.idx</div><div class="line"><a name="l01627"></a><span class="lineno"> 1627</span>&#160;</div><div class="line"><a name="l01628"></a><span class="lineno"> 1628</span>&#160;     # print the first line; the &lt;epsilon&gt; indicates where the chopping symbol has been inserted</div><div class="line"><a name="l01629"></a><span class="lineno"> 1629</span>&#160;     &gt; farcompilestrings  --entry_type=line tmp/RU.long.csym.idx | farprintstrings --symbols=wmaps/wmt13.ru.wmap | head -1</div><div class="line"><a name="l01630"></a><span class="lineno"> 1630</span>&#160;     &lt;s&gt;     , &lt;epsilon&gt;   , &lt;epsilon&gt;      , &lt;epsilon&gt;        , &lt;epsilon&gt;            , &lt;epsilon&gt;     55.053     , &lt;epsilon&gt;   4.53     , &lt;epsilon&gt;       5.28  , &lt;epsilon&gt;          6.19  , &lt;epsilon&gt;  , &lt;epsilon&gt;          74.000    , &lt;epsilon&gt;   18.969        . &lt;/s&gt;</div><div class="line"><a name="l01631"></a><span class="lineno"> 1631</span>&#160;</div><div class="line"><a name="l01632"></a><span class="lineno"> 1632</span>&#160;     # Run HiFST, without chopping.  Input is the original source: tmp/RU.long.idx</div><div class="line"><a name="l01633"></a><span class="lineno"> 1633</span>&#160;     # hypotheses are written to output/exp.chopping/nochop/hyps and lattices to output/exp.chopping/nochop/LATS/</div><div class="line"><a name="l01634"></a><span class="lineno"> 1634</span>&#160;     # configuration is otherwise </div><div class="line"><a name="l01635"></a><span class="lineno"> 1635</span>&#160;     &gt; (time hifst.$TGTBINMK.bin --source.load=tmp/RU.long.idx --target.store=tmp/hyps.long.nochop --hifst.lattice.store=tmp/long.nochop.?.fst.gz --config=configs/CF.baseline ) &amp;&gt; log/log.long.nochop</div><div class="line"><a name="l01636"></a><span class="lineno"> 1636</span>&#160;</div><div class="line"><a name="l01637"></a><span class="lineno"> 1637</span>&#160;The decoder requires the following settings to use the chopping grammar:</div><div class="line"><a name="l01638"></a><span class="lineno"> 1638</span>&#160;</div><div class="line"><a name="l01639"></a><span class="lineno"> 1639</span>&#160;   * hifst.replacefstbyarc.exceptions=S,R,T # Specifies glue rule types in the grammar</div><div class="line"><a name="l01640"></a><span class="lineno"> 1640</span>&#160;   * cykparser.ntexceptionsmaxspan=S,Q,R,T,U # List of non-terminals not affected by cykparser.hrmaxheight. S should be here</div><div class="line"><a name="l01641"></a><span class="lineno"> 1641</span>&#160;</div><div class="line"><a name="l01642"></a><span class="lineno"> 1642</span>&#160;Now run HiFST, with chopping.  </div><div class="line"><a name="l01643"></a><span class="lineno"> 1643</span>&#160;</div><div class="line"><a name="l01644"></a><span class="lineno"> 1644</span>&#160;     # Input is the source with chopping symbols: tmp/RU.long.csym.idx</div><div class="line"><a name="l01645"></a><span class="lineno"> 1645</span>&#160;     # hypotheses are written to tmp/hyps.long.gchop and lattices to tmp/long.gchop.?.fst.gz</div><div class="line"><a name="l01646"></a><span class="lineno"> 1646</span>&#160;     &gt; (time hifst.$TGTBINMK.bin --source.load=tmp/RU.long.csym.idx --target.store=tmp/hyps.long.gchop --hifst.lattice.store=tmp/long.gchop.?.fst.gz --config=configs/CF.baseline --grammar.load=tmp/rules.shallow.chop.gz --cykparser.ntexceptionsmaxspan=S,Q,R,T,U --hifst.replacefstbyarc.exceptions=S,R,T) &amp;&gt; log/log.long.gchop</div><div class="line"><a name="l01647"></a><span class="lineno"> 1647</span>&#160;</div><div class="line"><a name="l01648"></a><span class="lineno"> 1648</span>&#160;Comparing the two experiments indicates that chopping requires less memory and runs faster (and note that the LM alone requires ~1GB):</div><div class="line"><a name="l01649"></a><span class="lineno"> 1649</span>&#160;                        </div><div class="line"><a name="l01650"></a><span class="lineno"> 1650</span>&#160;     Input/Grammar  Tot time      Max memory</div><div class="line"><a name="l01651"></a><span class="lineno"> 1651</span>&#160;     --------       --------      ----------</div><div class="line"><a name="l01652"></a><span class="lineno"> 1652</span>&#160;     Unchopped       1m 21s         2.6Gb</div><div class="line"><a name="l01653"></a><span class="lineno"> 1653</span>&#160;     Chopped               39s         1.6Gb  </div><div class="line"><a name="l01654"></a><span class="lineno"> 1654</span>&#160;     </div><div class="line"><a name="l01655"></a><span class="lineno"> 1655</span>&#160;</div><div class="line"><a name="l01656"></a><span class="lineno"> 1656</span>&#160;</div><div class="line"><a name="l01657"></a><span class="lineno"> 1657</span>&#160;</div><div class="line"><a name="l01658"></a><span class="lineno"> 1658</span>&#160;However, chopping restricts the space of translations.  Looking at the</div><div class="line"><a name="l01659"></a><span class="lineno"> 1659</span>&#160;scores of the best translation hypotheses, chopping the source</div><div class="line"><a name="l01660"></a><span class="lineno"> 1660</span>&#160;sentence prevents the decoder from finding the best scoring hypothesis</div><div class="line"><a name="l01661"></a><span class="lineno"> 1661</span>&#160;under the grammar; for the third sentence, the hypothesis produced</div><div class="line"><a name="l01662"></a><span class="lineno"> 1662</span>&#160;without chopping has a lower (i.e. better) combined cost (471.123)</div><div class="line"><a name="l01663"></a><span class="lineno"> 1663</span>&#160;than the hypothesis produced with chopping (474.753):</div><div class="line"><a name="l01664"></a><span class="lineno"> 1664</span>&#160;</div><div class="line"><a name="l01665"></a><span class="lineno"> 1665</span>&#160;     # best hypothesis for the 1st sentence, without chopping</div><div class="line"><a name="l01666"></a><span class="lineno"> 1666</span>&#160;     &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --input=tmp/long.nochop.1.fst.gz -w -m wmaps/wmt13.en.wmap</div><div class="line"><a name="l01667"></a><span class="lineno"> 1667</span>&#160;     &lt;s&gt; the decline in economic activity caused , until november , the permanent rise in unemployment in the state , according to the national institute of statistics and geography , in the first three quarters was recorded accelerated growth in unemployment , from january to march 55.053 resident sinaloa were unemployed , with the share of per cent of the economically active population , in the second quarter , the share rose to percent from july to september , she continued to rise to percent share , if calculated per number of people is more than unemployed people in sinaloa , the 18.969 people more than in the first half . &lt;/s&gt;     471.123,2.46484</div><div class="line"><a name="l01668"></a><span class="lineno"> 1668</span>&#160;</div><div class="line"><a name="l01669"></a><span class="lineno"> 1669</span>&#160;     # best hypothesis for the 1st sentence, with chopping</div><div class="line"><a name="l01670"></a><span class="lineno"> 1670</span>&#160;     &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --input=tmp/long.gchop.1.fst.gz -w -m wmaps/wmt13.en.wmap</div><div class="line"><a name="l01671"></a><span class="lineno"> 1671</span>&#160;     &lt;s&gt; the decline in economic activity caused , until november , a permanent rise in unemployment in the state , according to the national institute of statistics and geography , in the first three quarters was recorded accelerated growth in unemployment , from january to march 55.053 resident sinaloa were unemployed , with the share of per cent of the economically active population , in the second quarter , the share rose to percent from july to september , she continued to rise to percent stake , which is recalculated the number of people is more than unemployed people in sinaloa , the 18.969 people more than in the first half . &lt;/s&gt;       474.753,4.52344</div><div class="line"><a name="l01672"></a><span class="lineno"> 1672</span>&#160;</div><div class="line"><a name="l01673"></a><span class="lineno"> 1673</span>&#160;</div><div class="line"><a name="l01674"></a><span class="lineno"> 1674</span>&#160;</div><div class="line"><a name="l01675"></a><span class="lineno"> 1675</span>&#160;\section true_casing FST-based True Casing</div><div class="line"><a name="l01676"></a><span class="lineno"> 1676</span>&#160;</div><div class="line"><a name="l01677"></a><span class="lineno"> 1677</span>&#160;HiFST includes a tool typically used for  true casing the output. It relies on two models:</div><div class="line"><a name="l01678"></a><span class="lineno"> 1678</span>&#160;</div><div class="line"><a name="l01679"></a><span class="lineno"> 1679</span>&#160;- A true-case integer-mapped language model in ARPA or KenLM format.</div><div class="line"><a name="l01680"></a><span class="lineno"> 1680</span>&#160;- A flower transducer that transduces uncased words to every true case alternative.</div><div class="line"><a name="l01681"></a><span class="lineno"> 1681</span>&#160;  This model is loaded from a file with the following format per line, one for each uncased word:</div><div class="line"><a name="l01682"></a><span class="lineno"> 1682</span>&#160;     - uncased-case-word true-case-word1 prob1 true-case-word2 prob2 ...</div><div class="line"><a name="l01683"></a><span class="lineno"> 1683</span>&#160;     - This format is compatible with the unigram model for \ref SRILM [disambig](http://www.speech.sri.com/projects/srilm/manpages/disambig.1.html) tool (see `--map` option).</div><div class="line"><a name="l01684"></a><span class="lineno"> 1684</span>&#160;</div><div class="line"><a name="l01685"></a><span class="lineno"> 1685</span>&#160;Words must be integer-mapped. A file with this model is available:</div><div class="line"><a name="l01686"></a><span class="lineno"> 1686</span>&#160;</div><div class="line"><a name="l01687"></a><span class="lineno"> 1687</span>&#160;    &gt; head  G/tc.unimap</div><div class="line"><a name="l01688"></a><span class="lineno"> 1688</span>&#160;    1 1 1.0</div><div class="line"><a name="l01689"></a><span class="lineno"> 1689</span>&#160;    2 2 1.0</div><div class="line"><a name="l01690"></a><span class="lineno"> 1690</span>&#160;    3 5943350 0.00002 3 0.86370 5943349 0.13628</div><div class="line"><a name="l01691"></a><span class="lineno"> 1691</span>&#160;    4 4 1.00000</div><div class="line"><a name="l01692"></a><span class="lineno"> 1692</span>&#160;    5 5 1.00000</div><div class="line"><a name="l01693"></a><span class="lineno"> 1693</span>&#160;    6 5942623 0.00452 5942624 0.00002 6 0.99546</div><div class="line"><a name="l01694"></a><span class="lineno"> 1694</span>&#160;    7 5943397 0.00000 5943398 0.01875 7 0.98121 5943399 0.00004</div><div class="line"><a name="l01695"></a><span class="lineno"> 1695</span>&#160;    8 5941239 0.00003 8 0.99494 5941238 0.00502</div><div class="line"><a name="l01696"></a><span class="lineno"> 1696</span>&#160;    9 5942238 0.06269 9 0.93729 5942239 0.00002</div><div class="line"><a name="l01697"></a><span class="lineno"> 1697</span>&#160;    10 5943348 0.00001 10 0.99498 5943347 0.00501</div><div class="line"><a name="l01698"></a><span class="lineno"> 1698</span>&#160;</div><div class="line"><a name="l01699"></a><span class="lineno"> 1699</span>&#160;For example, under this model word 4 (comma &quot;,&quot;) transduces to itself with probability 1.</div><div class="line"><a name="l01700"></a><span class="lineno"> 1700</span>&#160;The uncased word 3 (&quot;the&quot;) has three upper-case alternatives: &quot;the&quot;, &quot;THE&quot;, and &quot;The&quot;, with the following probabilities</div><div class="line"><a name="l01701"></a><span class="lineno"> 1701</span>&#160;</div><div class="line"><a name="l01702"></a><span class="lineno"> 1702</span>&#160;     P(the | the) = 0.86</div><div class="line"><a name="l01703"></a><span class="lineno"> 1703</span>&#160;     P(THE | the) = 0.00002</div><div class="line"><a name="l01704"></a><span class="lineno"> 1704</span>&#160;     P(The | the) = 0.13628</div><div class="line"><a name="l01705"></a><span class="lineno"> 1705</span>&#160;</div><div class="line"><a name="l01706"></a><span class="lineno"> 1706</span>&#160;To generate these probabilities, you just need counts of truecased words. You can extract these unigrams</div><div class="line"><a name="l01707"></a><span class="lineno"> 1707</span>&#160;with \ref SRILM [ngram-count] (http://www.speech.sri.com/projects/srilm/manpages/ngram-count.1.html) tool,</div><div class="line"><a name="l01708"></a><span class="lineno"> 1708</span>&#160;and calculate the probability of each particular true-cased form given the aggregated number of lower-cased instances.</div><div class="line"><a name="l01709"></a><span class="lineno"> 1709</span>&#160;</div><div class="line"><a name="l01710"></a><span class="lineno"> 1710</span>&#160;</div><div class="line"><a name="l01711"></a><span class="lineno"> 1711</span>&#160;These models are provided to the recaser module via the following configuration options</div><div class="line"><a name="l01712"></a><span class="lineno"> 1712</span>&#160;</div><div class="line"><a name="l01713"></a><span class="lineno"> 1713</span>&#160;    &gt; cat configs/CF.recaser</div><div class="line"><a name="l01714"></a><span class="lineno"> 1714</span>&#160;    [recaser]</div><div class="line"><a name="l01715"></a><span class="lineno"> 1715</span>&#160;    lm.load=M/lm.tc.gz</div><div class="line"><a name="l01716"></a><span class="lineno"> 1716</span>&#160;    unimap.load=G/tc.unimap</div><div class="line"><a name="l01717"></a><span class="lineno"> 1717</span>&#160;</div><div class="line"><a name="l01718"></a><span class="lineno"> 1718</span>&#160;The true casing procedure is very similar to that of \ref SRILM [disambig](http://www.speech.sri.com/projects/srilm/manpages/disambig.1.html) tool.</div><div class="line"><a name="l01719"></a><span class="lineno"> 1719</span>&#160;In our case this is accomplished with two subsequent compositions, followed by exact pruning.</div><div class="line"><a name="l01720"></a><span class="lineno"> 1720</span>&#160;An acceptable performance vs speed/memory trade-off can be achieved e.g. with offline entropy pruning of the language model.</div><div class="line"><a name="l01721"></a><span class="lineno"> 1721</span>&#160;</div><div class="line"><a name="l01722"></a><span class="lineno"> 1722</span>&#160;A range of input lattices can be true-cased in the following way with our fst-based disambig tool:</div><div class="line"><a name="l01723"></a><span class="lineno"> 1723</span>&#160;</div><div class="line"><a name="l01724"></a><span class="lineno"> 1724</span>&#160;    # re-run the baseline </div><div class="line"><a name="l01725"></a><span class="lineno"> 1725</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline</div><div class="line"><a name="l01726"></a><span class="lineno"> 1726</span>&#160;    # recase the output lattices</div><div class="line"><a name="l01727"></a><span class="lineno"> 1727</span>&#160;    &gt; disambig.${TGTBINMK}.bin configs/CF.recaser --recaser.input=output/exp.baseline/LATS/?.fst.gz --recaser.output=output/exp.baseline/LATS/?.fst.recase.gz --range=1:2 -s lexstdarc</div><div class="line"><a name="l01728"></a><span class="lineno"> 1728</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --input=output/exp.baseline/LATS/?.fst.recase.gz --semiring=lexstdarc --label-map=wmaps/wmt13.en.wmap --range=1:2</div><div class="line"><a name="l01729"></a><span class="lineno"> 1729</span>&#160;    &lt;s&gt; Republican strategy of resistance to the renewal of obamas election &lt;/s&gt; </div><div class="line"><a name="l01730"></a><span class="lineno"> 1730</span>&#160;    &lt;s&gt; The leaders of the Republican justified their policies need to deal with the spin on the elections . &lt;/s&gt; </div><div class="line"><a name="l01731"></a><span class="lineno"> 1731</span>&#160;</div><div class="line"><a name="l01732"></a><span class="lineno"> 1732</span>&#160;Note that both models need to be integer-mapped, hence the external target wordmap (--label-map) must also map true case words.</div><div class="line"><a name="l01733"></a><span class="lineno"> 1733</span>&#160;</div><div class="line"><a name="l01734"></a><span class="lineno"> 1734</span>&#160;HiFST can include truecasing as subsequent step following decoding, prior to writing the output hypotheses. For instance:</div><div class="line"><a name="l01735"></a><span class="lineno"> 1735</span>&#160;    </div><div class="line"><a name="l01736"></a><span class="lineno"> 1736</span>&#160;    &gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline --recaser.lm.load=M/lm.tc.gz --recaser.unimap.load=G/tc.unimap</div><div class="line"><a name="l01737"></a><span class="lineno"> 1737</span>&#160;</div><div class="line"><a name="l01738"></a><span class="lineno"> 1738</span>&#160;    &gt; farcompilestrings --entry_type=line output/exp.baseline/hyps | farprintstrings --symbols=wmaps/wmt13.en.wmap </div><div class="line"><a name="l01739"></a><span class="lineno"> 1739</span>&#160;    &lt;s&gt; Republican strategy of resistance to the renewal of obamas election &lt;/s&gt;</div><div class="line"><a name="l01740"></a><span class="lineno"> 1740</span>&#160;    &lt;s&gt; The leaders of the Republican justified their policies need to deal with the spin on the elections . &lt;/s&gt;</div><div class="line"><a name="l01741"></a><span class="lineno"> 1741</span>&#160;</div><div class="line"><a name="l01742"></a><span class="lineno"> 1742</span>&#160;However, the output lattices are left in uncased form:</div><div class="line"><a name="l01743"></a><span class="lineno"> 1743</span>&#160;</div><div class="line"><a name="l01744"></a><span class="lineno"> 1744</span>&#160;    &gt; printstrings.${TGTBINMK}.bin --semiring=lexstdarc --label-map=wmaps/wmt13.en.wmap --input=output/exp.baseline/LATS/1.fst.gz </div><div class="line"><a name="l01745"></a><span class="lineno"> 1745</span>&#160;    &lt;s&gt; republican strategy of resistance to the renewal of obamas election &lt;/s&gt;</div><div class="line"><a name="l01746"></a><span class="lineno"> 1746</span>&#160;</div><div class="line"><a name="l01747"></a><span class="lineno"> 1747</span>&#160;</div><div class="line"><a name="l01748"></a><span class="lineno"> 1748</span>&#160;\section server Client-Server Mode (Experimental)</div><div class="line"><a name="l01749"></a><span class="lineno"> 1749</span>&#160;</div><div class="line"><a name="l01750"></a><span class="lineno"> 1750</span>&#160;HiFST can run in server mode.</div><div class="line"><a name="l01751"></a><span class="lineno"> 1751</span>&#160;</div><div class="line"><a name="l01752"></a><span class="lineno"> 1752</span>&#160;     &gt; hifst.${TGTBINMK}.bin --config=configs/CF.baseline.server &amp;&gt; log/log.server &amp;</div><div class="line"><a name="l01753"></a><span class="lineno"> 1753</span>&#160;     &gt; pid=$! # catch the server pid</div><div class="line"><a name="l01754"></a><span class="lineno"> 1754</span>&#160;</div><div class="line"><a name="l01755"></a><span class="lineno"> 1755</span>&#160;Note that in this particular configuration, both source and target wordmaps are loaded.</div><div class="line"><a name="l01756"></a><span class="lineno"> 1756</span>&#160;Hifst can read tokenized Russian text and produce tokenized English translations</div><div class="line"><a name="l01757"></a><span class="lineno"> 1757</span>&#160;(see options `--prepro.wordmap.load` and `--postpro.wordmap.load`).</div><div class="line"><a name="l01758"></a><span class="lineno"> 1758</span>&#160;Also, to ensure that CYK parser never fails, out of vocabulary (OOV) words must be detected (`--ssgrammar.addoovs.enable`) and sentence markers (`&lt;s&gt;`,`&lt;/s&gt;`)</div><div class="line"><a name="l01759"></a><span class="lineno"> 1759</span>&#160;have to be added on the fly, as the shallow grammar relies on them (i.e. `S 1 1`).</div><div class="line"><a name="l01760"></a><span class="lineno"> 1760</span>&#160;</div><div class="line"><a name="l01761"></a><span class="lineno"> 1761</span>&#160;With the `hifst-client.${TGTBINMK}.bin` binary, we can read Russian tokenized text (`RU/RU.tune`) and submit translation requests to the server.</div><div class="line"><a name="l01762"></a><span class="lineno"> 1762</span>&#160;The output is stored in a file specified by the client tool (`--target.store`).</div><div class="line"><a name="l01763"></a><span class="lineno"> 1763</span>&#160;</div><div class="line"><a name="l01764"></a><span class="lineno"> 1764</span>&#160;    &gt; sleep 60 # make sure to wait for the server to finish loading, otherwise clients will fail</div><div class="line"><a name="l01765"></a><span class="lineno"> 1765</span>&#160;    &gt; hifst-client.${TGTBINMK}.bin --config=configs/CF.baseline.client --range=200:5:300 --target.store=output/exp.clientserver/translation1.txt &amp;&gt; log/log.client1 &amp;</div><div class="line"><a name="l01766"></a><span class="lineno"> 1766</span>&#160;    # Connect to localhost, port=1205 and translate a bunch of sentences. Lets do this in background, just for fun</div><div class="line"><a name="l01767"></a><span class="lineno"> 1767</span>&#160;    # Note that the localhost setting is in the config file; this can point to another machine, of course</div><div class="line"><a name="l01768"></a><span class="lineno"> 1768</span>&#160;    &gt; pid2=$!</div><div class="line"><a name="l01769"></a><span class="lineno"> 1769</span>&#160;</div><div class="line"><a name="l01770"></a><span class="lineno"> 1770</span>&#160;    &gt; hifst-client.${TGTBINMK}.bin --config=configs/CF.baseline.client --range=1:50,100,1300 --target.store=output/exp.clientserver/translation2.txt &amp;&gt; log/log.client2 &amp;</div><div class="line"><a name="l01771"></a><span class="lineno"> 1771</span>&#160;    # In the meantime, we request another 52 translations...</div><div class="line"><a name="l01772"></a><span class="lineno"> 1772</span>&#160;    &gt; wait $pid2</div><div class="line"><a name="l01773"></a><span class="lineno"> 1773</span>&#160;</div><div class="line"><a name="l01774"></a><span class="lineno"> 1774</span>&#160;    &gt; kill -9 $pid</div><div class="line"><a name="l01775"></a><span class="lineno"> 1775</span>&#160;    # We are finished -- kill the server</div><div class="line"><a name="l01776"></a><span class="lineno"> 1776</span>&#160;</div><div class="line"><a name="l01777"></a><span class="lineno"> 1777</span>&#160;    &gt; head -5 output/exp.clientserver/translation2.txt</div><div class="line"><a name="l01778"></a><span class="lineno"> 1778</span>&#160;    parliament does not support the amendment , which gives you the freedom of tymoshenko</div><div class="line"><a name="l01779"></a><span class="lineno"> 1779</span>&#160;    amendment , which would have led to the release of which is in prison , former prime minister , was rejected during the second reading of the bill to ease penalty for economic offences .</div><div class="line"><a name="l01780"></a><span class="lineno"> 1780</span>&#160;    the verdict is not final , the court will consider an appeal of tymoshenko in december .</div><div class="line"><a name="l01781"></a><span class="lineno"> 1781</span>&#160;    a proposal to repeal article 365 of the code of criminal procedure , according to which was convicted former prime minister , was supported by the 147 members of parliament .</div><div class="line"><a name="l01782"></a><span class="lineno"> 1782</span>&#160;    victory in libya</div></div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="Tutorial_8010_8basic_8md.html">Tutorial.010.basic.md</a></li>
    <li class="footer">Generated on Wed May 25 2016 10:26:01 for Cambridge SMT System by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
